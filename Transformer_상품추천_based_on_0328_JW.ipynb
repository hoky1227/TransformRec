{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer 상품추천 based on_0328_JW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e8ebe90954645ba848a55a291d8dde5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a43d0eef2b047d286dd94c4be27d268",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5b9c9117242e48beb264eefddd007990",
              "IPY_MODEL_ddc3b76533df4b8cafd72bd8cd8ea2c0"
            ]
          }
        },
        "5a43d0eef2b047d286dd94c4be27d268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b9c9117242e48beb264eefddd007990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cbe0f7f65064446b81c60abd8bc60e12",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 11212,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 11212,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7bbcd127b9843a1a04b526a7f0c591b"
          }
        },
        "ddc3b76533df4b8cafd72bd8cd8ea2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_549740a738b0446692872dc0b2449974",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11212/11212 [2:05:46&lt;00:00,  1.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a6ab87278204fdd91be82f29e764b97"
          }
        },
        "cbe0f7f65064446b81c60abd8bc60e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7bbcd127b9843a1a04b526a7f0c591b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "549740a738b0446692872dc0b2449974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a6ab87278204fdd91be82f29e764b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoky1227/bert_based-recommendation/blob/main/Transformer_%EC%83%81%ED%92%88%EC%B6%94%EC%B2%9C_based_on_0328_JW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsJc4nu7CBFk",
        "outputId": "2434ff5a-b9b5-436a-aae3-b0a65bd73f04"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrwXm8HTWl_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a3626a-2c5c-4bb2-a9f7-f7b09b23ed0a"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow-addons\n",
        "!pip install -q pyyaml h5py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=468fc5c3f6048b746af853deea035840a119519b03b856a96f5073701d1da8f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqIBocN1XMe7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil.parser import parse\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWOQecXbXSaD"
      },
      "source": [
        "# %cd gdrive/MyDrive/Colab Notebooks/하렉스\n",
        "\n",
        "# df = pd.read_csv('data/2017online retail.csv')\n",
        "# df = df[(df['Customer'].notnull())&(df['Amount']>0)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXqnnj2fYDm5"
      },
      "source": [
        "# df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF1ULCiwZUj8"
      },
      "source": [
        "#### 'ItemCode' 안에 이상한 것들 찾아서 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwLg0TDbYbiy"
      },
      "source": [
        "# tmp = []\n",
        "# for i in df['ItemCode']:\n",
        "#     try:\n",
        "#         int(i)\n",
        "#     except:\n",
        "#         try:\n",
        "#             int(i[0:5])\n",
        "#         except:\n",
        "#             tmp.append(i)\n",
        "\n",
        "# df = df[[True if str(i) not in list(set(tmp)) else False for i in df['ItemCode']]].reset_index(drop=True)\n",
        "# df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI7Stgo4ZbCb"
      },
      "source": [
        "#### 구매 시간 기준 정렬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywqs_NmZZv1b"
      },
      "source": [
        "# len(df['Customer'].unique())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd0V1pl6ZaZD"
      },
      "source": [
        "# df['Date'] = [parse(i) for i in df['Date']]\n",
        "# df = df.sort_values(['Date']).reset_index(drop=True)\n",
        "# df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J--3958veo-2"
      },
      "source": [
        "#### 월 및 일주일 단위로 데이터 쪼개기\n",
        "- week1: 1일 ~ 7일\n",
        "- week2: 8일 ~ 14일\n",
        "- week3: 15일 ~ 21일\n",
        "- week4: 22일 ~ 끝까지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nwjHVrDZPPc"
      },
      "source": [
        "# month1_week1 = df[(np.array([True if i.month==1 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month1_week2 = df[(np.array([True if i.month==1 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month1_week3 = df[(np.array([True if i.month==1 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month1_week4 = df[(np.array([True if i.month==1 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]\n",
        "\n",
        "# month2_week1 = df[(np.array([True if i.month==2 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month2_week2 = df[(np.array([True if i.month==2 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month2_week3 = df[(np.array([True if i.month==2 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month2_week4 = df[(np.array([True if i.month==2 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]\n",
        "\n",
        "# month3_week1 = df[(np.array([True if i.month==3 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month3_week2 = df[(np.array([True if i.month==3 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month3_week3 = df[(np.array([True if i.month==3 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month3_week4 = df[(np.array([True if i.month==3 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]\n",
        "\n",
        "# month4_week1 = df[(np.array([True if i.month==4 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month4_week2 = df[(np.array([True if i.month==4 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month4_week3 = df[(np.array([True if i.month==4 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month4_week4 = df[(np.array([True if i.month==4 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]\n",
        "\n",
        "# month5_week1 = df[(np.array([True if i.month==5 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month5_week2 = df[(np.array([True if i.month==5 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month5_week3 = df[(np.array([True if i.month==5 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month5_week4 = df[(np.array([True if i.month==5 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]\n",
        "\n",
        "# month6_week1 = df[(np.array([True if i.month==6 else False for i in df['Date']]))&(np.array([True if i.day>=1 else False for i in df['Date']]))&(np.array([True if i.day<=7 else False for i in df['Date']]))]\n",
        "# month6_week2 = df[(np.array([True if i.month==6 else False for i in df['Date']]))&(np.array([True if i.day>=8 else False for i in df['Date']]))&(np.array([True if i.day<=14 else False for i in df['Date']]))]\n",
        "# month6_week3 = df[(np.array([True if i.month==6 else False for i in df['Date']]))&(np.array([True if i.day>=15 else False for i in df['Date']]))&(np.array([True if i.day<=21 else False for i in df['Date']]))]\n",
        "# month6_week4 = df[(np.array([True if i.month==6 else False for i in df['Date']]))&(np.array([True if i.day>=22 else False for i in df['Date']]))]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7dKy3gvfBPL"
      },
      "source": [
        "#### 잘 쪼개졌는지 확인\n",
        "- 쪼갠 전체 데이터셋의 크기를 다 더한 것이 원본 데이터의 크기와 같음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNXtv-gxbjem"
      },
      "source": [
        "# month1_week1.shape[0]+month1_week2.shape[0]+month1_week3.shape[0]+month1_week4.shape[0] + month2_week1.shape[0]+month2_week2.shape[0]+month2_week3.shape[0]+month2_week4.shape[0] + month3_week1.shape[0]+month3_week2.shape[0]+month3_week3.shape[0]+month3_week4.shape[0] + month4_week1.shape[0]+month4_week2.shape[0]+month4_week3.shape[0]+month4_week4.shape[0] + month5_week1.shape[0]+month5_week2.shape[0]+month5_week3.shape[0]+month5_week4.shape[0] + month6_week1.shape[0]+month6_week2.shape[0]+month6_week3.shape[0]+month6_week4.shape[0] == df.shape[0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aheihdzDGyU"
      },
      "source": [
        "# month_1 = df[[True if i.month==1 else False for i in df['Date']]]\n",
        "# month_2 = df[[True if i.month==2 else False for i in df['Date']]]\n",
        "# month_3 = df[[True if i.month==3 else False for i in df['Date']]]\n",
        "# month_4 = df[[True if i.month==4 else False for i in df['Date']]]\n",
        "# month_5 = df[[True if i.month==5 else False for i in df['Date']]]\n",
        "# month_6 = df[[True if i.month==6 else False for i in df['Date']]]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDmfae0fNDv"
      },
      "source": [
        "#### 쪼갠 데이터를 기반으로 학습 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxNLgE1-dpXM"
      },
      "source": [
        "# train_df = pd.DataFrame()\n",
        "# # for tmp_df in tqdm_notebook([month_1, month_2, month_3, month_4, month_5, month_6]):\n",
        "# for tmp_df in tqdm_notebook([month1_week1, month1_week2, month1_week3, month1_week4, \n",
        "#                              month2_week1, month2_week2, month2_week3, month2_week4,\n",
        "#                              month3_week1, month3_week2, month3_week3, month3_week4,\n",
        "#                              month4_week1, month4_week2, month4_week3, month4_week4,\n",
        "#                              month5_week1, month5_week2, month5_week3, month5_week4,\n",
        "#                              month6_week1, month6_week2, month6_week3, month6_week4]):\n",
        "#     train = []\n",
        "#     label = []\n",
        "\n",
        "#     for uesr_code in tmp_df['Customer'].unique():\n",
        "#         buy_items = [i for i in tmp_df[tmp_df['Customer']==uesr_code]['Detail']]\n",
        "#         train.append(','.join(buy_items[:-1]))\n",
        "#         label.append(buy_items[-1])\n",
        "\n",
        "#     train_df = pd.concat([train_df, pd.DataFrame({'user':tmp_df['Customer'].unique(), 'train':train, 'label':label})], axis=0)\n",
        "# train_df = train_df.drop_duplicates()\n",
        "# train_df = train_df[train_df['train'] != ''].reset_index(drop=True)\n",
        "# train_df['token_len'] = [len(sum([i.split(' ') for i in aaa.split(',')], [])) for aaa in train_df['train']]\n",
        "\n",
        "# token_len_cate = []\n",
        "# for i in train_df['token_len']:\n",
        "#     if i >= 0 and i < 100:\n",
        "#         token_len_cate.append('0:100')\n",
        "#     elif i >= 100 and i < 200:\n",
        "#         token_len_cate.append('100:200')\n",
        "#     elif i >= 200 and i < 300:\n",
        "#         token_len_cate.append('200:300')\n",
        "#     elif i >= 300 and i < 400:\n",
        "#         token_len_cate.append('300:400')\n",
        "#     elif i >= 400 and i < 500:\n",
        "#         token_len_cate.append('400:500')\n",
        "#     elif i >= 500 and i < 600:\n",
        "#         token_len_cate.append('500:600')\n",
        "#     elif i >= 600 and i < 700:\n",
        "#         token_len_cate.append('600:700')\n",
        "#     elif i >= 700 and i < 800:\n",
        "#         token_len_cate.append('700:800')\n",
        "#     elif i >= 800 and i < 900:\n",
        "#         token_len_cate.append('800:900')\n",
        "#     elif i >= 900:\n",
        "#         token_len_cate.append('900:')\n",
        "# train_df['token_len_cate'] = token_len_cate\n",
        "\n",
        "# train_df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTYLa1rrCY80"
      },
      "source": [
        "#### 내 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "65jRNKKyCgRs",
        "outputId": "628c143c-cc90-4a67-f68e-2e651196e217"
      },
      "source": [
        "data = pd.read_csv(path + 'train_bert.csv')\n",
        "data.pop('Unnamed: 0')\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "data"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>Detail</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33113.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;RED HANGING H...</td>\n",
              "      <td>PINK OWL SOFT TOY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38170.0</td>\n",
              "      <td>METAL 4 HOOK HANGER FRENCH CHATEAU&amp;&amp;PACK OF 12...</td>\n",
              "      <td>HAND OVER THE CHOCOLATE   SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37696.0</td>\n",
              "      <td>URBAN BLACK RIBBONS&amp;&amp;SCANDINAVIAN REDS RIBBONS...</td>\n",
              "      <td>CERAMIC CAKE DESIGN SPOTTED MUG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36242.0</td>\n",
              "      <td>PACK OF 20 SKULL PAPER NAPKINS&amp;&amp;PACK 20 DOLLY ...</td>\n",
              "      <td>I'M ON HOLIDAY METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33004.0</td>\n",
              "      <td>60 TEATIME FAIRY CAKE CASES&amp;&amp;KEY FOB , SHED&amp;&amp;G...</td>\n",
              "      <td>GROW YOUR OWN BASIL IN ENAMEL MUG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11570</th>\n",
              "      <td>37496.0</td>\n",
              "      <td>SWEETHEART CERAMIC TRINKET BOX&amp;&amp;STRAWBERRY CER...</td>\n",
              "      <td>GLITTER HANGING BUTTERFLY STRING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11571</th>\n",
              "      <td>34224.0</td>\n",
              "      <td>NATURAL SLATE CHALKBOARD LARGE&amp;&amp;S/4 CACTI CANDLES</td>\n",
              "      <td>FRENCH STYLE WALL DRESSER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11572</th>\n",
              "      <td>37448.0</td>\n",
              "      <td>SET OF 4 FAIRY CAKES COASTERS&amp;&amp;SET/6 PINK  BUT...</td>\n",
              "      <td>HOUSE WRECKING METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11573</th>\n",
              "      <td>34857.0</td>\n",
              "      <td>FELTCRAFT HAIRBANDS PINK + WHITE&amp;&amp;6 RIBBONS SH...</td>\n",
              "      <td>FELTCRAFT DOLL ROSIESET OF 12 MINI BUNNIES IN ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11574</th>\n",
              "      <td>35039.0</td>\n",
              "      <td>LUNCH BAG  BLACK SKULL.&amp;&amp;SET OF 72 RETRO SPOT ...</td>\n",
              "      <td>UNION STRIPE WITH FRINGE  HAMMOCK</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11575 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Customer  ...                                              label\n",
              "0       33113.0  ...                                  PINK OWL SOFT TOY\n",
              "1       38170.0  ...                     HAND OVER THE CHOCOLATE   SIGN\n",
              "2       37696.0  ...                    CERAMIC CAKE DESIGN SPOTTED MUG\n",
              "3       36242.0  ...                          I'M ON HOLIDAY METAL SIGN\n",
              "4       33004.0  ...                  GROW YOUR OWN BASIL IN ENAMEL MUG\n",
              "...         ...  ...                                                ...\n",
              "11570   37496.0  ...                   GLITTER HANGING BUTTERFLY STRING\n",
              "11571   34224.0  ...                          FRENCH STYLE WALL DRESSER\n",
              "11572   37448.0  ...                          HOUSE WRECKING METAL SIGN\n",
              "11573   34857.0  ...  FELTCRAFT DOLL ROSIESET OF 12 MINI BUNNIES IN ...\n",
              "11574   35039.0  ...                  UNION STRIPE WITH FRINGE  HAMMOCK\n",
              "\n",
              "[11575 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "wSCGdatWEGvU",
        "outputId": "659ee429-e523-4c57-8f6b-1a8f65648058"
      },
      "source": [
        "train_df = data.copy()\n",
        "train_df.rename(columns = {'Detail':'train'}, inplace=True)\n",
        "train_df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>train</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33113.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;RED HANGING H...</td>\n",
              "      <td>PINK OWL SOFT TOY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38170.0</td>\n",
              "      <td>METAL 4 HOOK HANGER FRENCH CHATEAU&amp;&amp;PACK OF 12...</td>\n",
              "      <td>HAND OVER THE CHOCOLATE   SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37696.0</td>\n",
              "      <td>URBAN BLACK RIBBONS&amp;&amp;SCANDINAVIAN REDS RIBBONS...</td>\n",
              "      <td>CERAMIC CAKE DESIGN SPOTTED MUG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36242.0</td>\n",
              "      <td>PACK OF 20 SKULL PAPER NAPKINS&amp;&amp;PACK 20 DOLLY ...</td>\n",
              "      <td>I'M ON HOLIDAY METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33004.0</td>\n",
              "      <td>60 TEATIME FAIRY CAKE CASES&amp;&amp;KEY FOB , SHED&amp;&amp;G...</td>\n",
              "      <td>GROW YOUR OWN BASIL IN ENAMEL MUG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11570</th>\n",
              "      <td>37496.0</td>\n",
              "      <td>SWEETHEART CERAMIC TRINKET BOX&amp;&amp;STRAWBERRY CER...</td>\n",
              "      <td>GLITTER HANGING BUTTERFLY STRING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11571</th>\n",
              "      <td>34224.0</td>\n",
              "      <td>NATURAL SLATE CHALKBOARD LARGE&amp;&amp;S/4 CACTI CANDLES</td>\n",
              "      <td>FRENCH STYLE WALL DRESSER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11572</th>\n",
              "      <td>37448.0</td>\n",
              "      <td>SET OF 4 FAIRY CAKES COASTERS&amp;&amp;SET/6 PINK  BUT...</td>\n",
              "      <td>HOUSE WRECKING METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11573</th>\n",
              "      <td>34857.0</td>\n",
              "      <td>FELTCRAFT HAIRBANDS PINK + WHITE&amp;&amp;6 RIBBONS SH...</td>\n",
              "      <td>FELTCRAFT DOLL ROSIESET OF 12 MINI BUNNIES IN ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11574</th>\n",
              "      <td>35039.0</td>\n",
              "      <td>LUNCH BAG  BLACK SKULL.&amp;&amp;SET OF 72 RETRO SPOT ...</td>\n",
              "      <td>UNION STRIPE WITH FRINGE  HAMMOCK</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11575 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Customer  ...                                              label\n",
              "0       33113.0  ...                                  PINK OWL SOFT TOY\n",
              "1       38170.0  ...                     HAND OVER THE CHOCOLATE   SIGN\n",
              "2       37696.0  ...                    CERAMIC CAKE DESIGN SPOTTED MUG\n",
              "3       36242.0  ...                          I'M ON HOLIDAY METAL SIGN\n",
              "4       33004.0  ...                  GROW YOUR OWN BASIL IN ENAMEL MUG\n",
              "...         ...  ...                                                ...\n",
              "11570   37496.0  ...                   GLITTER HANGING BUTTERFLY STRING\n",
              "11571   34224.0  ...                          FRENCH STYLE WALL DRESSER\n",
              "11572   37448.0  ...                          HOUSE WRECKING METAL SIGN\n",
              "11573   34857.0  ...  FELTCRAFT DOLL ROSIESET OF 12 MINI BUNNIES IN ...\n",
              "11574   35039.0  ...                  UNION STRIPE WITH FRINGE  HAMMOCK\n",
              "\n",
              "[11575 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQmA7cFfS1v"
      },
      "source": [
        "#### 학습 데이터의 토큰 갯수 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIlSDEc3DlJz"
      },
      "source": [
        "train_df['token_len'] = [len(sum([i.split(' ') for i in aaa.split('&&')], [])) for aaa in train_df['train']]\n",
        "\n",
        "token_len_cate = []\n",
        "for i in train_df['token_len']:\n",
        "    if i >= 0 and i < 100:\n",
        "        token_len_cate.append('0:100')\n",
        "    elif i >= 100 and i < 200:\n",
        "        token_len_cate.append('100:200')\n",
        "    elif i >= 200 and i < 300:\n",
        "        token_len_cate.append('200:300')\n",
        "    elif i >= 300 and i < 400:\n",
        "        token_len_cate.append('300:400')\n",
        "    elif i >= 400 and i < 500:\n",
        "        token_len_cate.append('400:500')\n",
        "    elif i >= 500 and i < 600:\n",
        "        token_len_cate.append('500:600')\n",
        "    elif i >= 600 and i < 700:\n",
        "        token_len_cate.append('600:700')\n",
        "    elif i >= 700 and i < 800:\n",
        "        token_len_cate.append('700:800')\n",
        "    elif i >= 800 and i < 900:\n",
        "        token_len_cate.append('800:900')\n",
        "    elif i >= 900:\n",
        "        token_len_cate.append('900:')\n",
        "train_df['token_len_cate'] = token_len_cate"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OiFp_ehEQSER",
        "outputId": "5d4e444d-1aa8-4b6f-ab88-ef5e290a4298"
      },
      "source": [
        "token_len_sort = sorted({i:j for i, j in zip(train_df['token_len_cate'].value_counts().keys(), train_df['token_len_cate'].value_counts().values)}.items(), key=lambda x:x[0])\n",
        "x = [i for (i,_) in token_len_sort]\n",
        "y = [i for (_,i) in token_len_sort]\n",
        "print(y)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6198, 3149, 1297, 568, 246, 86, 21, 3, 3, 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXAiNbuTfhlz"
      },
      "source": [
        "#### 토큰 갯수 512 이하로 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClCqbjQdGd0J"
      },
      "source": [
        "filtering_train_df = train_df.copy()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8w8eOJQQ3S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a5e881-aaee-44cc-c1d9-e382000b8da2"
      },
      "source": [
        "filtering_train_df = train_df[train_df['token_len']<400].reset_index(drop=True)\n",
        "filtering_train_df['token_len'].max()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfhU7g2IDC3J"
      },
      "source": [
        "# cut = round(len(data1) * 0.8)\n",
        "# data.rename(columns = {'Detail':'train'}, inplace=True)\n",
        "# filtering_train_df, filtering_test_df = data[:cut], data[cut:]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTDYnAiwlCg6"
      },
      "source": [
        "#### 학습을 위해 토큰으로 자르기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIAMM6IYekZ2"
      },
      "source": [
        "filtering_train_df['split_train_token'] = ['<esp> ' + i.strip().replace('&&', ' <esp> ') + ' <esp>' for i in filtering_train_df['train']]\n",
        "filtering_train_df['split_label_token'] = ['<esp> ' + i.strip() + ' <esp>' for i in filtering_train_df['label']]\n",
        "# filtering_train_df['split_train_token'] = ['<sos> ' + i.strip().replace('&&', ' <eos> ') + ' <eos>' for i in filtering_train_df['train']]\n",
        "# filtering_train_df['split_label_token'] = ['<sos> ' + i.strip() + ' <eos>' for i in filtering_train_df['label']]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "grUN5n41gX2W",
        "outputId": "e30e674d-01b1-46dd-a3ae-50ca77543390"
      },
      "source": [
        "filtering_train_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>train</th>\n",
              "      <th>label</th>\n",
              "      <th>token_len</th>\n",
              "      <th>token_len_cate</th>\n",
              "      <th>split_train_token</th>\n",
              "      <th>split_label_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33113.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;RED HANGING H...</td>\n",
              "      <td>PINK OWL SOFT TOY</td>\n",
              "      <td>302</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; JUMBO BAG PINK WITH WHITE SPOTS &lt;esp&gt; RE...</td>\n",
              "      <td>&lt;esp&gt; PINK OWL SOFT TOY &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38170.0</td>\n",
              "      <td>METAL 4 HOOK HANGER FRENCH CHATEAU&amp;&amp;PACK OF 12...</td>\n",
              "      <td>HAND OVER THE CHOCOLATE   SIGN</td>\n",
              "      <td>60</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; METAL 4 HOOK HANGER FRENCH CHATEAU &lt;esp&gt;...</td>\n",
              "      <td>&lt;esp&gt; HAND OVER THE CHOCOLATE   SIGN &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37696.0</td>\n",
              "      <td>URBAN BLACK RIBBONS&amp;&amp;SCANDINAVIAN REDS RIBBONS...</td>\n",
              "      <td>CERAMIC CAKE DESIGN SPOTTED MUG</td>\n",
              "      <td>185</td>\n",
              "      <td>100:200</td>\n",
              "      <td>&lt;esp&gt; URBAN BLACK RIBBONS &lt;esp&gt; SCANDINAVIAN R...</td>\n",
              "      <td>&lt;esp&gt; CERAMIC CAKE DESIGN SPOTTED MUG &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33004.0</td>\n",
              "      <td>60 TEATIME FAIRY CAKE CASES&amp;&amp;KEY FOB , SHED&amp;&amp;G...</td>\n",
              "      <td>GROW YOUR OWN BASIL IN ENAMEL MUG</td>\n",
              "      <td>95</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; 60 TEATIME FAIRY CAKE CASES &lt;esp&gt; KEY FO...</td>\n",
              "      <td>&lt;esp&gt; GROW YOUR OWN BASIL IN ENAMEL MUG &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35547.0</td>\n",
              "      <td>STRAWBERRY SCENTED SET/9 T-LIGHTS&amp;&amp;WOODEN FRAM...</td>\n",
              "      <td>RED HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>347</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; STRAWBERRY SCENTED SET/9 T-LIGHTS &lt;esp&gt; ...</td>\n",
              "      <td>&lt;esp&gt; RED HANGING HEART T-LIGHT HOLDER &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...                              split_label_token\n",
              "0   33113.0  ...                  <esp> PINK OWL SOFT TOY <esp>\n",
              "1   38170.0  ...     <esp> HAND OVER THE CHOCOLATE   SIGN <esp>\n",
              "2   37696.0  ...    <esp> CERAMIC CAKE DESIGN SPOTTED MUG <esp>\n",
              "3   33004.0  ...  <esp> GROW YOUR OWN BASIL IN ENAMEL MUG <esp>\n",
              "4   35547.0  ...   <esp> RED HANGING HEART T-LIGHT HOLDER <esp>\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5o1x06Y3Qh"
      },
      "source": [
        "MAX_LENGTH = 512"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vUJ-Vy-Yvnu"
      },
      "source": [
        "# # for i in range(len(train_token)):\n",
        "# #     if len(train_token[i]) > MAX_LENGTH:\n",
        "# #         while 1:\n",
        "# #             id = train_token[i].index('<esp>')\n",
        "# #             train_token[i] = train_token[i][id:]\n",
        "# #             if len(train_token[i]) <= MAX_LENGTH:\n",
        "# #                 break\n",
        "# for i in range(len(filtering_train_df)):\n",
        "#     filtering_train_df['split_train_token'][i] = filtering_train_df['split_train_token'][i].split(' ')\n",
        "#     if len(filtering_train_df['split_train_token'][i]) > MAX_LENGTH:\n",
        "#         while 1:\n",
        "#             id = filtering_train_df['split_train_token'][i].index('<esp>')\n",
        "#             filtering_train_df['split_train_token'][i] = filtering_train_df['split_train_token'][i][id:]\n",
        "#             # tokenized_texts[i][0] = '[CLS]'\n",
        "#             if len(filtering_train_df['split_train_token'][i]) <= MAX_LENGTH:\n",
        "#                 break\n",
        "#     filtering_train_df['split_train_token'][i] = ' '.join(x for x in filtering_train_df['split_train_token'][i])\n",
        "\n",
        "# filtering_train_df.head()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrRaQMinZ0H"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(filtering_train_df['split_train_token'] + filtering_train_df['split_label_token'], target_vocab_size=2**13)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNc2-ElvnZ7l"
      },
      "source": [
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XNjMLlYnZ9h",
        "outputId": "201cba96-1233-4e88-e0dd-3d645890ce84"
      },
      "source": [
        "print('시작 토큰 번호 :',START_TOKEN)\n",
        "print('종료 토큰 번호 :',END_TOKEN)\n",
        "print('단어 집합의 크기 :',VOCAB_SIZE)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "시작 토큰 번호 : [6176]\n",
            "종료 토큰 번호 : [6177]\n",
            "단어 집합의 크기 : 6178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyU2LyVcnZ_u"
      },
      "source": [
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    tokenized_inputs.append(sentence1)\n",
        "    tokenized_outputs.append(sentence2)\n",
        "\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGD78-kOL_wk"
      },
      "source": [
        "train_token, label_token = tokenize_and_filter(filtering_train_df['split_train_token'], filtering_train_df['split_label_token'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNW0IgXRMYfD",
        "outputId": "8b5ec77c-96eb-4152-aa71-d12a65f6a722"
      },
      "source": [
        "print('질문 데이터의 크기(shape) :', train_token.shape)\n",
        "print('답변 데이터의 크기(shape) :', label_token.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "질문 데이터의 크기(shape) : (11212, 512)\n",
            "답변 데이터의 크기(shape) : (11212, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHi5hUkwF6NG"
      },
      "source": [
        "#### GPU 작동 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMTEg_x0F5wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b784550-eceb-4fb4-ccef-c5deb2a3105c"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T3tWOLGGDgI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "073640f5-fd29-40ac-b1ad-a8b24a0eb534"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syPtUTKK1fEt",
        "outputId": "4fcf5fd3-ac8f-490e-8398-282deb631a71"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "print(USE_CUDA)\n",
        "\n",
        "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
        "print('학습을 진행하는 기기:',device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "학습을 진행하는 기기: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb_xqSUBw5TI",
        "outputId": "94147f7a-b275-4121-e10c-96d99463ecd8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 31 08:31:39 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W /  70W |    222MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo4OnGGzGAGz"
      },
      "source": [
        "#### 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LomtZ4zL0ELY"
      },
      "source": [
        "# import math\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class TransformerModel(nn.Module):\n",
        "\n",
        "#     def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "#         super(TransformerModel, self).__init__()\n",
        "#         from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "#         self.model_type = 'Transformer'\n",
        "#         self.src_mask = None\n",
        "#         self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "#         encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "#         self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "#         self.encoder = nn.Embedding(ntoken, ninp)\n",
        "#         self.ninp = ninp\n",
        "#         self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "#         self.init_weights()\n",
        "\n",
        "#     def _generate_square_subsequent_mask(self, sz):\n",
        "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "#         return mask\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         initrange = 0.1\n",
        "#         self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "#         self.decoder.bias.data.zero_()\n",
        "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "#     def forward(self, src):\n",
        "#         if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "#             device = src.device\n",
        "#             mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "#             self.src_mask = mask\n",
        "\n",
        "#         src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "#         src = self.pos_encoder(src)\n",
        "#         output = self.transformer_encoder(src, self.src_mask)\n",
        "#         output = self.decoder(output)\n",
        "#         return output\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "\n",
        "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.pe[:x.size(0), :]\n",
        "#         return self.dropout(x)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUAoFitg13BG"
      },
      "source": [
        "# import torchtext\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# # TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "# #                             init_token='<sos>',\n",
        "# #                             eos_token='<eos>',\n",
        "# #                             lower=True)\n",
        "# train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "# TEXT.build_vocab(train_txt)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# def batchify(data, bsz):\n",
        "#     data = TEXT.numericalize([data.examples[0].text])\n",
        "#     # 데이터셋을 bsz 파트들로 나눕니다.\n",
        "#     nbatch = data.size(0) // bsz\n",
        "#     # 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들) 은 잘라냅니다.\n",
        "#     data = data.narrow(0, 0, nbatch * bsz)\n",
        "#     # 데이터에 대하여 bsz 배치들로 동등하게 나눕니다.\n",
        "#     data = data.view(bsz, -1).t().contiguous()\n",
        "#     return data.to(device)\n",
        "\n",
        "# batch_size = 20\n",
        "# eval_batch_size = 10\n",
        "# train_data = batchify(train_txt, batch_size)\n",
        "# val_data = batchify(val_txt, eval_batch_size)\n",
        "# test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiasLbC2bev"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# lr = 5.0 # 학습률\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "# import time\n",
        "# def train():\n",
        "#     model.train() # 학습 모드를 시작합니다.\n",
        "#     total_loss = 0.\n",
        "#     start_time = time.time()\n",
        "#     ntokens = len(TEXT.vocab.stoi)\n",
        "#     for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "#         data, targets = get_batch(train_data, i)\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(data)\n",
        "#         loss = criterion(output.view(-1, ntokens), targets)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "#         log_interval = 200\n",
        "#         if batch % log_interval == 0 and batch > 0:\n",
        "#             cur_loss = total_loss / log_interval\n",
        "#             elapsed = time.time() - start_time\n",
        "#             print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "#                   'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "#                   'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "#                     epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "#                     elapsed * 1000 / log_interval,\n",
        "#                     cur_loss, math.exp(cur_loss)))\n",
        "#             total_loss = 0\n",
        "#             start_time = time.time()\n",
        "\n",
        "# def evaluate(eval_model, data_source):\n",
        "#     eval_model.eval() # 평가 모드를 시작합니다.\n",
        "#     total_loss = 0.\n",
        "#     ntokens = len(TEXT.vocab.stoi)\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(0, data_source.size(0) - 1, bptt):\n",
        "#             data, targets = get_batch(data_source, i)\n",
        "#             output = eval_model(data)\n",
        "#             output_flat = output.view(-1, ntokens)\n",
        "#             total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "#     return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmbnOw_YTj04"
      },
      "source": [
        "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
        "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_token,\n",
        "        'dec_inputs': label_token[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
        "    },\n",
        "    {\n",
        "        'outputs': label_token[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2MT1lyXKzzg"
      },
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  \"\"\"Calculate the attention weights. \"\"\"\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask to zero out padding tokens\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "\n",
        "  return output\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # scaled dot-product attention\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # concatenation of heads\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # final linear layer\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "    # apply sin to even index in the array\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd index in the array\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "  \n",
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "  \n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "  \n",
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "  \n",
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "  # mask the future tokens for decoder inputs at the 1st attention block\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "  # mask the encoder outputs for the 2nd attention block\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4kPBiQkLF8B"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaxW16nJLK9E"
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rMmAF6jLONf"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDXvuc8Tj6J"
      },
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udey1pWqABXR"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVHbISTxTj8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3121870-8283-4f81-fbbb-a1b447b7c7a8"
      },
      "source": [
        "model.fit(dataset, epochs=EPOCHS)\n",
        "# if tf.test.is_gpu_available():\n",
        "#   print(\"On GPU:\")\n",
        "#   with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n",
        "#     model.fit(dataset, epochs=EPOCHS)\n",
        "#     # assert x.device.endswith(\"GPU:0\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "176/176 [==============================] - 155s 833ms/step - loss: 0.1914 - accuracy: 0.0016\n",
            "Epoch 2/30\n",
            "176/176 [==============================] - 149s 849ms/step - loss: 0.1198 - accuracy: 0.0128\n",
            "Epoch 3/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0668 - accuracy: 0.0140\n",
            "Epoch 4/30\n",
            "176/176 [==============================] - 150s 850ms/step - loss: 0.0491 - accuracy: 0.0155\n",
            "Epoch 5/30\n",
            "176/176 [==============================] - 150s 849ms/step - loss: 0.0379 - accuracy: 0.0169\n",
            "Epoch 6/30\n",
            "176/176 [==============================] - 150s 850ms/step - loss: 0.0305 - accuracy: 0.0179\n",
            "Epoch 7/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0246 - accuracy: 0.0186\n",
            "Epoch 8/30\n",
            "176/176 [==============================] - 150s 852ms/step - loss: 0.0207 - accuracy: 0.0192\n",
            "Epoch 9/30\n",
            "176/176 [==============================] - 150s 853ms/step - loss: 0.0174 - accuracy: 0.0196\n",
            "Epoch 10/30\n",
            "176/176 [==============================] - 150s 854ms/step - loss: 0.0146 - accuracy: 0.0200\n",
            "Epoch 11/30\n",
            "176/176 [==============================] - 150s 853ms/step - loss: 0.0124 - accuracy: 0.0203\n",
            "Epoch 12/30\n",
            "176/176 [==============================] - 150s 854ms/step - loss: 0.0105 - accuracy: 0.0208\n",
            "Epoch 13/30\n",
            "176/176 [==============================] - 150s 852ms/step - loss: 0.0087 - accuracy: 0.0211\n",
            "Epoch 14/30\n",
            "176/176 [==============================] - 150s 850ms/step - loss: 0.0071 - accuracy: 0.0214\n",
            "Epoch 15/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0058 - accuracy: 0.0216\n",
            "Epoch 16/30\n",
            "176/176 [==============================] - 150s 852ms/step - loss: 0.0048 - accuracy: 0.0219\n",
            "Epoch 17/30\n",
            "176/176 [==============================] - 150s 852ms/step - loss: 0.0040 - accuracy: 0.0220\n",
            "Epoch 18/30\n",
            "176/176 [==============================] - 150s 853ms/step - loss: 0.0034 - accuracy: 0.0223\n",
            "Epoch 19/30\n",
            "176/176 [==============================] - 150s 853ms/step - loss: 0.0031 - accuracy: 0.0223\n",
            "Epoch 20/30\n",
            "176/176 [==============================] - 150s 855ms/step - loss: 0.0029 - accuracy: 0.0223\n",
            "Epoch 21/30\n",
            "176/176 [==============================] - 150s 852ms/step - loss: 0.0028 - accuracy: 0.0224\n",
            "Epoch 22/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0027 - accuracy: 0.0224\n",
            "Epoch 23/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0027 - accuracy: 0.0224\n",
            "Epoch 24/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0025 - accuracy: 0.0225\n",
            "Epoch 25/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0022 - accuracy: 0.0225\n",
            "Epoch 26/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0020 - accuracy: 0.0226\n",
            "Epoch 27/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0018 - accuracy: 0.0227\n",
            "Epoch 28/30\n",
            "176/176 [==============================] - 150s 850ms/step - loss: 0.0017 - accuracy: 0.0227\n",
            "Epoch 29/30\n",
            "176/176 [==============================] - 150s 850ms/step - loss: 0.0015 - accuracy: 0.0227\n",
            "Epoch 30/30\n",
            "176/176 [==============================] - 150s 851ms/step - loss: 0.0014 - accuracy: 0.0227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09e71b5750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3L0LlZK0reo"
      },
      "source": [
        "# best_val_loss = float(\"inf\")\n",
        "# epochs = 1 # 에포크 수\n",
        "# best_model = None\n",
        "\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     epoch_start_time = time.time()\n",
        "#     train()\n",
        "#     val_loss = evaluate(model, val_data)\n",
        "#     print('-' * 89)\n",
        "#     print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "#           'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "#                                      val_loss, math.exp(val_loss)))\n",
        "#     print('-' * 89)\n",
        "\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         best_model = model\n",
        "\n",
        "#     scheduler.step()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EnZzdWL_718"
      },
      "source": [
        "name = 'Transformer_' + str(EPOCHS) + '.h5'\n",
        "# name = 'Transformer_' + str(epochs) + '.pt'"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctgBKzffNI58"
      },
      "source": [
        "model.save_weights(path+f'Transformer_{EPOCHS}_weights.ckpt')\n",
        "# model.save(path + name)\n",
        "# torch.save(model, path+name)\n",
        "# tf.saved_model.save(model, path+name)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR-vDKccX0Xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5b8610-b6c7-49a8-abc5-4149c93cec1c"
      },
      "source": [
        "# model = tf.keras.models.load_model(path + name)\n",
        "model.load_weights(path+f'Transformer_{EPOCHS}_weights.ckpt')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0998074190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvgs4Pf4TkEC"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 예측 시작\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "#   print('Input: {}'.format(sentence))\n",
        "#   print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence\n",
        "\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYuXOFHwYcc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a4e8bce3-7c06-4fee-bc27-d0578300c15b"
      },
      "source": [
        "filtering_train_df.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>train</th>\n",
              "      <th>label</th>\n",
              "      <th>token_len</th>\n",
              "      <th>token_len_cate</th>\n",
              "      <th>split_train_token</th>\n",
              "      <th>split_label_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33113.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;RED HANGING H...</td>\n",
              "      <td>PINK OWL SOFT TOY</td>\n",
              "      <td>302</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; JUMBO BAG PINK WITH WHITE SPOTS &lt;esp&gt; RE...</td>\n",
              "      <td>&lt;esp&gt; PINK OWL SOFT TOY &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38170.0</td>\n",
              "      <td>METAL 4 HOOK HANGER FRENCH CHATEAU&amp;&amp;PACK OF 12...</td>\n",
              "      <td>HAND OVER THE CHOCOLATE   SIGN</td>\n",
              "      <td>60</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; METAL 4 HOOK HANGER FRENCH CHATEAU &lt;esp&gt;...</td>\n",
              "      <td>&lt;esp&gt; HAND OVER THE CHOCOLATE   SIGN &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37696.0</td>\n",
              "      <td>URBAN BLACK RIBBONS&amp;&amp;SCANDINAVIAN REDS RIBBONS...</td>\n",
              "      <td>CERAMIC CAKE DESIGN SPOTTED MUG</td>\n",
              "      <td>185</td>\n",
              "      <td>100:200</td>\n",
              "      <td>&lt;esp&gt; URBAN BLACK RIBBONS &lt;esp&gt; SCANDINAVIAN R...</td>\n",
              "      <td>&lt;esp&gt; CERAMIC CAKE DESIGN SPOTTED MUG &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33004.0</td>\n",
              "      <td>60 TEATIME FAIRY CAKE CASES&amp;&amp;KEY FOB , SHED&amp;&amp;G...</td>\n",
              "      <td>GROW YOUR OWN BASIL IN ENAMEL MUG</td>\n",
              "      <td>95</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; 60 TEATIME FAIRY CAKE CASES &lt;esp&gt; KEY FO...</td>\n",
              "      <td>&lt;esp&gt; GROW YOUR OWN BASIL IN ENAMEL MUG &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35547.0</td>\n",
              "      <td>STRAWBERRY SCENTED SET/9 T-LIGHTS&amp;&amp;WOODEN FRAM...</td>\n",
              "      <td>RED HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>347</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; STRAWBERRY SCENTED SET/9 T-LIGHTS &lt;esp&gt; ...</td>\n",
              "      <td>&lt;esp&gt; RED HANGING HEART T-LIGHT HOLDER &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...                              split_label_token\n",
              "0   33113.0  ...                  <esp> PINK OWL SOFT TOY <esp>\n",
              "1   38170.0  ...     <esp> HAND OVER THE CHOCOLATE   SIGN <esp>\n",
              "2   37696.0  ...    <esp> CERAMIC CAKE DESIGN SPOTTED MUG <esp>\n",
              "3   33004.0  ...  <esp> GROW YOUR OWN BASIL IN ENAMEL MUG <esp>\n",
              "4   35547.0  ...   <esp> RED HANGING HEART T-LIGHT HOLDER <esp>\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k592OU1EY7xc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a807353a-e3f2-4f12-d13f-c2be72d155e7"
      },
      "source": [
        "predict(filtering_train_df['split_label_token'][2])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<esp> TEA TIME OVEN GLOVE <esp>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Br9KixTkFc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "9e8ebe90954645ba848a55a291d8dde5",
            "5a43d0eef2b047d286dd94c4be27d268",
            "5b9c9117242e48beb264eefddd007990",
            "ddc3b76533df4b8cafd72bd8cd8ea2c0",
            "cbe0f7f65064446b81c60abd8bc60e12",
            "a7bbcd127b9843a1a04b526a7f0c591b",
            "549740a738b0446692872dc0b2449974",
            "3a6ab87278204fdd91be82f29e764b97"
          ]
        },
        "outputId": "52d199bb-07ee-41b6-ddbc-bee38c314ffc"
      },
      "source": [
        "predict_item = [predict(i) for i in tqdm_notebook(filtering_train_df['split_train_token'])]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e8ebe90954645ba848a55a291d8dde5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=11212.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjlcS-7hphRs"
      },
      "source": [
        "filtering_train_df['predict_item'] = predict_item"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBtGxj_ppnI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a43977-9058-42b4-b739-99bae1328e73"
      },
      "source": [
        "pd.Series([i in list(filtering_train_df['split_label_token']) for i in filtering_train_df['predict_item']]).value_counts()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     11043\n",
              "False      169\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7emrtFTn6XVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5a1cee41-5636-4dc3-cef2-2c73247dabdf"
      },
      "source": [
        "filtering_train_df.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>train</th>\n",
              "      <th>label</th>\n",
              "      <th>token_len</th>\n",
              "      <th>token_len_cate</th>\n",
              "      <th>split_train_token</th>\n",
              "      <th>split_label_token</th>\n",
              "      <th>predict_item</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33113.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;RED HANGING H...</td>\n",
              "      <td>PINK OWL SOFT TOY</td>\n",
              "      <td>302</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; JUMBO BAG PINK WITH WHITE SPOTS &lt;esp&gt; RE...</td>\n",
              "      <td>&lt;esp&gt; PINK OWL SOFT TOY &lt;esp&gt;</td>\n",
              "      <td>&lt;esp&gt; CHINESE DRAGON PAPER LANTERNS &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38170.0</td>\n",
              "      <td>METAL 4 HOOK HANGER FRENCH CHATEAU&amp;&amp;PACK OF 12...</td>\n",
              "      <td>HAND OVER THE CHOCOLATE   SIGN</td>\n",
              "      <td>60</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; METAL 4 HOOK HANGER FRENCH CHATEAU &lt;esp&gt;...</td>\n",
              "      <td>&lt;esp&gt; HAND OVER THE CHOCOLATE   SIGN &lt;esp&gt;</td>\n",
              "      <td>&lt;esp&gt; HAND OVER THE CHOCOLATE   SIGN &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37696.0</td>\n",
              "      <td>URBAN BLACK RIBBONS&amp;&amp;SCANDINAVIAN REDS RIBBONS...</td>\n",
              "      <td>CERAMIC CAKE DESIGN SPOTTED MUG</td>\n",
              "      <td>185</td>\n",
              "      <td>100:200</td>\n",
              "      <td>&lt;esp&gt; URBAN BLACK RIBBONS &lt;esp&gt; SCANDINAVIAN R...</td>\n",
              "      <td>&lt;esp&gt; CERAMIC CAKE DESIGN SPOTTED MUG &lt;esp&gt;</td>\n",
              "      <td>&lt;esp&gt; CERAMIC CAKE DESIGN SPOTTED PLATE &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33004.0</td>\n",
              "      <td>60 TEATIME FAIRY CAKE CASES&amp;&amp;KEY FOB , SHED&amp;&amp;G...</td>\n",
              "      <td>GROW YOUR OWN BASIL IN ENAMEL MUG</td>\n",
              "      <td>95</td>\n",
              "      <td>0:100</td>\n",
              "      <td>&lt;esp&gt; 60 TEATIME FAIRY CAKE CASES &lt;esp&gt; KEY FO...</td>\n",
              "      <td>&lt;esp&gt; GROW YOUR OWN BASIL IN ENAMEL MUG &lt;esp&gt;</td>\n",
              "      <td>&lt;esp&gt; GROW YOUR OWN BASIL IN ENAMEL MUG &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35547.0</td>\n",
              "      <td>STRAWBERRY SCENTED SET/9 T-LIGHTS&amp;&amp;WOODEN FRAM...</td>\n",
              "      <td>RED HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>347</td>\n",
              "      <td>300:400</td>\n",
              "      <td>&lt;esp&gt; STRAWBERRY SCENTED SET/9 T-LIGHTS &lt;esp&gt; ...</td>\n",
              "      <td>&lt;esp&gt; RED HANGING HEART T-LIGHT HOLDER &lt;esp&gt;</td>\n",
              "      <td>&lt;esp&gt; PARTY METAL SIGN &lt;esp&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...                                   predict_item\n",
              "0   33113.0  ...      <esp> CHINESE DRAGON PAPER LANTERNS <esp>\n",
              "1   38170.0  ...     <esp> HAND OVER THE CHOCOLATE   SIGN <esp>\n",
              "2   37696.0  ...  <esp> CERAMIC CAKE DESIGN SPOTTED PLATE <esp>\n",
              "3   33004.0  ...  <esp> GROW YOUR OWN BASIL IN ENAMEL MUG <esp>\n",
              "4   35547.0  ...                   <esp> PARTY METAL SIGN <esp>\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdAZ2DZxAg-h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi9-_SPMCzdd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKiTmHCzFF8f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-8k7G_XHYbV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XNNToYkc01B"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEzYKjgLc0x6"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYDh1iP3c0uU"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXJtz7IUc0rM"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvNzgvHgHNB7"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2St4UFVxVqji"
      },
      "source": [
        "# def transformer(vocab_size, num_layers, dff,\n",
        "#                 d_model, num_heads, dropout,\n",
        "#                 name=\"transformer\", stats='train'):\n",
        "\n",
        "#   # 인코더의 입력\n",
        "#   inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "#   # 디코더의 입력\n",
        "#   dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "#   # 인코더의 패딩 마스크\n",
        "#   enc_padding_mask = tf.keras.layers.Lambda(\n",
        "#       create_padding_mask, output_shape=(1, 1, None),\n",
        "#       name='enc_padding_mask')(inputs)\n",
        "\n",
        "#   # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
        "#   look_ahead_mask = tf.keras.layers.Lambda(\n",
        "#       create_look_ahead_mask, output_shape=(1, None, None),\n",
        "#       name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "#   # 디코더의 패딩 마스크(두번째 서브층)\n",
        "#   dec_padding_mask = tf.keras.layers.Lambda(\n",
        "#       create_padding_mask, output_shape=(1, 1, None),\n",
        "#       name='dec_padding_mask')(inputs)\n",
        "\n",
        "#   # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
        "#   enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "#       d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "#   )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
        "\n",
        "#   # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
        "#   dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "#       d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "#   )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "#   # 다음 단어 예측을 위한 출력층\n",
        "#   if stats == 'train':\n",
        "#     outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "# #   elif stats == 'test':\n",
        "\n",
        "\n",
        "#   return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "\n",
        "\n",
        "# def create_padding_mask(x):\n",
        "#   mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "#   # (batch_size, 1, 1, key의 문장 길이)\n",
        "#   return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "# def create_look_ahead_mask(x):\n",
        "#   seq_len = tf.shape(x)[1]\n",
        "#   look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "#   padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
        "#   return tf.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "\n",
        "# def encoder(vocab_size, num_layers, dff,\n",
        "#             d_model, num_heads, dropout,\n",
        "#             name=\"encoder\"):\n",
        "#   inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "#   # 인코더는 패딩 마스크 사용\n",
        "#   padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "#   # 포지셔널 인코딩 + 드롭아웃\n",
        "#   embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "#   embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "#   embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "#   outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "#   # 인코더를 num_layers개 쌓기\n",
        "#   for i in range(num_layers):\n",
        "#     outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "#         dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
        "#     )([outputs, padding_mask])\n",
        "\n",
        "#   return tf.keras.Model(\n",
        "#       inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "  \n",
        "\n",
        "# def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "#   inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "#   # 인코더는 패딩 마스크 사용\n",
        "#   padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "#   # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
        "#   attention = MultiHeadAttention(\n",
        "#       d_model, num_heads, name=\"attention\")({\n",
        "#           'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "#           'mask': padding_mask # 패딩 마스크 사용\n",
        "#       })\n",
        "\n",
        "#   # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "#   attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "#   attention = tf.keras.layers.LayerNormalization(\n",
        "#       epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "#   # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
        "#   outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
        "#   outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "#   # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "#   outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "#   outputs = tf.keras.layers.LayerNormalization(\n",
        "#       epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "#   return tf.keras.Model(\n",
        "#       inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "  \n",
        "\n",
        "# class PositionalEncoding(tf.keras.layers.Layer):\n",
        "#   def __init__(self, position, d_model):\n",
        "#     super(PositionalEncoding, self).__init__()\n",
        "#     self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "#   def get_angles(self, position, i, d_model):\n",
        "#     angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "#     return position * angles\n",
        "\n",
        "#   def positional_encoding(self, position, d_model):\n",
        "#     angle_rads = self.get_angles(\n",
        "#         position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "#         i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "#         d_model=d_model)\n",
        "\n",
        "#     # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
        "#     sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "#     # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
        "#     cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "#     angle_rads = np.zeros(angle_rads.shape)\n",
        "#     angle_rads[:, 0::2] = sines\n",
        "#     angle_rads[:, 1::2] = cosines\n",
        "#     pos_encoding = tf.constant(angle_rads)\n",
        "#     pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "#     print(pos_encoding.shape)\n",
        "#     return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "#   def call(self, inputs):\n",
        "#     return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "\n",
        "\n",
        "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "#   def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "#     super(MultiHeadAttention, self).__init__(name=name)\n",
        "#     self.num_heads = num_heads\n",
        "#     self.d_model = d_model\n",
        "\n",
        "#     assert d_model % self.num_heads == 0\n",
        "\n",
        "#     # d_model을 num_heads로 나눈 값.\n",
        "#     # 논문 기준 : 64\n",
        "#     self.depth = d_model // self.num_heads\n",
        "\n",
        "#     # WQ, WK, WV에 해당하는 밀집층 정의\n",
        "#     self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "#     self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "#     self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "#     # WO에 해당하는 밀집층 정의\n",
        "#     self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "#   # num_heads 개수만큼 q, k, v를 split하는 함수\n",
        "#   def split_heads(self, inputs, batch_size):\n",
        "#     inputs = tf.reshape(\n",
        "#         inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "#     return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "#   def call(self, inputs):\n",
        "#     query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "#         'value'], inputs['mask']\n",
        "#     batch_size = tf.shape(query)[0]\n",
        "\n",
        "#     # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
        "#     # q : (batch_size, query의 문장 길이, d_model)\n",
        "#     # k : (batch_size, key의 문장 길이, d_model)\n",
        "#     # v : (batch_size, value의 문장 길이, d_model)\n",
        "#     # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
        "#     query = self.query_dense(query)\n",
        "#     key = self.key_dense(key)\n",
        "#     value = self.value_dense(value)\n",
        "\n",
        "#     # 2. 헤드 나누기\n",
        "#     # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "#     # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "#     # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "#     query = self.split_heads(query, batch_size)\n",
        "#     key = self.split_heads(key, batch_size)\n",
        "#     value = self.split_heads(value, batch_size)\n",
        "\n",
        "#     # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
        "#     # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "#     scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "#     # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
        "#     scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "#     # 4. 헤드 연결(concatenate)하기\n",
        "#     # (batch_size, query의 문장 길이, d_model)\n",
        "#     concat_attention = tf.reshape(scaled_attention,\n",
        "#                                   (batch_size, -1, self.d_model))\n",
        "\n",
        "#     # 5. WO에 해당하는 밀집층 지나기\n",
        "#     # (batch_size, query의 문장 길이, d_model)\n",
        "#     outputs = self.dense(concat_attention)\n",
        "\n",
        "#     return outputs\n",
        "\n",
        "\n",
        "# def scaled_dot_product_attention(query, key, value, mask):\n",
        "#   # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "#   # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "#   # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "#   # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
        "\n",
        "#   # Q와 K의 곱. 어텐션 스코어 행렬.\n",
        "#   matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "#   # 스케일링\n",
        "#   # dk의 루트값으로 나눠준다.\n",
        "#   depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "#   logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "#   # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
        "#   # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
        "#   if mask is not None:\n",
        "#     logits += (mask * -1e9)\n",
        "\n",
        "#   # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
        "#   # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
        "#   attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "#   # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "#   output = tf.matmul(attention_weights, value)\n",
        "\n",
        "#   return output, attention_weights\n",
        "\n",
        "\n",
        "# def decoder(vocab_size, num_layers, dff,\n",
        "#             d_model, num_heads, dropout,\n",
        "#             name='decoder'):\n",
        "#   inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "#   enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "\n",
        "#   # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
        "#   look_ahead_mask = tf.keras.Input(\n",
        "#       shape=(1, None, None), name='look_ahead_mask')\n",
        "#   padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "#   # 포지셔널 인코딩 + 드롭아웃\n",
        "#   embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "#   embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "#   embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "#   outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "#   # 디코더를 num_layers개 쌓기\n",
        "#   for i in range(num_layers):\n",
        "#     outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "#         dropout=dropout, name='decoder_layer_{}'.format(i),\n",
        "#     )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "#   return tf.keras.Model(\n",
        "#       inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "#       outputs=outputs,\n",
        "#       name=name)\n",
        "  \n",
        "\n",
        "# def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "#   inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "#   enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "\n",
        "#   # 룩어헤드 마스크(첫번째 서브층)\n",
        "#   look_ahead_mask = tf.keras.Input(\n",
        "#       shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "\n",
        "#   # 패딩 마스크(두번째 서브층)\n",
        "#   padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "#   # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
        "#   attention1 = MultiHeadAttention(\n",
        "#       d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "#           'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "#           'mask': look_ahead_mask # 룩어헤드 마스크\n",
        "#       })\n",
        "\n",
        "#   # 잔차 연결과 층 정규화\n",
        "#   attention1 = tf.keras.layers.LayerNormalization(\n",
        "#       epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "#   # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
        "#   attention2 = MultiHeadAttention(\n",
        "#       d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "#           'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
        "#           'mask': padding_mask # 패딩 마스크\n",
        "#       })\n",
        "\n",
        "#   # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "#   attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "#   attention2 = tf.keras.layers.LayerNormalization(\n",
        "#       epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "#   # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
        "#   outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
        "#   outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "#   # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "#   outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "#   outputs = tf.keras.layers.LayerNormalization(\n",
        "#       epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "#   return tf.keras.Model(\n",
        "#       inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "#       outputs=outputs,\n",
        "#       name=name)\n",
        "  \n",
        "\n",
        "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "#   def __init__(self, d_model, warmup_steps=4000):\n",
        "#     super(CustomSchedule, self).__init__()\n",
        "#     self.d_model = d_model\n",
        "#     self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "#     self.warmup_steps = warmup_steps\n",
        "\n",
        "#   def __call__(self, step):\n",
        "#     arg1 = tf.math.rsqrt(step)\n",
        "#     arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "#     return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# def loss_function(y_true, y_pred):\n",
        "#   y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "#   loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "#       from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "#   mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "#   loss = tf.multiply(loss, mask)\n",
        "\n",
        "#   return tf.reduce_mean(loss)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxfKeJx8LTdH"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}