{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer_berttokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1um9DudzORj0fCHBL_nLFIClSurr3rwvV",
      "authorship_tag": "ABX9TyOcp3xpO+7gV1nd4umnh0Eg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e5de485328c45998a366027a9c6f8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fec5035d2f4d4e52b8f42f8a3f3e1857",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_469c01fc6b7248e3a9427e74f0a06e1e",
              "IPY_MODEL_0cdf33a5177f4d9c8af01491a4e698b5"
            ]
          }
        },
        "fec5035d2f4d4e52b8f42f8a3f3e1857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "469c01fc6b7248e3a9427e74f0a06e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_93aecd983bba4f6b9bf8a0f777576252",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d3f23625fed4ab6b625b8e59976a986"
          }
        },
        "0cdf33a5177f4d9c8af01491a4e698b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12d9c2e56c934dedb5cc664ff4040ae0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 121kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d471ead201304ce3ae54c61ea2ac2882"
          }
        },
        "93aecd983bba4f6b9bf8a0f777576252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d3f23625fed4ab6b625b8e59976a986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12d9c2e56c934dedb5cc664ff4040ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d471ead201304ce3ae54c61ea2ac2882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b62b3f7d32413b918d8510c0f89a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17b121dec6a84635bbdaac5d624c03df",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99e300fbf6f54155a79d361cca17472f",
              "IPY_MODEL_afe3b311f37542aa96e1d58351f2098f"
            ]
          }
        },
        "17b121dec6a84635bbdaac5d624c03df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99e300fbf6f54155a79d361cca17472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3c03f115cc84c2cb27a15d3c12b46b0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e9bb6cb5fe04b87a7aa0bf7368d86a3"
          }
        },
        "afe3b311f37542aa96e1d58351f2098f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_acb54a964d504380aab3404d370325b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 69.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26d79e335b0d4577bb33c7140d68543f"
          }
        },
        "d3c03f115cc84c2cb27a15d3c12b46b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e9bb6cb5fe04b87a7aa0bf7368d86a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acb54a964d504380aab3404d370325b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26d79e335b0d4577bb33c7140d68543f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b817fece6cc347b892f4ed1e0df15100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e964aa4d28034c18a734a9a5e6c8ffd2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78ecac05075b44edae010b455d71c5ba",
              "IPY_MODEL_76463f6e942041e5b582713a1a4da226"
            ]
          }
        },
        "e964aa4d28034c18a734a9a5e6c8ffd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78ecac05075b44edae010b455d71c5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_45672213101c46a38d38d79b58459fa9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f99ae78fd32430d83f59e08fd01bdac"
          }
        },
        "76463f6e942041e5b582713a1a4da226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c97c4e9781ba49c7b36e5695615a2040",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 1.36MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f710cac951e34f0e89b64fbe8e008051"
          }
        },
        "45672213101c46a38d38d79b58459fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f99ae78fd32430d83f59e08fd01bdac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c97c4e9781ba49c7b36e5695615a2040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f710cac951e34f0e89b64fbe8e008051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c939ef82cf642378cfe68976ce31bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f9fce561bce4c389483b796ca977198",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1683a54c6a2948b1a37dc924698faf78",
              "IPY_MODEL_5f6425a1bf40462f8abc0ff7780f6784"
            ]
          }
        },
        "8f9fce561bce4c389483b796ca977198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1683a54c6a2948b1a37dc924698faf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1995aa937dd64a0fbe636c8eb7ee7947",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8629f73d46164e6fb86ba30fc8615fa4"
          }
        },
        "5f6425a1bf40462f8abc0ff7780f6784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_928431d359a0477685be431f04b72ccc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:11&lt;00:00, 37.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd99a5305fbf4c87be1a7b1bd654bd6d"
          }
        },
        "1995aa937dd64a0fbe636c8eb7ee7947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8629f73d46164e6fb86ba30fc8615fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "928431d359a0477685be431f04b72ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd99a5305fbf4c87be1a7b1bd654bd6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7db2a14d158644fcab8f84c0d0d31cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a04c887ada2c43bfbb8146162ec6a48e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_335ed7f0aca64a05a95b75918ebffef6",
              "IPY_MODEL_0928efcd6ce7450eb9f781ed5c473762"
            ]
          }
        },
        "a04c887ada2c43bfbb8146162ec6a48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "335ed7f0aca64a05a95b75918ebffef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0c041864da947ebab5b98ab5bb88a9e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b5e64d38a274644be99c7b74ba30ceb"
          }
        },
        "0928efcd6ce7450eb9f781ed5c473762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb4d014715a54d65a5c803ff9d54321f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:08&lt;00:00, 52.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff13fe6b521e4cb5ae2bc1c44ffe8189"
          }
        },
        "e0c041864da947ebab5b98ab5bb88a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b5e64d38a274644be99c7b74ba30ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb4d014715a54d65a5c803ff9d54321f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff13fe6b521e4cb5ae2bc1c44ffe8189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoky1227/bert_based-recommendation/blob/main/Transformer_berttokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPLO_iJO4Nc6",
        "outputId": "de77200a-4d6a-42c5-a8e3-751f3c82b2dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCxrXDm4r8Vl",
        "outputId": "b6e496c0-7393-4b0b-b581-a03841bd9d97"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-transformers\n",
        "# !pip install pytorchtools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=c414a2acfbf46b69451935898e9cd95f5253a8fc58dfa0920835a3f8f2b8f80e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 9.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.43)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/bd/3f9cc87a8faa561903644ec6ef7e7e408ca3640e77c5944124ad6adbaecd/boto3-1.17.39-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Collecting botocore<1.21.0,>=1.20.39\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ad/abdc982cb695a20764df007a2d7cb0ac8964c9591fd014006e40334e4a74/botocore-1.20.39-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 34.5MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.39->boto3->pytorch-transformers) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.39 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.17.39 botocore-1.20.39 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.6 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfRAoKR5s31c"
      },
      "source": [
        "# % pip install sentencepiece\n",
        "# !pip install tokenizers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoNXPUWlWYa"
      },
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSPrB5vgVF7w"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "# import tensorflow_datasets as tfds\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "from torch.utils.data import Dataset\n",
        "# from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import csv"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QwynQI4h4Vbr",
        "outputId": "53d5be42-74dd-4fef-c1fa-48a5ef92220a"
      },
      "source": [
        "path = '/content/drive/MyDrive/'\n",
        "\n",
        "data_bert = pd.read_csv(path + 'train_bert.csv')\n",
        "# data_space = pd.read_csv('/content/drive/MyDrive/train_space.csv')\n",
        "Detail = pd.read_csv(path + 'Detail.csv')\n",
        "data_bert.pop('Unnamed: 0')\n",
        "# data_space.pop('Unnamed: 0')\n",
        "Detail.pop('Unnamed: 0')\n",
        "data_bert = data_bert.sample(frac=1).reset_index(drop=True)\n",
        "data_bert.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>Detail</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34606.0</td>\n",
              "      <td>DAISY FOLKART HEART DECORATION&amp;&amp;ROSE FOLKART H...</td>\n",
              "      <td>BASKET OF FLOWERS SEWING KIT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33874.0</td>\n",
              "      <td>BROWN CHECK CAT DOORSTOP&amp;&amp;CREAM CUPID HEARTS C...</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35182.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;JUMBO SHOPPER...</td>\n",
              "      <td>I'M ON HOLIDAY METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33891.0</td>\n",
              "      <td>PACK OF 60 PINK PAISLEY CAKE CASES&amp;&amp;PACK OF 72...</td>\n",
              "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38251.0</td>\n",
              "      <td>PACK OF 6 SKULL PAPER PLATES&amp;&amp;PACK OF 20 SKULL...</td>\n",
              "      <td>PINK SMALL GLASS CAKE STANDMAXWELL 2 TONE PINK...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...                                              label\n",
              "0   34606.0  ...                       BASKET OF FLOWERS SEWING KIT\n",
              "1   33874.0  ...                 WHITE HANGING HEART T-LIGHT HOLDER\n",
              "2   35182.0  ...                          I'M ON HOLIDAY METAL SIGN\n",
              "3   33891.0  ...                       BAKING SET 9 PIECE RETROSPOT\n",
              "4   38251.0  ...  PINK SMALL GLASS CAKE STANDMAXWELL 2 TONE PINK...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "z2ViS1-cDnj3",
        "outputId": "b3fbdcfe-5bb5-49c3-ca78-d3c88e1fb168"
      },
      "source": [
        "data_bert.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11575.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>35354.741771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1694.022935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>32346.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>33939.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>35298.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>36841.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>38287.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Customer\n",
              "count  11575.000000\n",
              "mean   35354.741771\n",
              "std     1694.022935\n",
              "min    32346.000000\n",
              "25%    33939.000000\n",
              "50%    35298.000000\n",
              "75%    36841.000000\n",
              "max    38287.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgINBUuxys-H",
        "outputId": "04cec101-d22f-4fea-e89a-1b98372e508e"
      },
      "source": [
        "print(len(data_bert))\n",
        "# print(len(data_space))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI-4IVcprfN3",
        "outputId": "e42dbe07-8e13-4f7a-c843-783bbea145ae"
      },
      "source": [
        "label_bert = data_bert['label'].values.tolist()\n",
        "# label_space = data_space['label'].values.tolist()\n",
        "print(len(label_bert))\n",
        "# print(len(label_space))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiJLUIJXjSKF",
        "outputId": "f0c5a8f1-f790-46fc-dcad-0f5dd989282e"
      },
      "source": [
        "detail = Detail['0'].unique()\n",
        "detail = detail.tolist()\n",
        "len(detail)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfDWE9z85od3",
        "outputId": "83ca3bd8-224a-434c-ef02-e7668970a215"
      },
      "source": [
        "dic = {string : i for i,string in enumerate(detail)}\n",
        "# print(dic)\n",
        "len(dic)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbNQ5oDm8VAC",
        "outputId": "4d246eab-e867-4969-9e64-162a0a6565a1"
      },
      "source": [
        "labels = []\n",
        "for v in (label_bert):\n",
        "    labels.append(dic.get(v))\n",
        "print(labels)\n",
        "print(len(labels))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[924, 661, 1442, 424, None, 424, 721, 647, 453, 400, 1299, 158, 283, 1169, 535, 1771, 48, 661, 396, 310, 432, 517, 2165, 180, 1226, 1089, 424, 1029, 727, 2891, 1375, 491, 2900, 1618, 1003, 109, 65, 202, 213, 40, 829, 1599, 199, 307, 243, 314, 1121, 1178, 310, 1668, 121, 1008, 3297, 749, 2253, 183, 1222, 665, 1019, 2196, None, 827, 870, 729, 348, 1457, 1300, 349, 121, 50, 2014, 1424, 2069, 1062, 402, 946, 67, 988, 1612, 342, 1183, 2349, 229, 299, 3033, 190, 1745, 161, 352, 231, 67, 514, 1966, 288, 1618, 727, 1803, 984, 1309, 951, 306, 1264, 657, 1299, 109, 200, 987, 1673, 90, 943, 1943, 117, 947, 158, 1406, 503, 124, 1686, 298, 1929, 1730, 663, 1152, 989, 749, 113, 190, 1822, 1188, 37, 965, 135, 55, 1086, 798, 1195, 2648, None, 765, 1735, 1223, 2403, 1584, 378, 1941, 937, 2223, 1903, 1688, 882, 1941, 186, 570, 649, 661, 29, 125, 361, 592, 158, 2008, 1909, 190, 177, 370, 473, 562, 840, 1229, 1677, 617, 383, 457, 1035, 2, 134, 550, 2306, 130, 1822, 34, 950, 1557, 550, 1143, None, 1103, 399, 248, 539, 573, 347, 1758, 311, 207, 1444, 1677, 1074, 2373, 121, 1085, 294, 722, 665, 908, 1066, 1465, 1543, 1493, 432, 1471, 954, 58, 306, None, 570, 2942, 800, 280, 1221, 701, 244, 189, 989, 1195, 950, 644, 827, 788, 113, 615, None, 614, 562, 306, 1161, 303, 442, 948, 489, 497, 754, 67, 452, 194, 657, 497, 1249, 1614, 2038, 140, 828, 910, 403, 966, 293, 180, 661, 67, 1049, 428, 351, 1049, 195, 947, None, 298, 617, 1532, 56, 636, 617, 617, None, 357, 134, 4, 394, 954, 279, 211, 950, 1435, 935, 121, 1018, 59, 1710, 298, 1509, 1269, 1735, 1680, 221, 198, 50, 490, 189, 258, 183, 808, 1427, 614, 258, 1849, 1228, 1815, 1065, 1686, 319, 1430, 532, 211, None, 999, 274, 1950, None, 453, 349, 1077, 1188, None, 781, 244, 2987, 642, 1081, 200, 306, 1808, 742, 208, 2937, 2009, 359, 224, 1068, 661, 1818, 2373, 2218, 90, 971, 79, 615, 906, 956, 369, 2131, 956, 299, 668, 464, 1178, 123, 299, 1828, 135, 911, 1163, 390, 258, 696, 773, 1086, 619, 294, 411, 47, 0, 53, 661, 1689, 1304, 1928, 451, 274, 391, 1103, 2311, 67, 1735, 1991, 562, 139, 956, 492, 331, 1302, 1623, 425, 331, 930, 989, 653, 352, 27, 636, 948, 1558, 136, 392, 499, 1253, 317, 391, 868, 649, 947, 800, 106, 1403, 553, 2159, 1724, None, 552, 208, 32, 358, 1188, 1409, 331, 137, 1409, 322, 728, 577, 432, 1306, 273, 2815, 59, 1229, 259, 378, 1969, 190, 2774, 1985, 1169, 256, 2353, 317, 640, 643, 435, 937, 947, 661, 323, 1112, 2903, 966, 1735, 1260, 348, 1947, 557, 478, 576, 148, 241, 2075, 544, 1344, 1618, 1231, 1710, 438, 306, 590, 2952, 797, 1663, 1623, 768, 2909, 472, 823, 883, 914, 720, 1062, 620, 956, 1087, 1040, None, 617, 1598, 947, 46, 663, 1231, 293, 2704, None, 1011, 2402, 46, 970, 678, 238, 615, 1065, 949, 1640, 1226, 956, 832, 1745, 1751, 552, 905, 183, None, 1688, 169, 1950, 1396, 1625, 575, 534, 0, 138, 295, 336, 620, 117, 29, 1382, 532, 1557, 1592, 180, 988, 312, 617, 459, 748, 238, 135, 46, 1546, 268, 380, 200, 2229, 682, 327, 1141, 1342, 105, 514, 300, 214, 140, 83, 649, 2697, 49, 50, 2240, 528, 956, 1002, 355, 798, 399, 2602, 1640, 711, 1344, 208, 1465, 1038, 729, 50, 782, 189, 1601, 222, 1687, 133, 112, None, 373, 106, 1848, 1419, 882, 151, 638, 278, 1029, 1663, 2127, 731, 64, 1034, 1301, 1332, 360, 590, 1227, 1066, 48, 827, 1981, 402, 276, None, 236, 370, 2467, 293, 2177, 311, 728, 1435, 2704, 2262, 1954, 1601, 2151, 2014, 1758, 932, 2863, 1601, 1164, 2759, 570, 90, 138, 2602, 3433, 56, 503, 1809, 121, 1713, 768, 452, 958, 2163, 39, 1598, 1066, 842, 592, 661, 661, 794, 192, 424, 113, 1059, 734, 388, 213, 48, 931, None, 1311, 1533, None, 109, 914, 1019, 585, 613, 347, 1033, 1113, 58, 1465, 2174, 2234, 1066, 2139, 68, 1077, 554, 2109, None, None, 162, 1814, 871, 306, 65, 400, 65, 29, 797, 586, 1186, 935, 1892, 665, 792, 59, 355, 653, 1503, 342, 183, 55, 90, 755, 2, 891, 192, 372, 432, 134, 1684, 290, 173, 1174, 29, 484, 549, 1008, 1301, 215, 1758, 114, 440, 109, 1370, 141, 192, 246, 577, 728, 355, 590, 946, 936, 3006, 2248, 539, 993, 403, 2164, 46, 755, 138, 947, 585, 117, 211, 2570, 59, 207, 273, 798, None, 2144, 2325, 213, 57, 1431, 1449, 2628, 404, 195, 300, 1187, 1442, 273, 728, 244, 286, 144, 636, 568, 1249, 489, 1119, 1612, 328, 1617, 781, 1663, 310, 729, 1273, 295, 244, 1132, 1594, 391, 1694, 661, 433, 643, 1299, 386, 114, 283, 115, 1644, 177, 184, 236, 302, 823, 199, 2132, 590, 207, 37, 3113, 244, 468, 1347, 199, 1808, 357, 1191, 955, 2008, 586, 65, 499, 285, 792, 1041, 914, 636, 408, 1725, 827, 456, 1004, 1081, 1880, 1440, 615, 667, 3, 4, 1435, 647, 1895, 999, 2075, 1075, 761, 400, 228, 684, 709, 1500, 307, 661, 256, 402, 1596, 1349, 582, 994, 1796, 527, 905, 988, 706, 112, 99, 321, 1033, 1006, 661, 1195, 1000, 187, 342, 1515, 1457, 2619, 1241, 1000, 723, 323, 65, 2, 688, 30, 988, 661, 562, None, 902, 53, 151, 1220, 1273, 298, 23, 997, 151, 721, 1942, 347, 139, 581, 684, 883, 215, 2366, 661, 2062, 649, 464, 79, 321, 112, 259, 405, 355, 682, 348, 525, 375, 586, 913, 2686, 1695, 926, 997, 1620, 1735, 342, 202, 792, 180, 345, 577, 792, 1901, 349, 506, 1195, 910, 331, 1661, 643, 1049, 440, 798, 376, 166, 128, 2493, 1166, 42, 259, None, 661, 138, 51, 2557, 1528, 189, None, 828, 631, 238, 1745, 499, 1525, 121, None, 906, 185, 1609, 1006, 451, 355, 386, 1713, 2863, 419, 661, 1260, 532, 105, 1332, 941, 828, 577, 1519, 615, 1458, 846, 829, 761, 914, 570, 1526, 276, 1002, 134, 198, 50, 55, 3227, 2212, 1442, 499, 1970, 1273, 0, 1003, 1437, 42, None, None, 947, 2, 475, 1519, 768, 1409, 1222, 488, 357, 274, 492, 632, 2338, 552, 65, 649, 2561, 1543, 1105, 240, 2143, 610, 373, 355, 1686, 310, 1751, 2131, 574, 51, 1323, None, 989, 267, 644, 199, 1396, 2159, 1362, 724, 121, 1735, 1764, 661, None, 195, 139, 1020, 782, 1618, 56, 827, 988, 1931, 130, 1264, 144, 970, 827, 1262, 2352, 783, 946, 144, 47, 590, 215, 776, 615, 1123, 109, 1469, 706, 130, 130, 577, 1041, 1737, 958, 319, 1885, 2744, 224, 532, 259, 575, 682, 954, 517, 310, 227, 42, 1299, 840, 3433, 190, None, 2179, 2089, 121, 899, None, 2783, 313, 67, 914, 402, 468, 399, 663, 385, 196, 682, 1309, 1076, 1199, 1065, 846, 312, 311, 139, 827, 438, 590, 605, 1039, 3268, None, 403, 946, 2571, 53, None, 215, 788, 139, 472, 380, 1010, 1037, 1318, 1673, 39, 457, 661, 53, 231, 866, 911, 591, 1694, 622, 729, 515, 180, 674, 1167, 1714, 2845, 183, 1419, 531, 2035, 1091, 184, 746, 655, 121, 932, 352, None, 90, 196, 130, 1112, 190, 247, 2942, 195, 711, 352, 153, 1178, 1006, 499, 482, 58, 336, 170, 1908, 869, 600, 3022, 1969, 1820, 267, 56, 0, 1406, 894, 1039, 386, 1106, 198, 468, 1222, 46, 106, 1105, 562, 277, 65, 673, 665, 178, 832, 1451, 2105, 1437, 1612, 66, 949, 295, 1123, 278, 87, 1262, 130, None, 198, 318, 806, 1070, 400, 130, 389, 615, 972, 792, 2342, 186, 1062, 705, 624, 343, 443, 29, 197, 192, 1610, 290, 36, 1599, 630, 588, 306, 731, 859, 197, 37, 1677, 950, 2270, 2678, 1006, 212, 600, 434, 484, 494, 333, 647, 953, 508, 989, 989, 295, 79, 2075, 311, 717, 2230, 1706, 444, 649, 1603, 717, 661, 936, 1299, 403, 728, 524, 215, 761, 1877, 1223, 1604, 1002, 1535, 2072, 1406, 1765, 111, 614, 566, 277, 443, 189, 41, 532, 1872, 310, 742, 491, 1049, 524, 272, 298, 661, 159, 733, 2243, 2821, 1681, 1295, 58, 1815, 1461, 661, 491, 717, 564, 797, 302, 914, 1085, 883, 2512, 857, 299, 848, 956, 1227, 274, 1240, 1090, 133, 1815, None, 1017, 914, 203, 441, 914, 207, 1195, 615, None, 958, 355, 661, 814, 267, 256, 881, 2325, 1965, 198, 782, 440, 306, 2822, 1430, 58, 905, None, None, 149, 1771, 450, 130, 34, 1255, 3288, 136, 1332, 1195, 2136, 502, 452, 518, 208, 1519, 302, 610, 881, 125, 781, 617, 477, 1437, 236, 2199, 332, 848, 1020, 568, 562, 1268, 444, 473, 829, 1558, 610, 1072, 273, 2226, 379, 379, 90, 457, 1610, 1273, 372, 633, 453, 1918, 456, 495, 190, 679, 684, 106, 322, 294, 1300, 1187, 1516, 539, 619, 891, 400, 124, 1560, 491, 204, 1, 1286, 479, 619, 90, 403, 562, 1735, 500, 728, 653, 653, 511, 189, 195, 457, 3028, 1373, 258, 706, 158, 905, 1428, 2055, 1047, 3341, 808, 1227, 827, 1745, 947, 800, 566, 940, 502, 914, 1953, 1268, 684, 792, None, 211, 425, 1745, 244, 259, 914, 720, 453, 1018, 448, 394, 247, 286, 2065, 614, 135, 930, 1769, 792, 2503, 866, 1120, 503, 1623, 1598, 661, 146, 190, 1524, 196, 1990, 277, 1718, 38, 950, 367, 947, 1849, 189, 435, 1985, 243, 453, 192, 1789, 130, 1695, 1977, 403, 459, 215, 538, 184, None, 1686, 729, 661, 321, None, 500, 441, 768, 645, 2470, 1217, 1029, 207, 357, 1807, 2350, 545, 343, 360, 2295, 443, 848, 2393, 1352, 303, 950, 866, 1087, 301, 790, 950, 965, 705, 212, 642, 663, 277, 287, 121, 401, 1187, 352, 984, 1981, 1212, 1599, 1195, 651, 68, 1332, 1362, 352, 236, 616, 1004, 1299, 443, 456, 126, 521, 888, 829, 1598, 376, 411, 1489, 294, None, 213, 985, 66, 1312, 953, 384, 2001, 450, 299, 142, 296, 115, 67, 1706, 1908, 765, 1519, 152, None, 570, 61, 293, 367, 940, 823, None, 347, 2270, 1196, 2664, 1647, 385, 296, 2164, 2427, 949, 907, 523, 558, 376, 154, 114, 930, 942, 59, 636, 1337, 840, 2844, 317, 316, 569, 649, 1571, 784, 309, 1020, 263, 273, 294, 882, 424, 1004, 46, 936, 2403, 139, 665, 258, 819, 136, 311, 385, 289, 552, 756, 236, 1243, 642, 1954, 564, 1869, 1599, 1667, 2482, None, 1596, 629, 41, 1614, 452, 159, 29, 134, 997, 1274, 3094, 278, 267, 1087, 401, 1816, 2294, 531, 431, 124, 1, 309, 214, 918, 63, 696, 1503, 149, 268, 17, 1869, 2280, 2325, 139, 866, 2177, 1048, 207, 190, 69, 1231, 279, 402, 307, 215, 1803, 2125, 956, 170, 684, 133, 891, 2022, 332, 46, 113, None, 1487, 650, 139, 1067, 1954, 1524, 630, 1195, 1620, 1043, 3234, 368, 1610, 261, 328, 160, 1709, 3, 1714, 682, 400, 617, 139, 130, 1187, 609, 215, 723, 727, 1598, 36, 1106, 211, 989, 577, 1008, 1519, 1222, 2574, 293, 661, 792, 2199, 300, 1440, 432, 1304, 1440, 1599, 133, 661, 1221, 1616, 1286, 321, 1640, 1002, 1248, 376, 500, 727, 1908, 1153, 580, 955, 1302, 192, 649, 781, 334, 1311, 385, 3316, 195, 1435, 1311, 358, 343, 403, 179, 55, 791, 184, 682, 1, 523, 587, 1062, 465, 1143, 1449, 1091, 1816, None, 306, 1242, 930, 180, 278, 1086, 617, 386, 1467, 383, 617, 1747, 109, 355, 1546, 950, 67, 781, 827, 1312, 431, 278, 1413, 883, 124, 3020, 213, 2008, 1233, 883, 1699, None, 1420, 644, 617, 989, 2630, 1018, 619, 327, 1286, 885, 1502, 146, 59, 113, 604, 317, 782, 729, 111, 1675, 2321, 403, 170, 1049, 628, 65, 458, 2, 2044, 256, 400, 190, 438, 635, 1085, 2192, 1467, 1494, None, 661, None, 682, 2208, 484, 2482, 1181, 63, 1412, 1288, 299, 615, 1299, 1376, 413, 187, 1427, 2492, 180, 684, 456, 1179, 1751, 1232, 1020, 570, 29, 672, 1233, 1299, 400, 622, 1524, 312, 1457, 245, 1025, 187, 1418, 1930, 600, 649, 1068, 1264, 230, 499, 3171, 619, 511, 636, 1713, 715, 2139, 0, 145, 827, 192, 249, 362, 575, 1344, 92, 456, 425, 107, 1946, None, 177, 385, 180, 636, 661, 720, 177, 86, 590, 1745, 661, 3194, 140, 1620, 212, 2384, 3350, 557, 1852, 1385, 61, 465, 46, 1034, 1300, 955, 1713, 705, 788, 1302, 576, 792, 1803, 259, 258, 119, 1553, 857, 534, 717, 3298, 534, 2174, 1599, 797, 1499, 1442, 1651, 612, 999, 1640, 407, 655, 620, 1767, 2264, 32, 827, 731, 661, 549, 1016, 566, 2810, 1745, 1143, 259, 256, 400, 948, 1885, 88, 997, 377, 67, 665, 256, 2593, 571, 1170, 67, 124, 144, 914, 32, 1486, 800, 1735, 386, 472, 827, 511, 608, 2, 729, 320, 731, 394, 2073, 2404, 403, 1264, 1377, 514, 1166, 1216, 2221, 717, 334, 1186, 114, 236, 499, 3107, 2253, 790, 911, 46, 2040, 391, 554, 1299, 1299, None, 190, 196, 1869, 190, 168, 1261, 2551, 53, 756, 295, 2053, 1152, 1952, 579, 2628, 1815, 212, 1647, 1053, 948, 472, 409, 1066, 56, 55, 59, 500, 729, 1273, 684, 941, 1072, 3190, 536, 2044, 684, 883, 731, 1467, 797, 2132, 215, 135, 408, 1063, 461, 55, 1028, 1328, 45, 53, 1038, 749, 1114, 1461, 684, 294, 1300, 931, 1220, 400, 273, 1195, 869, 827, 424, 1437, 2035, 215, 661, 2926, 2199, 2121, 68, None, 768, 947, 562, 255, 378, 832, 989, 159, 1116, 207, 2972, 1575, 3, 1399, 984, 50, 1814, 492, 1442, 947, 215, 307, 823, 47, None, 144, 3394, 2745, 1244, 1625, 2181, 381, None, 728, 211, 68, 215, None, 1892, 106, 2164, 502, 613, 1020, 452, 1246, 1745, 1200, 29, 451, 456, 604, 259, 936, 286, 117, 513, 196, 420, 614, 407, 286, 829, 558, 1805, 619, 649, 1687, 731, 1612, 275, 1006, 468, 1599, 318, 1989, 301, 1495, 999, 306, None, 215, 418, 772, 1007, 29, 435, 1357, 545, 1735, 331, 1495, 207, 787, 32, 199, 577, 125, 192, 1594, 521, 592, 615, 1234, 185, 949, 1815, 245, 111, 577, 2128, 418, 1997, 728, 814, 452, 612, 1437, 247, 352, 86, None, 1074, 787, 1041, 1880, 46, 1018, 230, None, 1292, 905, 797, 278, 1299, 432, 952, 3201, 908, 1442, 2817, 370, 50, 1273, 1240, 684, 519, 303, 2256, 109, 2745, 661, 1249, None, 729, 1594, 552, 431, 882, 343, 615, 2214, 37, 2165, 881, 1465, 1879, 36, 800, 2427, 319, 317, 391, 649, None, 722, None, 195, 65, 49, 341, 710, 189, 370, 178, 1686, 827, 2418, 200, 829, 403, 647, 776, 189, 711, 396, 230, 420, 607, 79, 461, 780, 478, 1008, 29, 866, 569, 697, 66, 1348, 352, 308, 620, 932, 534, 404, 500, 357, 885, 955, 443, 745, None, 577, 2144, 27, 988, 451, 177, 1771, 114, 385, 1502, 1188, 2265, 1004, 1809, None, 576, 124, 928, 1885, 2, 158, 310, 393, 204, 203, 859, 1148, 385, 399, 1424, 65, 1623, 1416, 556, 399, 548, 54, 33, 1123, 942, 114, 969, 2962, 317, 2046, 1013, 2718, 556, 949, 1735, 29, 782, 974, 386, 697, 943, 504, None, 420, 1299, 1457, 379, 1704, 357, 1043, 1247, 672, 541, 332, 651, 1085, 147, 1033, 207, None, 503, 2929, 508, 799, 1853, 374, 590, 381, 196, 940, 280, 3227, 144, 36, 197, 500, 1952, 471, 394, 195, 90, 870, 2121, 819, 430, 722, 577, 1065, 1437, 154, 1152, 468, 496, 2882, 418, 432, 139, 1195, 46, 1305, 1269, 1942, 400, 522, 37, 1178, 722, 914, 480, 1969, 727, 184, 479, 512, 121, 483, 2181, 236, 144, None, 1889, 2035, 1941, 1309, 1049, 346, 2248, 399, 121, 294, 309, 2657, 649, 1645, 1065, 551, 711, 2174, 1484, 534, 297, 135, 947, 33, None, 275, 495, 1088, 2016, 352, 2321, 871, 661, 806, 1469, 236, 941, 668, 3311, 478, 399, 212, 2227, 1885, 672, 827, 1942, 539, 323, 452, 47, 278, 2465, 504, 905, 88, 2181, 312, 1528, 1059, 1986, 1456, 438, 2392, 1571, 888, 310, 947, 1324, 138, 30, 1188, 106, 1648, 949, 180, 399, 782, 940, 335, 391, 724, 234, 212, 1006, 1731, 221, 843, 334, 2593, 777, 253, 222, 248, 534, 400, None, 800, 34, 2525, 1237, 727, 619, 1774, 376, 503, 1066, 784, 502, 1049, 2, 668, 926, 2308, 499, 411, 67, 862, 2482, 114, 3165, None, 211, 312, None, 754, 1342, 1226, 319, 876, 438, 320, 2062, 936, 1967, None, 458, 139, 827, 109, 1019, 1066, 1068, 932, 187, 357, 2193, 362, 764, 32, 1195, 857, 139, None, 761, 562, 277, 156, 1867, 1011, 586, 682, 191, 1299, 1663, 723, 1, 204, 263, 448, 317, 4, 1509, 306, 48, 2709, 438, 1242, 2177, 149, 1029, 504, 789, 360, None, 433, 661, 253, 3, 346, 816, 248, 1534, 484, 53, 111, 3285, 357, 928, 1689, 1437, 2199, 307, 560, 1686, 777, 236, 3200, 219, 1495, 197, 495, 1457, 446, 195, 828, 128, 2157, 1693, 432, 2141, 971, 1222, 1299, 1088, 2227, 312, 1612, 323, 661, 134, 483, 393, 989, 988, None, 213, 64, 306, None, 507, 53, 502, 1849, 412, 765, 453, 498, None, 1224, 649, 1302, 834, 300, 1618, 277, 597, 378, 246, 1467, 1741, 539, 2671, 3070, 81, 276, 278, 133, 730, 135, 2073, 106, 2811, 67, 586, 258, 569, 140, 465, 1818, 48, 781, 2353, 402, 443, 649, 256, 1557, 590, 617, 1442, 272, 1877, 329, 438, 1612, 411, 1348, 1751, 638, 400, 913, 1880, 51, 1255, 273, 2165, 1409, 310, 195, 3006, 911, 99, 401, 840, 531, 781, 31, 2119, 432, 400, 503, 108, 144, 429, 958, 396, 806, 295, 109, 989, 523, 1987, 792, 799, 117, 2174, 1002, 1304, 77, 400, 1849, 711, 942, 113, 959, 519, 430, 1434, None, 661, 829, 1694, 312, 297, 409, 1606, 549, 32, 954, 130, 54, 1188, 1540, 453, 1269, 293, 1006, 311, None, 434, 57, 1393, 595, 444, 840, 482, None, 400, 400, 728, 649, 1612, 1707, 237, 891, 400, 549, 320, 575, 1206, 2255, 385, 47, 1179, 819, 661, 267, 2267, 1059, 483, 989, 255, 661, 783, 714, 601, 436, 1883, 159, 409, 392, 1002, 2942, 617, 30, 717, 947, 1399, 2195, 777, 3033, 1500, 562, 3, 155, 117, 764, 1062, 264, 831, 604, 2602, 1241, 2035, 2174, 385, 1840, None, 513, 590, 212, None, 582, 190, 720, 828, 56, 325, 41, 411, 49, 1699, 27, 932, 866, 1115, 569, 729, 177, 619, 759, 56, 1068, 2139, 1003, 50, 1144, 661, 697, 1848, 1299, 828, 331, 2923, 858, 1599, 522, 148, 577, 375, 317, 80, 231, 1983, 1219, 547, 619, 2684, 491, 1065, 2349, 974, 962, 1875, 319, None, 987, 325, 316, 185, 1120, 46, 1465, 1571, 177, 513, 2103, 667, 1066, 278, 642, 349, 203, 906, 221, 617, 1576, 391, 131, 1562, 2392, 2007, 346, 1047, 1188, 262, 932, 130, 988, 892, 155, 932, 1018, 352, 2503, 481, 661, 308, 870, 438, 2008, 661, 2353, None, 253, 653, 2075, 684, 40, 274, 539, 615, 189, 1188, 344, 1082, 378, 1085, 370, 196, 1077, 989, 351, 108, 106, 2353, 121, 207, 1727, 944, 647, 140, 612, 255, 55, 1809, 439, 784, 559, 1118, 299, 214, 109, 1880, 1002, 844, 223, 829, 1264, 273, 1382, 87, 403, 1442, 1249, 440, 2923, 344, 405, 224, 2008, 195, 408, 84, 1818, 306, 311, 169, 1616, 2616, 657, 1725, 432, 386, None, 287, 2560, 2150, 1195, 177, 759, 728, 1640, 432, 1796, 1393, 237, 196, 408, 1004, 483, 162, 609, 1941, 597, 3230, 1212, 1332, 1487, 580, 848, 1306, 1025, 2345, 1393, 1286, 2212, 456, 311, 200, 1062, 776, 622, 199, 1705, None, 294, 1216, 482, 955, 615, 236, 610, 1229, 256, 1143, 617, 905, 1526, 974, 193, 866, 112, 1257, 696, 1509, 661, 2046, 882, 264, 1598, 831, 130, 806, 83, 947, 121, 1651, 1188, 1687, 1033, 1221, 1334, 471, 211, 756, 443, 305, 306, 649, 453, 491, 629, 1747, 412, 419, 1072, 1017, 30, 484, 661, None, 46, 464, 405, 2311, 29, 443, 1892, None, 135, 513, 954, 1977, 293, 306, 144, 853, 214, 962, 428, 1486, 1210, 141, 1990, None, 680, 370, 121, 2293, 1062, 2059, 68, 3, 400, 49, 1143, 473, 1873, 198, 1058, 723, 1568, 1442, 482, 257, 590, 697, 2109, 268, 1020, 403, 139, 1081, 55, 1241, 1526, 1665, 312, 190, 1133, 180, 372, 800, 432, 503, 1374, 134, 1695, 881, 432, 65, 1077, 403, 2710, 1188, 554, 711, 38, 411, 1942, 2255, 1598, 149, 684, 881, 400, 1085, 29, 649, 647, 2271, 955, 2903, 121, 424, 1019, 159, 134, 1186, 199, 1206, 152, 1268, 1065, 1598, 2100, 612, 138, 672, 2247, 3, 932, 1931, 1373, 742, 1424, 319, 428, 385, 2422, 1596, 827, 1761, 215, 649, 948, 507, 782, 1620, 1767, 797, 854, 321, 1992, 617, 2582, 319, 343, 1435, 1063, 3, 1614, 1735, 2616, 1818, 1984, 2627, 491, 1062, 168, 43, 1088, 610, 955, 276, 553, 136, 906, 1767, 1523, 190, 1020, 320, 294, 797, None, 1434, 306, None, 1167, 2876, 831, 412, 196, 405, 684, 1195, 2166, 50, 1249, 655, 1508, 184, 201, 988, 1026, 619, 490, 419, 1798, 948, 649, 3298, 2007, 55, 221, 2902, 590, 1009, 1113, 1280, 955, 1040, 612, 644, 673, 772, 362, 327, 287, 610, None, 321, 1085, 99, 378, 244, 300, 546, 202, 823, 2321, 404, None, 277, 1003, 1375, 395, 1002, 661, 1229, 457, 1144, None, 1501, 819, 1233, 1614, 661, 1815, 906, 1240, 947, 614, 2617, 111, 512, 610, 2349, 667, 425, 869, 500, 256, 846, 714, 113, 268, 46, 1040, 2242, 67, 446, 370, 160, 319, 124, 195, None, 2400, None, 395, 2174, None, 183, 969, 848, 263, 3029, 914, None, 661, 114, 65, 399, 1880, 31, None, 1686, 710, 491, 208, 1805, 1707, None, 135, 200, 432, 628, 460, 275, 460, 244, 134, None, 134, 1515, 1686, 2009, 1025, 854, 884, 683, 946, 3016, 515, 402, 480, 285, 136, 513, 1260, 197, 577, 479, 401, 1885, 636, 195, 200, 2165, 914, 881, 411, 1491, 3, 1377, 312, 1065, 1981, 612, 1072, 1519, 111, 50, 380, 641, 823, 59, 151, 221, 950, 303, 1872, 207, 1771, 59, 681, 871, 997, 1957, 2574, 790, 1295, 185, 684, 2619, 629, 121, 1610, 1303, 1076, 557, 651, 1462, 283, 891, 370, 64, 764, 731, 557, 204, 403, 311, 400, 200, 914, 947, 1435, 936, 312, 1074, 2179, 882, 486, 122, 2792, 722, 551, 60, 244, 267, 207, 1599, 399, 1190, 128, 1731, None, 58, 566, 286, 40, 632, 432, 1504, 294, 1112, 55, 1890, 1280, 449, 661, 1303, 1064, 1019, 196, 303, 1348, 59, 458, 432, 1735, 1598, 1849, 1484, 1723, 207, 303, 1949, 386, 34, 576, 629, 293, 1301, 1017, 479, 130, 728, 636, 459, 3, 55, 1500, 1679, 957, 2162, 204, 380, 604, None, 109, 304, 2669, 2305, 1062, 914, 1161, 150, 663, 23, 840, 1571, 417, 857, 1066, 370, 1735, 447, 1205, 124, 412, 731, 180, 55, 777, 728, 424, 932, 526, 614, 135, 47, 215, 1489, None, 1016, 1771, 2347, 121, 310, 1241, 2096, 797, 559, 811, 65, 781, 2056, 343, 310, 48, 932, 383, 609, 41, 298, 954, 914, 1174, 1066, 1877, 278, 827, 55, 936, 949, 535, 60, 1751, 585, 649, 2078, 340, 341, 765, 1805, 2893, 151, 1178, None, 806, 2023, 1773, 1437, 1114, 333, 306, 214, 199, 2121, 545, 113, 47, 2174, 914, 1557, 158, 139, 657, 932, 319, 2482, 2008, 425, 305, 697, 41, 2332, 946, 204, 472, 1849, 538, 194, 939, 317, 213, 759, 443, 781, 781, 597, 30, 1755, 352, 1997, 1068, 1264, 443, 106, 1437, 1068, 1623, 192, 2161, 1264, 243, 574, 2688, 130, 495, 322, 781, 195, 2073, 1246, 1004, 386, 117, 614, 376, 1028, 1957, 1997, 614, 643, 211, 2455, 1063, 792, 614, 513, 1043, 866, 2810, 534, 1311, 1105, 894, 256, 843, 908, None, 481, 432, 736, 134, 953, 2949, 1114, 614, 1596, 581, 1740, 2392, 407, 2427, 792, 268, 1231, 832, 1485, 58, 407, 109, 281, 661, 211, 128, 144, 267, None, 1085, 2867, 620, 54, 883, 1026, 1269, 800, 306, 1815, 334, 1803, 205, 1981, 195, 195, 306, 400, 191, 649, 402, 1216, 628, 30, 1800, 384, 661, 1693, 401, 183, 716, 1849, 115, 199, 531, 332, 1599, 613, 525, 789, 1825, 349, 951, 661, 755, 950, 1066, 950, 1687, 586, 1380, 562, 513, 932, 615, 661, 259, 2001, 1066, 1303, 276, None, 939, 118, 743, 206, 2075, 230, 1985, 793, 566, 62, 956, 1819, 294, 1041, None, 27, 1418, 1598, 1885, 1188, 112, 500, 341, 31, 313, 220, 1370, 2181, 1724, 248, 1314, 1251, 1085, 1434, 294, 58, 1554, 96, 68, 352, None, 407, 1249, 253, 208, 384, 643, 477, 1892, 195, 828, 1223, 1233, 1183, 829, 195, 1786, 1617, 56, 207, 674, 199, 286, 1062, 121, 442, 792, 1442, 883, 829, 723, 871, 41, 31, 382, 322, 716, 1, 1066, 59, 989, 2193, 465, 55, 2007, 402, 1434, 1758, 1057, 643, 2555, 453, 1437, 1105, 195, 390, 1604, 769, 273, 258, 76, 894, 125, 1485, None, 106, 715, 947, 2628, 1897, 661, 2220, 1361, 199, 947, 447, 195, 1009, 442, 989, 264, 2314, 380, 190, 190, 2096, 1449, 866, 322, 499, 661, 264, 562, 400, 1123, 1049, 187, 947, 142, 941, 386, 170, 310, 161, 478, 570, 955, 326, 452, 486, 53, 1354, 201, 1696, 139, 122, None, 385, 1879, 1002, 1215, 2139, 1003, 590, 1437, 360, 388, 399, 1295, 661, 39, 1452, 1081, 1264, 402, 237, 1316, 1286, 133, 1305, 661, 1367, 729, 1728, 950, 1614, 2465, 882, None, 1877, 806, 258, 590, 2323, 350, 777, 620, 1233, 30, 535, 1005, 211, 391, 3070, 1242, 189, 130, 1981, 1733, 475, 310, None, 354, 739, 706, 1047, 29, 1187, 1274, 534, 3022, 716, 355, 30, 1112, 797, 1337, 914, 1508, 945, 178, 255, 121, 316, 786, 255, 1981, 37, 896, 881, 950, 307, 3110, 783, 1889, 716, 615, 450, 132, 610, 359, 33, 527, 2404, 1909, 432, 317, 522, 661, 402, 50, None, 1849, 756, 1004, 237, 41, 1186, 1131, 882, 613, 331, 1026, 653, 131, 829, 1128, 2528, 661, 782, 988, 151, 309, 1294, 1540, 661, 1257, 1591, 1561, 63, 1241, 696, 1144, 2053, 185, 393, 1877, 0, 867, 825, 310, None, 910, 1885, 432, 109, 243, 1226, 480, 937, 1004, 255, 2844, 29, 661, 2022, 138, 1020, 342, 661, 1087, 782, 930, 55, 709, None, 1815, 2429, 1523, 1406, 456, 590, 970, 502, 1971, 956, 636, 113, 195, 557, None, 1064, None, 932, 792, 272, 819, 1740, 756, 928, 697, 132, 545, 1745, 775, 1348, 2, 2680, 355, 146, 64, 2407, 509, 144, 1849, 482, 1143, 943, 1987, 439, 1599, 130, 186, 873, 1106, 2496, 1062, 114, None, 29, 298, 1229, 400, 340, 2162, 954, 300, 2242, 288, 69, 3, 471, 1476, 1590, 2127, 955, 54, 1006, 184, 617, 424, 34, 827, 135, 347, 199, 1273, 989, 1227, 1286, None, 108, None, 3353, 615, 1, 1286, None, 709, 119, 727, 1922, 132, 1066, 430, 273, 636, 42, 133, 483, 619, 1773, 1348, 2267, 110, 2075, 298, 2007, 2140, 673, 87, 112, 313, 869, 1499, 1614, 139, 320, 2691, 832, 253, 2373, 189, 409, 2109, 295, 2256, 1436, 1435, 1437, 1877, 121, 655, 502, 2573, 681, 2109, 2196, 236, 1502, 1187, 68, 1326, 517, 576, 1745, 121, 780, 862, 2403, 1328, 1767, 615, 196, None, 563, 1003, 1798, 2, 171, 158, 150, 179, 417, 2835, 782, 296, 1485, 400, 661, 42, 14, 128, 3145, 2221, 987, 64, 643, 510, 294, 401, 610, 808, 781, 380, 2371, 394, 513, 828, 2060, 636, 499, 617, 1648, 144, 923, 2491, 796, 911, 548, 130, 196, 1337, None, 197, 558, 2416, 234, None, 636, 298, 61, 2072, 673, 1400, 706, 1648, 962, 536, 1625, 55, 1223, 1916, 299, 1798, 2035, 2270, 199, 198, 592, 755, 350, 627, 306, 1835, 1515, 1516, 234, 120, 288, 496, 1456, 263, 0, 1183, 1286, 2318, 1011, 300, 661, 1188, 1434, 784, 936, 954, 1079, 833, 1612, 535, 1268, 379, 310, 1088, 1519, 633, 1665, 2416, 840, 661, 640, 550, 2686, 590, 272, 90, 60, 519, 1114, 391, 2219, 1533, 380, 1588, 541, 736, 2400, 146, 2571, 317, 783, 1745, 151, 41, 713, 987, 320, 391, 1571, 2845, 114, 158, 1216, 1009, 1773, 613, 50, 2733, 800, 2006, 491, 947, 881, 647, 134, 956, 1740, 1599, 1287, 1291, 896, 1437, 403, None, 2103, 196, 661, 274, 958, 400, 792, 534, 1395, 2528, 481, 923, 93, 178, 307, 857, 2121, 937, 619, 519, 1566, 424, 236, 827, 447, 1606, 211, 282, 453, 1735, 936, 306, 1229, 539, 109, 1224, 52, 2768, 947, 461, 936, 425, 2443, 2046, 1062, 968, 825, 2020, 362, 134, 239, 1609, 375, 1034, 168, 311, 117, 1436, 374, 306, 661, 781, 114, 569, 786, None, 277, 1598, 2062, 1854, 496, 3, 1164, 617, 2477, 461, 746, 2684, 619, 2514, 307, 139, 1, 560, 189, 950, 133, 640, 189, 868, 96, 215, 1300, 1724, 615, 211, 134, 1187, 2236, 1195, 438, 133, 435, 311, 383, 357, 478, 2174, 1995, 1348, 1189, 444, 116, 215, 1815, 535, 989, 885, 1329, 761, 1885, 492, 649, 948, 1707, 430, 1243, 503, 195, 296, 1736, None, 344, 196, 295, 590, 2007, 651, 1072, 866, 1526, 55, 135, 1305, 1230, 1613, 1067, 2044, 862, None, 199, 342, 106, 829, 456, 3278, 1970, 456, None, 615, 185, 171, 194, 374, 1523, 369, 2772, 882, 1526, 936, 267, 355, 948, 394, 1049, 781, 1592, 614, 239, 2035, None, 499, 619, 970, 729, 215, None, None, 1941, 267, 349, 1943, 110, 58, 944, 315, 53, 691, 301, 400, 1062, 717, 160, 192, 1066, 334, 162, 114, 1663, 456, 152, 394, 59, 2820, 319, 314, 353, 636, 556, 46, 1875, 194, 1228, 2619, 729, 352, 1406, 159, 318, 179, 2363, 253, 278, 1444, 781, 1509, 451, 125, 281, 273, 1640, 290, 1072, 306, 383, 2139, 114, 37, None, 83, None, None, 405, 1062, 109, 826, 2482, 823, 208, 160, 117, 3133, 661, 728, 672, 800, 2575, 1243, 784, 411, 178, 2244, 433, 481, 1311, 2008, 1269, None, 1601, 661, 649, 1429, 55, 263, 294, 394, 561, 1035, 1640, 49, 808, 292, 1348, 514, 1106, 1074, 661, 547, 67, 199, 968, 386, 1334, 555, 192, 204, 1286, 1577, 2402, 381, 1062, 1435, 148, 1560, 1678, 141, 642, 1057, 1057, 1260, 1679, 141, 632, 1918, 774, 400, 546, 950, 2463, 1598, 308, 3089, 238, 215, 2148, 404, 473, 399, 2306, 1429, 930, 2756, 328, 312, 29, 696, 185, 2560, 109, 1603, 383, 715, 1561, 1977, 1269, 1957, 1767, 561, 867, 730, 1562, 263, 3277, 821, 1816, 2740, 642, 1251, 1942, 1246, 851, 1596, 589, 385, 1184, 331, None, 504, 117, 706, 1385, 495, 726, 349, 193, 142, 1231, 438, 2015, 4, 1188, 590, 307, 1981, 894, 171, 2774, 615, 84, 642, 672, 714, 1214, 1174, 521, 192, 122, None, 403, 1274, 1877, 213, 249, 757, 1740, 2740, 1255, 200, 2119, 364, 604, 509, 258, 1121, 194, 370, 121, 311, 534, 1449, 1041, 1519, 108, 950, 106, 1553, 177, 400, 403, 357, 1693, 536, 340, 955, 1263, 385, 669, 519, 1089, None, 491, 1667, 115, 154, 207, 1440, 1226, 2386, 709, 79, 294, None, 529, 1105, 306, 464, 320, 661, None, 385, 461, 336, 331, 197, 513, 894, 1706, 307, 1596, 2219, 1966, 451, 792, 590, 711, 320, 380, 38, 3298, 66, 1010, 1196, 1820, 999, 284, 211, 1970, 1596, 319, 567, 1526, 55, 558, 548, 1148, 1970, 2464, 947, 1085, 400, 199, 628, 545, 1807, 809, 1105, 1195, 2199, 434, 2404, 311, 516, 2213, 814, 1169, 630, 171, 1771, 550, 2659, 1519, 16, 1105, 866, 113, 797, 994, 1612, 196, 1616, 1300, 527, 130, 1771, 1043, 3234, 168, 303, 1090, 577, 322, 502, 1798, None, 989, 121, 1, 577, 248, 2130, 376, 552, 110, 518, 524, 1613, 1264, 717, 2230, 706, 1814, 1233, 1640, 969, 759, 756, 153, 84, 199, None, 729, 1465, 570, 1006, 2248, 121, 2061, 114, 590, 306, 882, 2320, 313, 522, 331, 729, 2193, 789, 661, 1735, 1029, None, 1880, 988, 27, 3316, 706, 1062, 731, 949, 121, 1123, 1693, 204, 213, 1401, 357, 1040, 607, 1504, 135, 54, 1059, 2416, 2151, 911, 1829, 355, 525, 133, 1473, 148, 2007, 1611, 377, 683, 185, 2033, 348, 281, 276, 661, 2165, 1682, 2066, 1684, 189, None, 152, 258, 944, 1584, 1767, 653, 2306, 189, 2872, 911, 1106, 696, 1526, 1062, 352, 792, 303, 701, 1205, 1527, 117, 460, 1612, 319, 63, 1526, 135, None, 453, 696, 1598, None, 132, 334, 326, 797, 3029, 2841, None, 649, 2027, 949, 635, 51, 523, 791, 792, 617, 1854, 1081, 2952, 1084, 399, 562, 1693, 1221, 106, 2, 1023, 968, 1591, 1269, 238, 471, 432, 1377, 777, 1266, 1062, 385, 954, 670, 1201, 1701, 966, 1995, 1648, 61, 391, 247, 869, 1017, 523, 197, 306, 1952, 1825, 155, 644, 955, 1016, 449, 190, 609, 570, 2019, 240, 194, 850, 2247, 790, 785, 605, 239, 645, None, 1091, 171, 423, 937, None, 3133, 492, 1981, 391, 380, 314, 211, 1872, 366, 642, 2242, 1465, 613, 81, 949, 132, None, 130, 484, None, 614, 146, 153, 151, 2248, 940, 2355, 263, 974, 407, 1888, 936, 52, 162, 911, 197, 792, 296, 108, 532, 2, 777, 130, 2531, 2150, 2361, 522, 435, 2253, 320, 860, 782, 402, 400, 2573, 1735, 2142, 970, None, 1112, 1212, 1188, 92, 221, 1740, 1066, 1997, 193, 1005, 2599, 731, 1942, 211, 761, 946, 259, 710, 568, 449, 1406, 711, 274, 2398, 2555, 2253, 451, 398, None, 697, 1009, 820, 724, 2184, 1849, 2420, 525, 759, 113, 183, None, 1004, 1241, 378, 110, 267, 492, 1348, 906, 577, 286, 419, 135, 106, None, 130, 1930, 34, 124, 90, 322, 1393, 278, 420, 1498, 214, 1644, 29, 300, 144, 624, 377, 126, 180, 67, 633, 50, 783, 298, 430, 39, 1009, 121, 936, 189, 153, 3, 294, 87, 682, 291, 1250, 2686, 349, 106, 424, 1916, 428, 362, 1025, 2050, 788, 31, 661, 2671, 941, 1987, 491, 823, 311, 112, 293, 1583, 190, 86, 968, 1450, 1616, 2242, 259, 189, 2400, 869, 386, 382, 660, 2962, 40, 373, 1947, 1286, 1087, 507, 937, 755, 554, 36, 1899, 352, 1735, 1981, 212, 1064, 134, 195, 1219, 135, 723, 663, 244, 243, 195, 1468, 1735, 1952, 180, 3165, 478, 1065, 457, None, 636, 527, 149, 844, 841, 753, 189, 1334, 728, 59, 1117, 1745, None, 404, 1187, 68, 106, 624, 151, 1723, 1510, None, 121, 999, 989, 1941, 115, 190, 1321, 783, 731, 590, 496, 344, 111, 637, 2934, 661, 947, 461, 2055, 2127, 575, 400, 819, 761, 244, 1220, 199, 280, 295, 432, 2848, 292, 665, 948, 161, 335, 783, 1771, 2383, 68, 1348, 1087, 1036, 966, 150, 386, 564, 1918, 335, 2179, 596, 2648, 1618, 29, 425, 52, 285, 1268, 259, 2526, 479, 138, 1614, 376, 832, 1195, 24, 1626, 577, 424, 1815, 999, 148, 121, 193, 629, 589, 969, 936, 954, 357, 2157, 1292, 1298, 317, 2181, 287, 357, 1928, 400, 117, 212, 1699, 112, 643, 51, 784, 144, 34, 346, 186, 706, 478, 2484, 234, 946, 2065, 1065, 1242, 2235, 212, 1735, 121, 190, 196, 2127, 1066, 504, 308, 2496, 3413, 222, 664, 1699, 1741, 923, 2590, 349, None, 130, 215, 267, 940, 1231, 323, 274, 2380, 381, 610, 239, 3307, 125, 721, 960, 110, 2355, 828, 1724, 1, 314, None, 619, 402, 1495, 1768, 503, 204, 1250, 2229, 288, 1026, 663, 147, 50, 1587, 195, 207, 352, 31, 1295, 89, 211, 1739, 1217, 958, 106, 214, 966, 64, 2349, 1002, 2353, 305, 309, 1606, None, 2669, 504, 200, 190, 619, 1708, 656, 309, 255, 787, 2344, 285, 1835, 655, 375, 1332, 620, 1599, 1004, 67, 2, 673, 890, 1176, 1195, 997, 49, 302, 1087, 2032, 1032, 2470, 236, 991, None, 307, 305, 562, 1328, 532, 936, 1500, 189, 640, 935, 47, 268, 532, 969, 3187, 453, 1764, None, 310, 399, None, 196, 1062, 207, 558, 148, 190, 199, 150, 121, 400, 483, 2392, 432, 1889, 2159, 1519, 50, 1618, 213, 403, 415, 1643, 400, 784, 297, 185, 2686, 139, 1599, 942, 34, 1971, 90, 1314, 1849, 2008, 1133, 1526, 1604, 361, 684, 726, 195, 857, 800, 570, 322, 1068, 522, None, 438, 1088, 2165, 243, 137, 781, 831, 114, 207, 510, 2265, 208, 158, 950, 723, 217, 439, 352, 2363, 1301, 1723, 1040, 121, 2477, 590, 32, 511, 453, 1440, 473, 749, 869, 500, 204, 138, 134, 1440, 52, 561, 192, 435, 2300, 1601, 418, 293, 194, 60, 1767, 661, 617, 0, 1735, 1112, 1560, 64, 588, 1564, 1039, 1246, 116, 1026, 1612, 40, 1885, 50, 318, 212, 720, 300, 499, 355, 1152, 954, 277, 405, 857, 936, 587, 1599, 434, 1072, 1616, 173, 636, 196, 2151, 1230, 2015, 451, 17, 287, 267, 387, 1321, 999, 1940, 23, 231, 200, 862, 183, 151, 180, 800, 784, 1119, 224, 46, 212, 453, 1599, 311, 936, 772, 447, 180, 373, 320, 1735, 1909, 832, 30, 2308, 1229, None, 193, 185, 729, 1152, 661, 1972, 177, 312, 624, 111, 989, 120, 1004, 1007, 1599, 38, 539, 2601, 133, 177, 2593, 195, 1178, 50, 768, 162, None, 828, 2373, 192, 1716, 130, 943, 156, 1949, 45, 946, 438, 258, 322, 1002, 1435, 248, 276, 1751, 1015, 784, 1709, 1274, 320, 189, 2525, 1999, 989, 1141, 2538, 1041, 1269, 349, 452, 866, 681, 340, 711, 181, 661, 1571, 306, 653, 902, 2698, 158, 736, 1224, 610, 2754, 1561, 49, 2012, 2131, 970, 149, 989, 513, 1442, 1442, 570, 761, 1660, 106, 561, 652, 181, 1532, 300, 438, 784, 2305, 577, 342, 303, 199, 499, 612, 1555, 997, 1747, 1231, 213, 1357, 507, 1000, 2038, 391, 277, 1950, 531, 1231, 1495, 142, 1767, None, 946, 727, 629, 1434, 749, 936, 2859, 274, 829, 545, 941, 1362, 2002, 259, 827, 277, 113, 661, 1306, 385, 153, 609, 538, 550, 258, 1070, 1442, 310, 178, 940, 432, 332, 3453, 1143, 613, 1081, 1332, 1056, 1221, 111, 34, 491, 400, 1228, 65, 1610, 729, 282, 2413, 2353, 454, 569, 789, 400, 158, 1519, 2985, 653, 1815, 447, 152, 2974, 989, 748, 46, 2253, 1489, 619, 783, 400, 1376, 295, 288, 310, 2689, 1493, 2373, 552, 207, 855, 1626, 1034, 1979, None, 1533, 989, 638, 465, 785, 1440, 2929, 1344, 1725, 244, 375, 139, 2099, 134, 586, 840, 1965, 717, 386, 190, 1090, 882, 451, 478, 1590, 507, 1063, 178, 1735, 1123, 1105, 320, 1230, 2014, 957, 1814, 348, 147, 1966, 786, 936, 425, 2363, 380, 619, 661, 661, 438, 273, 743, 1740, 1246, 954, 1598, 49, 1619, 947, 568, 2017, 68, 2418, 1772, 288, 310, 1942, 840, 788, 1249, 130, 649, 1970, 332, 480, 433, 3298, 134, 1981, 950, 516, 910, 69, 1849, 1617, 571, 1215, 256, 197, 538, 931, 3033, 306, 517, 614, 400, 375, 58, 492, 1822, 1696, 800, 267, 139, 1269, None, 428, 653, 1978, 588, 312, 949, 768, 1693, 1693, None, 1640, 241, 295, 617, 523, 3, 1625, 1, 180, 947, 85, 3075, 2270, 1957, 1003, 48, None, 1299, 1612, 962, 215, 722, 1267, 932, 1183, 651, 1435, 212, 914, None, 450, 109, 274, 941, 1221, 171, 152, 592, 357, 743, 215, 968, 303, 1758, 647, None, 451, 911, 41, 947, 947, 641, 1195, 958, 1875, 341, 957, 273, 402, 2063, 559, 175, 471, None, 2689, 54, 576, 36, 902, 1241, None, 275, 377, 431, 959, 2972, 800, 1305, 321, 345, 559, 256, 1444, 1881, 65, 547, 558, 751, 478, 569, 1519, 355, 1571, 256, 1937, 1195, 1964, 947, 906, 792, 332, 111, 1789, 831, 152, 888, 2240, 133, 483, 1144, 1063, 2177, 400, 73, 444, 23, 93, 1456, 1004, 1153, 1164, 348, 404, 158, 424, 2168, 370, 1392, 293, 2365, 1436, 199, 759, 614, 3, 1885, 1286, 144, 1059, 568, 194, 343, 1144, 1571, 649, 276, 1304, 243, 126, 2022, 2012, 645, 554, 1028, 1266, 1526, 483, 638, 1495, 946, 372, 214, 358, 122, 477, 831, 1017, 1085, 1318, 2039, 906, 610, 483, 142, 829, 2225, 661, 949, 1342, 781, 230, 4, 4, 207, 1112, 256, 918, None, 447, 391, 385, 654, 896, 783, 1143, 343, 780, 343, 328, 139, 59, 1560, 399, 389, 1435, 396, 1815, 1371, 2848, 661, 1187, 1736, 1300, 576, 1332, 459, None, 300, 53, 777, 1614, 1018, 1682, 399, 661, 367, 296, 300, 1174, 203, 846, 1229, None, 443, 1686, 1302, 405, 724, 890, 1035, 319, 452, 2139, 425, 1376, 633, 969, 197, 317, 792, 483, 349, 2056, 1442, 1026, 1923, 1163, 867, 2008, 1435, 69, 1066, 937, 1614, 310, 208, 117, 195, 1174, 2551, 400, 570, 471, 107, 937, 1092, 620, None, 1950, 947, 495, 484, 792, 949, 1231, 407, 831, 198, 617, 2600, 1700, 781, 149, 306, 180, 1957, 999, 1618, 667, 723, 90, 34, 410, 471, 2630, 390, 1217, 403, 376, 122, 1087, 1713, 828, 1735, 534, 1803, 787, 749, 773, 1105, 1286, 288, 1059, 1571, 576, 498, 2163, 684, 728, 963, 331, 31, None, 434, 575, 1105, 87, 341, 2491, 1678, None, 499, 1241, 1027, 299, 987, 1473, 445, 400, 256, 551, 1457, 1026, 799, None, 610, 431, 1500, 397, 883, 2085, 277, 41, 189, 2514, 3089, 1584, 240, 2017, 617, 661, 1449, 141, 630, 1323, 277, 263, 966, 29, 538, 299, 279, 480, 777, 1526, 55, 196, 267, 592, 186, 121, 600, 1361, 130, 1814, 441, 1258, 1227, 279, 1008, 1305, 1, 1625, 589, 1293, 1699, 1141, 993, 3, 792, 792, 778, 256, 438, 132, 215, 2248, 825, 491, 1068, 118, 964, 1558, 298, 1286, 29, 121, 1349, 1026, 965, 937, 1034, 256, 1396, 198, 590, 1393, 344, 468, 869, 831, 661, None, 393, 764, 1016, 629, 759, 190, 1153, 256, 203, 787, 465, 391, 800, 829, 2233, 46, 1606, 725, 1950, 253, 1052, 970, 155, 1877, 159, 207, 244, 1526, 2403, 297, None, 461, 1022, 2321, 829, 1428, 171, 532, 32, 112, 525, 1528, 376, 236, 814, 636, None, 834, 612, 438, 250, 1805, 1735, 577, 380, 189, 194, 55, 663, 661, 946, 1093, 65, 211, 1994, 558, 38, 438, 312, 1243, 296, 1777, 2233, 130, 412, 1814, 1686, 1040, 2151, 28, 361, 2082, 1224, 1015, 2065, 2373, 2225, 2, 649, 130, 247, 761, 1508, 1905, 505, 1395, 2129, 295, 1323, 1519, 2724, 499, 1932, None, 197, 709, None, 613, 1029, 59, 55, 673, 477, 527, 1043, 519, 2175, 1112, 1347, 575, 589, 616, 1011, 278, 1942, 1446, 624, 517, 888, 248, 273, 661, 2329, 420, 2109, 950, 1592, 320, 989, 1825, 1825, 48, 1231, 615, 661, 902, 661, 2887, 48, 973, 114, 2444, 204, 1062, 649, 870, 1860, 1404, 307, 2132, 1243, 610, 453, None, 106, 936, 420, 696, 453, 278, 593, 619, 1515, 558, 1451, 480, 168, 810, 300, 317, 1892, 1026, None, 617, 661, 959, 2361, 1941, 346, 958, 944, 1740, 385, 811, 3002, 1904, 211, 831, 50, 516, 126, 729, 147, 731, 1284, 540, 2174, 1128, 800, 334, 1878, None, 2056, 79, 349, 380, 158, 262, 1557, 201, 304, 4, 430, 1761, 962, 1693, 2271, 424, 215, 1416, 1246, 827, 32, 181, 2392, 577, 111, 2015, 731, 247, 2625, 562, 519, 357, 1524, 1618, 128, 450, 829, 620, 2571, 1256, 948, 180, 502, 553, 211, 1061, 47, 190, 1032, 615, 1435, 2609, 1085, 153, 1872, 480, 946, 229, 369, 1231, 2044, 728, 2293, 88, 1735, 2139, None, 1814, None, 1112, 1592, 1081, 512, 3022, 1062, 1905, 211, 432, 800, 109, 784, 46, 452, 966, 64, 404, 477, 1486, 180, 1519, 2507, 2156, None, 524, 955, 215, 1265, 30, 834, 307, 2628, 69, 1459, 1369, 577, 1026, 224, 950, 450, 355, 1815, 311, 840, 727, 2144, 370, 504, 349, 1123, 213, 149, 447, 1997, 534, 936, 258, 911, 243, 2165, 394, 59, 1985, 195, 654, 576, 456, 1945, 2164, 348, 789, 121, 1623, 121, 914, 1243, 793, None, 1437, 394, 999, 935, 88, 1370, 1526, 781, 1710, 204, 139, 1922, 1451, 1311, 3075, 407, 277, 1885, 1195, 827, 866, 27, 1011, 343, 1814, 438, 579, 1263, 2787, 180, 34, 1396, None, 3, 1027, 1816, 50, 412, 1731, 870, 1292, 552, 561, 64, 796, 1306, 999, 370, 1970, 380, 947, 293, 186, 566, 1836, 4, 972, 949, 151, 1257, 3, None, 806, 29, 281, 610, 83, 222, 1260, 373, 198, 66, None, 1573, 201, 189, 1020, 275, 1431, 1087, 54, 661, 296, 77, 823, 1229, 971, 667, 1591, 614, 1849, 1815, 1099, 729, 207, 784, 195, 1404, 1442, 166, 311, None, 62, 3285, 832, 128, 859, 87, 342, 619, 1161, 1076, 377, 113, 617, 288, 125, 42, 294, 1635, 215, 431, None, 1707, 888, 1616, 1195, 821, 348, 268, 970, 180, 1940, 1809, 1519, 2041, 491, 517, 476, 1081, 2705, 65, None, 1086, 491, 259, 1740, 656, 1457, 1461, 1435, 950, 1671, 3194, 68, 2844, 268, 794, 29, 243, 706, 223, 256, 129, 332, 949, 381, 106, 955, 2065, 343, 729, 517, 711, 1618, 1066, 3271, 495, 55, 2242, 1693, 1816, 1137, 272, 209, 496, 3268, 481, 212, 117, 947, 1892, 110, None, 457, 1028, 2141, 1224, 514, 941, 585, 1889, 357, 499, 735, 147, 1386, 122, 604, 152, 524, 430, 457, 500, 146, 323, 1304, 431, 827, 459, 3006, 425, 130, 1393, 1167, 459, 178, 306, 403, 797, 403, 402, 2759, 2143, 2291, 319, 764, 494, 2366, 149, 435, 409, 649, 77, 402, 1176, 940, 939, 729, 1609, 272, 376, 1444, 562, 1431, 211, 201, 1143, 317, 111, 148, 549, 190, 1368, 887, 870, None, 468, 370, 198, 848, 294, 348, 661, 483, 321, 263, 1002, 577, 586, 424, 2462, 2754, 2255, 1002, 272, 215, 499, 774, 1348, 402, 215, 784, 1299, 435, 204, 649, 419, 90, 2128, 1123, 1449, 183, 52, 42, 1442, 2165, 2012, 443, 331, 1558, 494, 38, 1852, 306, 361, 121, 443, 1500, 2321, 1224, 2244, 1286, 2174, 142, None, 615, 90, 2226, 577, 2159, 1979, 79, 2811, 684, 791, 364, 346, 376, 331, 519, 999, 40, None, 776, 933, 1444, 189, 491, 949, None, 453, 800, 1526, 2014, 54, 111, 828, 415, 1942, 1292, 306, 562, 882, 1244, 1728, 2136, 1494, None, None, 2318, 866, 199, 110, 1571, 610, 662, 1908, 632, 1004, 1554, 28, 1228, 1751, 1516, 121, 379, 632, 761, 745, 1981, 531, 444, 320, 782, 68, 1743, 937, 207, 3006, 439, 562, 613, 1892, 1990, 1007, 1484, None, 940, 1905, 226, 570, 286, 882, 297, 523, 661, 661, 29, 661, 195, 1088, 1950, 184, 1695, 661, None, 2952, 946, 130, 610, 296, 808, 199, 272, 149, 776, 567, 1081, 2575, 1020, 30, 643, 1820, 138, 562, 1606, 840, 152, 950, 1087, 219, 1220, 59, 825, 842, 486, 590, 255, 133, 177, 256, 1044, 56, 256, 405, 2324, 1062, 31, 3236, 711, 432, 306, 3151, 908, 949, 411, 135, 1233, 985, 435, None, 195, 613, 403, 1049, 847, 2166, 342, 1693, 1699, 797, 1188, 1878, 614, 372, 2835, 348, 452, 234, 59, 323, 711, 1495, 238, 1264, 749, 277, 61, None, 300, 505, 1560, 649, 1525, 189, 914, 468, 1404, 525, 77, 298, 376, 473, 1437, 310, 661, 189, 2167, 2196, 370, 1767, 3190, 484, 369, 1000, 1348, 3094, 881, 300, 1393, 147, 1314, 54, 1513, 213, 790, None, 1033, 871, None, None, 539, 1361, 1269, 988, 1255, 1815, 2099, 132, 1596, 1249, 2267, 665, 989, 1187, 334, 2229, 377, 1820, 1693, None, 661, 1269, 1143, 1740, 208, 2493, 609, 932, 310, 418, None, 183, 641, 1587, 1434, 29, 1571, 1635, 653, 515, 244, 142, 303, 231, 848, 450, 1300, 661, 2131, 1100, 1228, 906, 1623, 342, 1533, 349, 378, 130, 1612, 710, 1115, 1240, 361, None, 263, 39, 214, 947, 357, 782, 215, 194, 2072, 321, 2344, 1614, 256, 532, 154, 698, 317, 2054, 130, 212, 2151, 1609, 88, 911, 300, 471, 265, 499, 146, 1404, 1765, 661, 1002, 586, 2211, 2127, 3306, 1745, 1077, 956, 259, 400, 510, 792, 1833, 1495, 521, 275, 1003, 547, 2042, 313, 1695, 661, 354, 332, 361, 274, 1583, 710, 402, 959, None, 2357, 971, 2035, 661, 109, 31, 1178, 125, 41, 274, 53, 32, 256, 1970, 519, 2497, 212, 3346, 949, 1368, 1678, 166, 590, 2416, 1057, 275, 947, 272, 729, 553, 1066, 223, 661, 66, 309, 1880, 88, 1287, 296, 1213, 449, 665, 431, 2403, 24, 2909, 721, 499, 2008, 213, 1822, 2077, 503, 1066, 366, 1592, 2078, 495, 1952, 1740, 1620, 67, 459, 896, 706, 661, 256, 1004, 2050, 1435, 1215, 170, 124, 342, 1268, 322, 2196, None, 483, 457, 256, 649, 211, 1609, 400, 106, 456, 382, 882, 34, 698, 2329, 192, 731, 563, 559, 347, 1357, 321, 1290, 2290, 805, 402, 472, 1040, 139, 1614, 2825, 1611, 1878, 307, 1081, 169, 1195, 294, 238, 942, None, 1617, 661, 207, 1268, 2985, 400, 721, 539, 615, 92, 136, 604, 2053, 343, 955, 1090, 198, 171, 947, 1224, 1465, 2249, 1268, 321, 866, 1332, 1767, 576, 55, 3343, 1241, 180, 559, 2462, 1484, 429, 623, 384, 212, 499, 1246, 576, 1526, 151, 240, 1839, 914, 2629, 985, 2, 1392, 401, 284, 198, 29, 50, 400, 438, 411, 29, 1610, None, 1907, 1519, 1508, 1348, 106, 729, 682, 2165, 211, 1981, 1449, 557, 661, 59, 385, 2402, 208, 1442, 781, 253, 620, 358, 504, 30, 1301, 2553, 838, 287, 121, 419, 787, 667, 1121, 130, 400, 2698, 2987, 820, 612, 401, 281, 791, 1287, 2403, 439, 1526, 759, 617, 1981, 1025, 653, 1026, 1532, 357, 1163, 258, 458, 1189, 645, 1370, 1004, 327, 432, 521, 1087, 1035, 407, 680, 1557, 1592, 503, 661, 177, 1292, 274, 586, 473, 342, 215, 1930, 1300, 151, 456, 1610, 1035, 1440, 306, 439, 1304, 1440, 2039, 888, 32, 1440, 657, 2, 405, 1188, 106, 409, 2422, 121, 727, 819, 58, 273, 966, 111, 280, 1120, 2407, 615, 2139, 138, 118, 278, 412, 256, 911, 1166, 1409, 570, 1926, 189, 403, 617, 519, 1991, 293, 263, 1105, 792, 112, 882, 491, 1048, 448, 1735, 617, 774, 1814, 1494, 2308, 3213, 513, 783, 227, 113, 2168, 1929, 490, 500, 101, 832, 50, 749, 2007, None, 1002, 2035, 263, 614, 202, 636, 950, 1232, 114, 268, 183, 1951, 2505, 55, 610, 1017, 220, 333, None, 207, 1062, 570, 307, 649, 527, 1899, 1065, 1084, 1809, 881, 240, 888, 435, 654, 858, 2628, 1019, 619, 1835, 617, 792, 1429, 620, 1815, 914, 480, None, 749, 1878, 478, 2185, 468, 444, 302, 1885, 2056, 500, 344, 1888, 954, 661, 661, 570, 521, 403, 151, 2801, 1745, 320, 936, 321, 506, 958, 1803, 866, 451, 496, 437, 769, 55, 1777, 551, 792, 144, 1195, 776, 1026, 1457, 2622, 728, 281, 1264, 38, 963, 1434, 1451, 610, 331, 124, 614, 661, 517, 1591, 184, 312, 562, 840, None, 195, 521, 230, 453, 827, 109, 415, 215, 55, 784, 214, 3, 336, 1342, 2165, 1047, 1375, None, 586, 194, 1576, 699, 2049, 1143, 3022, 352, 1602, 121, 287, 190, 300, 2760, 1981, 2756, 320, 2699, 379, 3413, 911, 1143, None, 385, 855, 1734, 471, 1851, 134, 661, 1114, 1555, 399, 212, 424, 267, 2248, 125, 2674, 2249, 1599, 56, 1114, 189, 386, 2066, 956, 400, 203, 1767, 2491, 661, 293, 418, 405, 479, 55, 294, 399, 139, 1767, 1885, 1853, 452, 444, 451, 2118, 2267, 464, 41, 29, 32, 604, 183, 244, 615, 1004, 768, 403, 155, 130, 261, 65, 216, 117, 308, 298, 1003, 950, 670, 1815, 1396, 797, 2974, 134, 47, 1119, 717, 1326, 1736, 523, None, 661, 48, 1309, 293, 199, 362, 177, 148, 29, 2129, 457, 2065, 684, 989, 435, 2109, 1442, 202, 1579, 133, 2150, 233, 614, 277, 1026, 412, 615, 321, 781, 568, 183, 1451, 533, 797, 440, 1386, 1393, 480, 1723, 1596, 120, 1599, 3201, 138, 124, 947, 479, 299, 1566, 716, 3227, 460, 661, 38, 2, 827, 81, 1186, 563, 2, 1735, 313, 683, 2075, 2900, 1688, 386, 296, 36, 121, 130, 1716, 871, 67, 797, 117, 48, 3069, 727, 215, 1305, 158, 52, 113, 459, 2258, 1237, 2900, 109, 1958, 3, 797, 321, 288, 1015, 425, 950, 2147, 684, 138, 2704, 1306, 914, 453, 400, 357, 148, 312, 151, 443, 348, 308, 949, 299, None, 180, 619, 1449, 256, 1562, 1449, 64, 284, 61, 1435, None, 30, 109, 215, None, 158, 2071, 649, 1981, 215, 1601, 1992, 1286, 792, 373, 3446, 113, 159, 491, 155, 1508, 1395, 661, 237, 997, 884, 283, 1214, 582, 887, 83, 221, 294, 1040, 132, 1480, 162, 400, 29, 2, 932, 1017, 196, 274, 1533, 628, 906, 54, 1119, 1299, 653, 153, 711, 1877, 989, 617, 525, 783, 1348, 1435, 130, 135, 1299, None, 561, 2066, 47, 2609, 1088, 1038, 278, 59, 473, 196, 1234, 2571, 1667, 482, 486, 784, 3037, 2718, 197, 1065, 385, 236, 400, 1677, 2230, 195, 661, 590, 144, 456, 1809, 2065, 1599, 1337, 491, None, 1368, 195, 268, 683, 385, 1970, 1819, 263, 303, 307, 989, None, None, 431, 1767, 1166, 130, 681, 41, 29, 857, 2174, 184, 1849, 80, 156, 1625, 346, 1263, 63, 1088, 148, 848, 738, 443, 1434, 207, 1619, 793, 1064, 728, 1120, 806, 2174, 228, 661, 273, 34, 2363, 16, 598, 64, 667, 46, 1380, 215, 2165, 80, 33, 1758, 716, None, 30, 586, 1334, 2109, 279, 37, 41, 1263, 728, 106, 843, 128, 2109, 58, 636, 2257, 1090, 197, 1034, 1179, 1253, 3285, 617, None, 531, 423, 384, 523, 1735, 121, 340, 1319, 1693, 238, 1137, 192, 212, 944, 2507, 605, 357, 885, 55, 649, 480, 195, 424, 950, 496, 1485, 597, 2470, 1499, 273, 300, 2305, 628, 522, 400, 159, 604, 1523, 1185, 292, 2002, 117, 1049, 55, 2049, 729, 805, 1815, 537, 575, 1043, 298, 2199, 1922, 2007, 997, 1618, 153, 936, 507, 1406, 1435, 343, 114, 75, 1713, 588, 927, None, 853, 1683, 600, 215, 556, 1003, 125, 1981, 491, 777, 215, 1131, 380, 847, 1201, 30, 384, 1090, 896, 661, 1337, 1912, 362, 2017, 2817, 47, 464, 730, 748, 3033, 1112, 1740, 384, None, 989, 1437, 878, 553, 148, 130, 185, 272, 947, 1867, 950, 1041, 310, 421, 661, 1987, 649, 438, 1166, 61, 391, 297, 2151, 1818, None, 559, 1195, 236, 914, None, 1671, None, 1526, 499, 1420, 974, 1532, 307, 832, 1040, 1724, 1362, 64, 1112, 586, None, 561, 54, 562, 1435, 615, 1299, 1350, 705, 3, 722, 376, 1429, 434, 215, 1724, 282, 1635, 999, 1508, 876, 197, 241, 310, 2674, None, 23, 150, 727, 1872, 146, 86, 2420, 781, 2510, 121, 1427, 198, 2528, 370, 561, 710, 1623, 308, 1888, 407, 1306, 68, 314, 617, 728, 1040, 50, 194, 451, 1188, 214, 697, 782, 827, 570, 1195, 1910, 586, 910, 300, 1903, 585, 342, 2280, 1457, 2502, 1678, 569, 312, 608, 405, 860, 1062, 84, 254, 3020, 372, 2844, 630, 819, 649, 1876, 48, 2165, 2492, 3394, 635, 229, 661, 684, 212, 511, 195, 2015, 452, 144, 120, 1997, 273, 1455, 1695, 208, 1941, 121, 724, 432, 1614, 839, 124, 2629, 319, 1047, 399, 400, 114, 114, 109, 617, 168, 90, 1579, None, 309, 276, 48, None, 403, 788, 829, 261, 955, 827, 185, 2066, 513, 1818, 491, 613, 391, 3071, 1264, 342, 1731, 376, 373, 321, 765, 114, None, 552, 211, 288, 456, 117, 882, 800, 1686, 1722, 661, 2267, 1040, 1040, 234, 1257, 1942, 398, 53, 661, 2590, 438, 2159, 1242, 2710, None, 1707, 881, 1112, 1242, 956, 1348, 130, 276, 989, 2668, 415, 1599, 38, 1231, 1023, 212, 190, 617, 2426, 1457, 502, 420, 1011, 306, 640, 211, 438, 230, 600, 183, None, 942, 68, 308, 2804, 2329, 2329, 2242, 196, 1049, 1592, 2035, None, 2413, 187, 1616, 105, 1144, 1408, 1987, 1872, 1217, 653, 1849, 180, 380, 438, 840, 517, 570, 1322, 255, 29, 590, 678, 2243, 531, 1015, 2, 2427, 1052, 827, 52, 190, 958, 478, 720, 1249, 644, 210, 400, 500, 358, 954, 521, 205, 1170, 1105, 1750, 312, 456, 434, 121, 1735, None, 1301, 881, 1601, 2347, 1516, 424, 314, 2103, 1149, 366, 988, 201, 373, 1444, 956, 46, 58, 130, 1482, 62, 340, 2195, 2028, 189, 2069, 828, 496, 959, 1027, 1174, 311, 1299, 307, 64, 724, 2243, None, 882, 285, 562, 238, 38, 835, 312, 999, 615, 349, 192, 296, 615, 764, 258, 941, 1892, 198, 1952, 64, 1008, 180, 628, 135, 2192, 933, 1983, 1144, 130, 1186, 289, 179, 1816, 1457, 1215, 1609, 617, 3073, 90, 310, 408, 974, None, 1215, 1017, 144, 90, 434, 950, 582, 1599, 402, 3078, 491, 90, 210, 2239, 673, 141, None, 432, 130, 1458, 1440, 207, 133, 1261, 1743, 194, 1087, 45, 1735, 118, 1949, 432, 522, 428, 199, 268, 792, 709, 1682, 588, 1206, 729, 1029, 523, 221, 960, 1724, 152, 112, 661, 1442, 1174, 1892, 2671, 1769, 723, 319, 1866, 346, 1479, 1819, 1195, 2355, 1041, 684, 2337, 579, 1601, 51, 1964, 661, 2219, 447, 53, 664, 483, 661, 1599, 334, 2042, 1018, 278, 503, 1827, None, 272, 307, 843, 2130, 708, 1986, 221, 195, 855, 3034, 575, 159, 658, 905, 661, 401, 2118, 1824, 1195, 258, 575, 911, 1063, 717, 1479, 1455, 1088, 2248, 911, 1016, 1038, 622, 1304, 64, 999, 461, 1773, 496, 1091, 888, 524, 1824, 1049, 2161, 224, 610, 438, 344, 2371, 153, 765, 531, 1440, 1994, 147, 75, 452, 273, 380, 1981, 3117, 154, 391, 298, 2253, 1735, 557, 1446, 1233, 644, 110, 636, 386, 1201, 2900, 576, 294, 857, 358, 395, 1260, 480, 711, 792, 2100, 2027, 1801, 1016, 728, 2166, 447, 1112, 274, 238, 294, 947, 577, 409, 644, 352, 781, 461, 711, 212, 866, 759, 130, 661, 1229, 1011, 1880, 851, 601, 1178, 130, 1301, 2103, 55, 783, 40, 653, 1427, 1687, None, 154, 2, 2044, 1849, 1326, 1046, 769, 590, 221, 682, 1428, 988, 1043, 866, 1814, 1334, 135, 2185, 1312, 1802, None, 319, 430, 1440, 2255, 59, 636, 632, 1065, 456, 3069, 564, 308, 1066, 123, 657, 2009, 253, 1088, 106, 989, 806, 1188, 1246, 199, 273, 1990, 1022, 1089, 27, 273, 148, 36, 438, None, 575, 274, 555, 116, 132, 312, 728, 1487, 451, 1716, 1877, 1243, 617, 473, 1049, 1002, 582, 911, 735, 236, 117, 661, 1937, 1268, 2891, 2055, 131, 438, 653, 2139, 1513, 905, 1002, 577, 649, 1707, 1568, 1413, 799, 1695, 275, 557, 1025, 2295, 200, 1215, 504, 502, 407, 386, 1153, 343, 787, 121, 132, 1488, 727, 523, 853, 946, 310, 0, 3, 1354, 135, 1752, 2365, 1201, 2233, 97, 307, 1229, 2497, 447, 1195, 342, 768, 554, 106, 180, 674, 576, 114, 955, 848, 424, 483, 402, 415, 399, 867, 130, 2349, 65, 882, 349, 588, 504, 1584, 1741, 48, 2105, 196, 2239, 45, 389, 586, 1885, 1049, 1023, 1745, 614, 226, 1814, 1286, 471, 1163, 617, 476, 392, 1257, 278, 717, 189, 1145, 274, 343, 536, 825, 499, 193, 1118, 2278, 2491, None, 2760, 495, 299, None, 212, 2698, 136, 2698, 840, 357, 696, 111, 523, 630, 2733, 212, 1722, 2, 2125, 1592, 213, 759, 1498, 1417, 392, 403, 277, 1669, 882, 709, 706, 1296, 764, 181, 177, 641, 189, 1526, 58, 2744, 1734, 302, 914, 471, 649, 550, 858, 121, 614, 1263, 2065, 207, 1112, 1090, 936, 991, 577, 430, 1070, 386, 29, 729, 355, 424, 157, 118, None, 1188, 114, 711, 402, 1326, 233, 402, 881, 1065, 642, 1082, 866, 478, None, 1741, 940, 1733, 1532, 139, 617, 1625, 2, 430, 954, 866, 655, 1224, 344, 451, 433, 650, 1822, 1679, 697, 402, 300, 486, 332, 706, 390, 1264, 153, 996, 118, 42, 492, 199, 956, 713, 288, 1186, 64, 709, 457, 651, 112, 661, 1414, 394, 499, 199, 352, 724, None, 110, 177, 195, 682, 4, 1141, 349, None, 729, 1179, 134, 357, 347, 661, 1002, 2218, 1612, 357, 425, 907, 911, 1435, 949, 180, 139, 238, 130, 170, 399, 68, 1049, 1767, 1076, 1849, 1696, 179, 220, 267, 661, 334, 950, 1222, 1231, 950, 537, 376, 1314, 368, 357, 61, 2323, 3343, 955, 710, 1710, 464, 1332, 555, 195, 3308, 684, 792, 590, 1491, 1526, 3164, 604, 311, 764, 706, 950, 691, 2400, 661, 358, 499, 109, 239, 431, 343, 1241, 661, 406, 682, 615, 185, 1321, 496, 419, 659, 1803, 1814, 173, 128, 317, 363, None, 1304, 914, 2077, 1459, 1484, 1494, 914, 106, 1479, 196, 2680, 434, 187, 997, 79, 586, 2545, 612, 988, 1872, 139, 107, 831, 871, 396, 797, 866, 1798, 590, 393, 2150, 930, 1981, 614, 329, 651, 1302, 372, 357, 721, 212, 661, 212, 914, 1699, 697, 994, 161, 245, 1226, 1444, 1716, 792, 428, 511, 452, 1723, 107, 1809, 1419, 349, 1120, 204, 213, 258, 151, 729, 3200, None, 759, 1526, 432, 1035, 67, 208, 2089, 3, 435, 843, 432, 710, 777, 947, 1052, 936, 405, 1195, 2171, 343, 590, 590, 620, 386, 1849, 829, 38, 467, 47, 1348, 1877, 458, 483, 283, 478, 2350, 1354, 651, 2482, 3285, 643, 1668, 2242, 443, 882, 204, 1814, 1002, 474, 883, 1424, 1767, 828, 357, 706, 320, 614, 244, 144, 1046, 1293, 1424, 255, 458, 1610, 227, 185, 114, 2007, 1614, 610, 2380, 412, None, 47, 888, 40, 1267, 293, 562, 684, 825, 371, 34, 1066, 442, 556, 1299, 697, 457, 106, 349, 697, 1188, 226, 1923, 649, 950, 1396, 1912, 781, 48, 697, 483, 661, 2162, 1452, 1401, 808, 2017, 307, 300, 1444, 1301, 2281, 325, 615, 590, 832, 827, 696, 2234, 2805, 924, 696, 531, 661, None, 1604, 390, 1584, None, 956, 1097, 783, 827, 1227, 347, 472, 1640, 871, 1872, 619, 1437, 832, 295, 1427, 586, 1214, 939, 34, 661, 723, 1713, 483, 610, 64, 241, 130, None, 385, 33, 614, 1786, 1612, 149, 79, 401, 721, 45, 1230, 1495, 121, 2344, 132, 320, None, 341, 402, 1815, 2015, 1406, 464, 1816, 318, 782, 380, 1769, 238, 264, None, 781, 950, 1191, 2371, 1442, 147, 1003, 617, 638, 378, 1735, 661, 2491, 557, 661, 588, 193, 320, None, 1164, 1009, 1446, 1226, 656, 2233, 661, 1311, 256, 758, 617, 124, 1086, 300, 1482, 1767, 442, 2600, 1057, 729, 184, 959, 545, 1562, 90, 1000, 1286, 190, 2175, 238, 434, 727, 1814, 1019, 535, None, 341, 1006, 1519, 180, 589, 717, 956, 46, 746, 479, 1047, 1409, 259, 1304, 1233, 257, 434, 268, 1186, 195, 832, 1062, 1072, 312, 615, 783, 654, 240, 759, 55, 212, 896, 1066, 451, 1141, 1610, 212, 1625, 1876, 209, 322, 684, 420, 2351, 617, 37, 1334, 400, 410, 32, 2132, 1936, 711, 48, 590, 386, 1695, 996, 604, 103, 1017, 68, 69, 0, 941, 1451, 34, 201, 835, 1026, 710, 840, 299, 1609, 1724, 945, 317, 243, 947, 661, None, 1877, 2463, 1241, 2, 1187, 636, 215, 940, 2582, 215, 792, 1062, None, 2121, 636, 649, 729, 1833, 428, 81, 491, 636, 619, 2459, 425, 323, 194, 1318, 2482, 2709, 649, 224, 1233, 1041, 1, 50, 914, 453, 420, 42, 195, 777, 514, 185, 424, 49, 477, 2267, 729, 2151, 2349, 79, 885, 955, 1072, 400, 452, 425, 156, 949, 134, 310, 1256, 586, 482, 1186, 256, 1326, 3079, 667, 1376, 189, 194, 140, 1240, 273, 259, 2416, 950, 199, 1588, 1503, 321, 522, 797, 2, 927, 189, 3187, 187, 286, 940, 476, 809, 180, 808, 300, 213, 1065, 1450, 1699, 29, 1002, 144, 2014, 1872, 615, 617, 189, 729, 1020, 448, 866, 947, 113, 514, 1129, 196, 109, 792, 366, 297, 910, 1034, 90, 184, 1440, 1828, 882, 1614, 828, 1605, 1086, 1582, 1735, 749, 201, 294, 199, 196, 1528, 45, 352, 2671, 1435, 2560, 819, 1061, 283, 376, 409, 1667, 1504, 3150, 278, 30, 258, 403, 939, 575, 615, 502, 948, 1825, 706, 1435, 376, 1767, 482, 253, 3350, 1508, 2, 2290, 610, 108, 263, 189, 2462, 1910, 1435, 505, 661, 1752, 180, 274, 489, 395, 871, 196, None, 436, 234, 1188, 1728, 344, 147, 3118, 194, 1367, 590, 1377, 280, 1614, None, 706, 36, 45, 23, 2019, 1189, 1294, 138, 117, 630, 1011, 236, 3118, 130, 212, 309, 2366, 636, 308, 661, 1866, 320, 272, 1849, 1940, 914, 794, 252, 955, 56, 617, 797, 2694, 277, 59, 300, 184, 1873, 647, 950, None, 1130, 880, 274, 2579, 398, 1526, 190, 452, 1982, 1040, 272, 46, 274, 1994, 590, 556, 1680, 263, 1604, 2151, 1100, 832, 1090, 1519, 432, 400, 413, 2099, 199, 797, 869, 190, 1435, 787, 347, 1229, 2159, 424, 859, 1412, 194, 523, 1815, 425, 1041, 120, 1764, 294, 295, 67, 2023, 1857, 1, 1519, 661, 190, 148, 186, 306, 829, 1611, 1678, 1143, 256, 661, 562, 196, 3014, 2, 505, 1062, 858, 786, 792, 243, 196, None, 1706, 956, 649, 300, 2949, 340, 1028, 1735, 765, 1357, 668, 778, 375, 405, 1035, 1951, 2219, 197, 988, 355, 911, 378, 2174, 661, 114, 211, 928, 1229, 988, 731, 684, 1467, 255, 189, 649, 792, 1035, 1109, 536, 989, 1510, 911, 2213, 571, None, 1255, 1179, 315, 969, 2257, 730, 857, 459, 273, 1143, 2185, 823, 2321, 1065, 277, 185, 273, 1802, 395, 1486, 1070, 189, 792, 1592, 1286, 1087, 984, 238, 319, 783, 398, 280, 947, 579, 1029, 250, 112, 716, 1195, 947, 1673, 649, 130, 940, 1240, 190, 1174, 1332, 500, 751, 103, 1451, 2668, 1815, 525, 840, 1186, 746, 2229, 1971, 307, 661, 29, 471, 207, 847, 410, 335, 352, 1085, 946, 33, 615, 546, 1314, 319, 2872, 1449, 1184, 649, 335, 808, 2185, 452, 987, 451, 109, 64, 792, 559, 892, 396, 1229, 1867, 1730, 610, 456, 697, 400, 400, 590, None, 1869, 49, 1716, 194, 563, 800, 437, 1442, 1228, 376, 728, 307, 183, None, 309, 649, 402, 400, 146, 1255, 1287, 947, 936, 310, 312, 643, 1143, None, None, 1085, 1062, 3027, 484, 452, 620, 263, 513, 334, 407, 256, 590, 192, 2103, 133, 954, 392, 663, 326, 276, 1835, 113, 1309, 1086, 117, 32, None, 654, 709, 97, 383, 0, 950, 949, 1437, 384, 351, 989, 941, 1048, 125, 857, 2525, 509, 432, 1249, 1130, 748, 240, 2012, 1, 1725, 112, 661, 610, 515, 346, 1623, 1818, 1093, 109, 286, 1457, 256, 1877, 355, 1601, 173, 108, 614, None, 739, 452, 949, 1635, 679, None, 415, 1614, 1892, 3058, 204, 3341, 1694, 123, 352, 1987, 987, 1885, 1348, 2553, 1255, 1780, 68, 2308, 1916, 882, 1435, 1495, 661, 129, 207, 469, 459, 2208, 244, 59, 1516, 2353, 1059, 320, 444, 1837, 120, 192, 189, 556, 711, 1860, 649, 947, 202, 2740, 1626, 361, 1981, 665, 1303, 2506, 259, 807, 130, 190, 1066, 253, 1435, 2237, 2151, 152, 2528, 288, None, 1827, 262, 552, 1892, 255, 1123, 303, 435, None, 199, 601, 1915, 1434, 482, 117, 310, None, 1007, 144, 1186, 1188, 2314, 1085, 2016, 306, 244, 1141, 798, 649, 1066, 474, 1040, 615, 288, 661, 2455, 1682, 499, 400, 549, 155, 358, 1072, 425, 525, 1713, 3410, 482, 1106, 947, 2077, 1695, 334, None, 144, 417, 881, 688, 1584, 1003, 380, 306, 457, 2290, 207, 958, 614, 1232, 582, 858, 712, 114, 3074, 14, 604, 2139, 2174, 937, 1612, 1594, 431, 1072, 268, 1835, 1525, 1429, 729, 723, 999, 2290, 1412, 272, 625, 458, 1227, 859, 601, 636, 1429, None, 313, 357, 48, 185, 1242, 211, 1489, 3164, 357, 1875, 136, None, 374, 706, 3446, 1598, 1286, 496, 958, 211, 988, 1229, 1374, 1244, 1332, 720, 1006, 764, 1087, 1745, None, 357, 500, 309, 531, 432, 1041, 577, 661, 618, 477, 1273, 823, 3, 370, 114, 1560, 684, 311, 459, 513, 2942, 1909, None, 626, 28, 987, 1299, 937, 731, 377, 894, 1872, 443, None, 385, 643, 274, None, 2404, 1454, 714, 258, 343, None, 570, 1200, 2531, 1499, 2628, 469, 781, 56, 438, 301, 319, 716, 1011, 477, 1143, 215, 1226, 130, 432, 941, 910, 296, 987, 1081, 614, 195, 556, 1144, 109, 663, 615, 106, 1818, 399, 3122, 1599, 303, 68, 851, 1454, 1728, 187, 1609, 1431, 3410, 418, 1837, 405, None, 2150, 277, 499, 2416, 722, 1337, 278, 1614, 503, 124, 661, 1038, None, 153, 29, 780, 184, None, 1418, 2258, 2016, 135, 782, 537, 231, 2324, 1508, 400, 313, 1286, 600, 117, 1435, 67, 2602, 294, 636, 781, 124, 443, 1780, 166, 133, 124, 401, 2109, 298, 503, None, 1040, 1449, 914, 29, 591, 1306, 403, 467, 2121, 374, 199, 146, 825, 749, 55, 295, 965, 139, 1081, 674, 1332, 564, 1816, 1311, 576, 393, 386, 1057, 1290, 800, 2691, 1062, 661, 2357, 319, 212, 1065, 59, 1446, 439, 1999, 296, 306, 376, None, 202, 386, 712, 988, 1292, 273, 2104, 255, 258, 2551, 878, 2032, 1451, 438, 111, 1143, 729, None, 1026, 373, 1231, 130, 361, 130, 800, 129, 586, 311, 319, 256, 2109, 789, None, 273, 969, 213, 661, 748, 2840, 452, 499, 1686, 1625, 749, 1343, 51, 786, 954, 1687, 1132, 295, 189, 272, 386, 334, 623, 2342, 342, 2221, 939, 716, 46, 3016, 241, 190, 373, 954, 866, 1087, 1062, 657, 534, 2477, 264, 399, 1869, 755, 1035, 988, 144, 523, 684, 307, 389, 682, 2840, 262, 2628, 2934, 32, 789, 331, 1737, 1519, 432, 1476, 1961, 197, 552, 619, 2502, 124, 1457, 1885, 400, 3113, 1228, 1601, 1000, 575, 267, 472, 936, 726, 2193, 263, 112, 1002, 2214, 1268, 418, 47, 1053, 33, 1417, 349, 2139, 945, 495, None, 855, 1446, 682, 199, 527, 151, 793, 1299, 124, 1592, 711, 436, 215, 1667, 1735, 293, 186, 2883, 907, 587, 138, 1849, 1598, 332, 79, 1371, 2426, 438, 1903, 388, 171, 2167, 1114, 2620, 3, 1583, 2044, 947, 1228, 840, 121, 195, 509, 362, 958, 1412, 131, 792, None, 144, 797, 109, 734, 310, 317, 819, 1428, 136, 399, 400, 1028, 661, 259, 293, 207, 1535, 914, 636, 298, 55, 1301, 425, 1508, 1105, 220, 55, 587, 537, 3110, 1519, 259, 457, 227, 2353, 1072, 194, 1617, 772, 123, 325, 216, 1735, 661, 2, 256, 936, 109, 1718, 1442, 1840, 1086, 2768, 2756, 1002, 283, 1619, 1814, 701, 512, 649, 984, 52, 276, 349, 151, 201, 561, 1442, 128, 1486, 424, 2007, 41, 1072, 2053, 2619, 189, 800, 1457, 211, 661, 1557, 577, 2613, 1231, 831, 1764, 2167, 1112, None, 592, 705, 1713, 631, 617, 2159, 2256, 2082, 1144, 806, 1769, 1393, 2046, 347, 148, 277, 772, 2491, 316, 62, 438, 1519, 1970, 2351, 189, 111, 3122, 503, 1062, 121, 428, 1635, 1316, 114, 2212, 941, 180, 661, 443, 1408, 914, 2699, 1598, 377, 777, 214, 277, 1268, 503, 64, 782, 2883, 287, None, 745, 195, 275, 709, 521, 1614, 1304, 1701, 1068, 1599, 303, 376, 1755]\n",
            "11575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ttxcCCpu-eQ",
        "outputId": "dabb526d-b547-4739-da65-a8518507354c"
      },
      "source": [
        "labelsdf = pd.DataFrame(labels)\n",
        "data1 = pd.concat([data_bert, labelsdf], axis=1)\n",
        "data1.dropna(inplace=True)\n",
        "len(data1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gvgJrpARwG_q",
        "outputId": "121b9091-eef6-418e-adfa-4275382d70c7"
      },
      "source": [
        "data1.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>Detail</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34606.0</td>\n",
              "      <td>DAISY FOLKART HEART DECORATION&amp;&amp;ROSE FOLKART H...</td>\n",
              "      <td>BASKET OF FLOWERS SEWING KIT</td>\n",
              "      <td>924.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33874.0</td>\n",
              "      <td>BROWN CHECK CAT DOORSTOP&amp;&amp;CREAM CUPID HEARTS C...</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>661.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35182.0</td>\n",
              "      <td>JUMBO BAG PINK WITH WHITE SPOTS&amp;&amp;JUMBO SHOPPER...</td>\n",
              "      <td>I'M ON HOLIDAY METAL SIGN</td>\n",
              "      <td>1442.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33891.0</td>\n",
              "      <td>PACK OF 60 PINK PAISLEY CAKE CASES&amp;&amp;PACK OF 72...</td>\n",
              "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
              "      <td>424.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>35323.0</td>\n",
              "      <td>PAPER BUNTING WHITE LACE&amp;&amp;VINTAGE UNION JACK S...</td>\n",
              "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
              "      <td>424.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...       0\n",
              "0   34606.0  ...   924.0\n",
              "1   33874.0  ...   661.0\n",
              "2   35182.0  ...  1442.0\n",
              "3   33891.0  ...   424.0\n",
              "5   35323.0  ...   424.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v88Ns520wDQm",
        "outputId": "8a7dec62-4caa-4bf6-de85-a0d393939673"
      },
      "source": [
        "labels = data1[0].values\n",
        "labels = labels.tolist()\n",
        "len(labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHoHtKlTz6hT",
        "outputId": "0710b8de-1a2c-4623-972e-82c2b06416f5"
      },
      "source": [
        "print(labels[:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[924.0, 661.0, 1442.0, 424.0, 424.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMU3vEqUyFd5"
      },
      "source": [
        "cut = round(len(data1) * 0.8)\n",
        "train, test = data1[:cut], data1[cut:]\n",
        "train_label, test_label = data1[:cut][0], data1[cut:][0]\n",
        "train_label, test_label = train_label.to_list(), test_label.to_list()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx_yHxhsnhZN",
        "outputId": "cc450021-3234-4557-dfa3-879b21f363a0"
      },
      "source": [
        "print(train_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[924.0, 661.0, 1442.0, 424.0, 424.0, 721.0, 647.0, 453.0, 400.0, 1299.0, 158.0, 283.0, 1169.0, 535.0, 1771.0, 48.0, 661.0, 396.0, 310.0, 432.0, 517.0, 2165.0, 180.0, 1226.0, 1089.0, 424.0, 1029.0, 727.0, 2891.0, 1375.0, 491.0, 2900.0, 1618.0, 1003.0, 109.0, 65.0, 202.0, 213.0, 40.0, 829.0, 1599.0, 199.0, 307.0, 243.0, 314.0, 1121.0, 1178.0, 310.0, 1668.0, 121.0, 1008.0, 3297.0, 749.0, 2253.0, 183.0, 1222.0, 665.0, 1019.0, 2196.0, 827.0, 870.0, 729.0, 348.0, 1457.0, 1300.0, 349.0, 121.0, 50.0, 2014.0, 1424.0, 2069.0, 1062.0, 402.0, 946.0, 67.0, 988.0, 1612.0, 342.0, 1183.0, 2349.0, 229.0, 299.0, 3033.0, 190.0, 1745.0, 161.0, 352.0, 231.0, 67.0, 514.0, 1966.0, 288.0, 1618.0, 727.0, 1803.0, 984.0, 1309.0, 951.0, 306.0, 1264.0, 657.0, 1299.0, 109.0, 200.0, 987.0, 1673.0, 90.0, 943.0, 1943.0, 117.0, 947.0, 158.0, 1406.0, 503.0, 124.0, 1686.0, 298.0, 1929.0, 1730.0, 663.0, 1152.0, 989.0, 749.0, 113.0, 190.0, 1822.0, 1188.0, 37.0, 965.0, 135.0, 55.0, 1086.0, 798.0, 1195.0, 2648.0, 765.0, 1735.0, 1223.0, 2403.0, 1584.0, 378.0, 1941.0, 937.0, 2223.0, 1903.0, 1688.0, 882.0, 1941.0, 186.0, 570.0, 649.0, 661.0, 29.0, 125.0, 361.0, 592.0, 158.0, 2008.0, 1909.0, 190.0, 177.0, 370.0, 473.0, 562.0, 840.0, 1229.0, 1677.0, 617.0, 383.0, 457.0, 1035.0, 2.0, 134.0, 550.0, 2306.0, 130.0, 1822.0, 34.0, 950.0, 1557.0, 550.0, 1143.0, 1103.0, 399.0, 248.0, 539.0, 573.0, 347.0, 1758.0, 311.0, 207.0, 1444.0, 1677.0, 1074.0, 2373.0, 121.0, 1085.0, 294.0, 722.0, 665.0, 908.0, 1066.0, 1465.0, 1543.0, 1493.0, 432.0, 1471.0, 954.0, 58.0, 306.0, 570.0, 2942.0, 800.0, 280.0, 1221.0, 701.0, 244.0, 189.0, 989.0, 1195.0, 950.0, 644.0, 827.0, 788.0, 113.0, 615.0, 614.0, 562.0, 306.0, 1161.0, 303.0, 442.0, 948.0, 489.0, 497.0, 754.0, 67.0, 452.0, 194.0, 657.0, 497.0, 1249.0, 1614.0, 2038.0, 140.0, 828.0, 910.0, 403.0, 966.0, 293.0, 180.0, 661.0, 67.0, 1049.0, 428.0, 351.0, 1049.0, 195.0, 947.0, 298.0, 617.0, 1532.0, 56.0, 636.0, 617.0, 617.0, 357.0, 134.0, 4.0, 394.0, 954.0, 279.0, 211.0, 950.0, 1435.0, 935.0, 121.0, 1018.0, 59.0, 1710.0, 298.0, 1509.0, 1269.0, 1735.0, 1680.0, 221.0, 198.0, 50.0, 490.0, 189.0, 258.0, 183.0, 808.0, 1427.0, 614.0, 258.0, 1849.0, 1228.0, 1815.0, 1065.0, 1686.0, 319.0, 1430.0, 532.0, 211.0, 999.0, 274.0, 1950.0, 453.0, 349.0, 1077.0, 1188.0, 781.0, 244.0, 2987.0, 642.0, 1081.0, 200.0, 306.0, 1808.0, 742.0, 208.0, 2937.0, 2009.0, 359.0, 224.0, 1068.0, 661.0, 1818.0, 2373.0, 2218.0, 90.0, 971.0, 79.0, 615.0, 906.0, 956.0, 369.0, 2131.0, 956.0, 299.0, 668.0, 464.0, 1178.0, 123.0, 299.0, 1828.0, 135.0, 911.0, 1163.0, 390.0, 258.0, 696.0, 773.0, 1086.0, 619.0, 294.0, 411.0, 47.0, 0.0, 53.0, 661.0, 1689.0, 1304.0, 1928.0, 451.0, 274.0, 391.0, 1103.0, 2311.0, 67.0, 1735.0, 1991.0, 562.0, 139.0, 956.0, 492.0, 331.0, 1302.0, 1623.0, 425.0, 331.0, 930.0, 989.0, 653.0, 352.0, 27.0, 636.0, 948.0, 1558.0, 136.0, 392.0, 499.0, 1253.0, 317.0, 391.0, 868.0, 649.0, 947.0, 800.0, 106.0, 1403.0, 553.0, 2159.0, 1724.0, 552.0, 208.0, 32.0, 358.0, 1188.0, 1409.0, 331.0, 137.0, 1409.0, 322.0, 728.0, 577.0, 432.0, 1306.0, 273.0, 2815.0, 59.0, 1229.0, 259.0, 378.0, 1969.0, 190.0, 2774.0, 1985.0, 1169.0, 256.0, 2353.0, 317.0, 640.0, 643.0, 435.0, 937.0, 947.0, 661.0, 323.0, 1112.0, 2903.0, 966.0, 1735.0, 1260.0, 348.0, 1947.0, 557.0, 478.0, 576.0, 148.0, 241.0, 2075.0, 544.0, 1344.0, 1618.0, 1231.0, 1710.0, 438.0, 306.0, 590.0, 2952.0, 797.0, 1663.0, 1623.0, 768.0, 2909.0, 472.0, 823.0, 883.0, 914.0, 720.0, 1062.0, 620.0, 956.0, 1087.0, 1040.0, 617.0, 1598.0, 947.0, 46.0, 663.0, 1231.0, 293.0, 2704.0, 1011.0, 2402.0, 46.0, 970.0, 678.0, 238.0, 615.0, 1065.0, 949.0, 1640.0, 1226.0, 956.0, 832.0, 1745.0, 1751.0, 552.0, 905.0, 183.0, 1688.0, 169.0, 1950.0, 1396.0, 1625.0, 575.0, 534.0, 0.0, 138.0, 295.0, 336.0, 620.0, 117.0, 29.0, 1382.0, 532.0, 1557.0, 1592.0, 180.0, 988.0, 312.0, 617.0, 459.0, 748.0, 238.0, 135.0, 46.0, 1546.0, 268.0, 380.0, 200.0, 2229.0, 682.0, 327.0, 1141.0, 1342.0, 105.0, 514.0, 300.0, 214.0, 140.0, 83.0, 649.0, 2697.0, 49.0, 50.0, 2240.0, 528.0, 956.0, 1002.0, 355.0, 798.0, 399.0, 2602.0, 1640.0, 711.0, 1344.0, 208.0, 1465.0, 1038.0, 729.0, 50.0, 782.0, 189.0, 1601.0, 222.0, 1687.0, 133.0, 112.0, 373.0, 106.0, 1848.0, 1419.0, 882.0, 151.0, 638.0, 278.0, 1029.0, 1663.0, 2127.0, 731.0, 64.0, 1034.0, 1301.0, 1332.0, 360.0, 590.0, 1227.0, 1066.0, 48.0, 827.0, 1981.0, 402.0, 276.0, 236.0, 370.0, 2467.0, 293.0, 2177.0, 311.0, 728.0, 1435.0, 2704.0, 2262.0, 1954.0, 1601.0, 2151.0, 2014.0, 1758.0, 932.0, 2863.0, 1601.0, 1164.0, 2759.0, 570.0, 90.0, 138.0, 2602.0, 3433.0, 56.0, 503.0, 1809.0, 121.0, 1713.0, 768.0, 452.0, 958.0, 2163.0, 39.0, 1598.0, 1066.0, 842.0, 592.0, 661.0, 661.0, 794.0, 192.0, 424.0, 113.0, 1059.0, 734.0, 388.0, 213.0, 48.0, 931.0, 1311.0, 1533.0, 109.0, 914.0, 1019.0, 585.0, 613.0, 347.0, 1033.0, 1113.0, 58.0, 1465.0, 2174.0, 2234.0, 1066.0, 2139.0, 68.0, 1077.0, 554.0, 2109.0, 162.0, 1814.0, 871.0, 306.0, 65.0, 400.0, 65.0, 29.0, 797.0, 586.0, 1186.0, 935.0, 1892.0, 665.0, 792.0, 59.0, 355.0, 653.0, 1503.0, 342.0, 183.0, 55.0, 90.0, 755.0, 2.0, 891.0, 192.0, 372.0, 432.0, 134.0, 1684.0, 290.0, 173.0, 1174.0, 29.0, 484.0, 549.0, 1008.0, 1301.0, 215.0, 1758.0, 114.0, 440.0, 109.0, 1370.0, 141.0, 192.0, 246.0, 577.0, 728.0, 355.0, 590.0, 946.0, 936.0, 3006.0, 2248.0, 539.0, 993.0, 403.0, 2164.0, 46.0, 755.0, 138.0, 947.0, 585.0, 117.0, 211.0, 2570.0, 59.0, 207.0, 273.0, 798.0, 2144.0, 2325.0, 213.0, 57.0, 1431.0, 1449.0, 2628.0, 404.0, 195.0, 300.0, 1187.0, 1442.0, 273.0, 728.0, 244.0, 286.0, 144.0, 636.0, 568.0, 1249.0, 489.0, 1119.0, 1612.0, 328.0, 1617.0, 781.0, 1663.0, 310.0, 729.0, 1273.0, 295.0, 244.0, 1132.0, 1594.0, 391.0, 1694.0, 661.0, 433.0, 643.0, 1299.0, 386.0, 114.0, 283.0, 115.0, 1644.0, 177.0, 184.0, 236.0, 302.0, 823.0, 199.0, 2132.0, 590.0, 207.0, 37.0, 3113.0, 244.0, 468.0, 1347.0, 199.0, 1808.0, 357.0, 1191.0, 955.0, 2008.0, 586.0, 65.0, 499.0, 285.0, 792.0, 1041.0, 914.0, 636.0, 408.0, 1725.0, 827.0, 456.0, 1004.0, 1081.0, 1880.0, 1440.0, 615.0, 667.0, 3.0, 4.0, 1435.0, 647.0, 1895.0, 999.0, 2075.0, 1075.0, 761.0, 400.0, 228.0, 684.0, 709.0, 1500.0, 307.0, 661.0, 256.0, 402.0, 1596.0, 1349.0, 582.0, 994.0, 1796.0, 527.0, 905.0, 988.0, 706.0, 112.0, 99.0, 321.0, 1033.0, 1006.0, 661.0, 1195.0, 1000.0, 187.0, 342.0, 1515.0, 1457.0, 2619.0, 1241.0, 1000.0, 723.0, 323.0, 65.0, 2.0, 688.0, 30.0, 988.0, 661.0, 562.0, 902.0, 53.0, 151.0, 1220.0, 1273.0, 298.0, 23.0, 997.0, 151.0, 721.0, 1942.0, 347.0, 139.0, 581.0, 684.0, 883.0, 215.0, 2366.0, 661.0, 2062.0, 649.0, 464.0, 79.0, 321.0, 112.0, 259.0, 405.0, 355.0, 682.0, 348.0, 525.0, 375.0, 586.0, 913.0, 2686.0, 1695.0, 926.0, 997.0, 1620.0, 1735.0, 342.0, 202.0, 792.0, 180.0, 345.0, 577.0, 792.0, 1901.0, 349.0, 506.0, 1195.0, 910.0, 331.0, 1661.0, 643.0, 1049.0, 440.0, 798.0, 376.0, 166.0, 128.0, 2493.0, 1166.0, 42.0, 259.0, 661.0, 138.0, 51.0, 2557.0, 1528.0, 189.0, 828.0, 631.0, 238.0, 1745.0, 499.0, 1525.0, 121.0, 906.0, 185.0, 1609.0, 1006.0, 451.0, 355.0, 386.0, 1713.0, 2863.0, 419.0, 661.0, 1260.0, 532.0, 105.0, 1332.0, 941.0, 828.0, 577.0, 1519.0, 615.0, 1458.0, 846.0, 829.0, 761.0, 914.0, 570.0, 1526.0, 276.0, 1002.0, 134.0, 198.0, 50.0, 55.0, 3227.0, 2212.0, 1442.0, 499.0, 1970.0, 1273.0, 0.0, 1003.0, 1437.0, 42.0, 947.0, 2.0, 475.0, 1519.0, 768.0, 1409.0, 1222.0, 488.0, 357.0, 274.0, 492.0, 632.0, 2338.0, 552.0, 65.0, 649.0, 2561.0, 1543.0, 1105.0, 240.0, 2143.0, 610.0, 373.0, 355.0, 1686.0, 310.0, 1751.0, 2131.0, 574.0, 51.0, 1323.0, 989.0, 267.0, 644.0, 199.0, 1396.0, 2159.0, 1362.0, 724.0, 121.0, 1735.0, 1764.0, 661.0, 195.0, 139.0, 1020.0, 782.0, 1618.0, 56.0, 827.0, 988.0, 1931.0, 130.0, 1264.0, 144.0, 970.0, 827.0, 1262.0, 2352.0, 783.0, 946.0, 144.0, 47.0, 590.0, 215.0, 776.0, 615.0, 1123.0, 109.0, 1469.0, 706.0, 130.0, 130.0, 577.0, 1041.0, 1737.0, 958.0, 319.0, 1885.0, 2744.0, 224.0, 532.0, 259.0, 575.0, 682.0, 954.0, 517.0, 310.0, 227.0, 42.0, 1299.0, 840.0, 3433.0, 190.0, 2179.0, 2089.0, 121.0, 899.0, 2783.0, 313.0, 67.0, 914.0, 402.0, 468.0, 399.0, 663.0, 385.0, 196.0, 682.0, 1309.0, 1076.0, 1199.0, 1065.0, 846.0, 312.0, 311.0, 139.0, 827.0, 438.0, 590.0, 605.0, 1039.0, 3268.0, 403.0, 946.0, 2571.0, 53.0, 215.0, 788.0, 139.0, 472.0, 380.0, 1010.0, 1037.0, 1318.0, 1673.0, 39.0, 457.0, 661.0, 53.0, 231.0, 866.0, 911.0, 591.0, 1694.0, 622.0, 729.0, 515.0, 180.0, 674.0, 1167.0, 1714.0, 2845.0, 183.0, 1419.0, 531.0, 2035.0, 1091.0, 184.0, 746.0, 655.0, 121.0, 932.0, 352.0, 90.0, 196.0, 130.0, 1112.0, 190.0, 247.0, 2942.0, 195.0, 711.0, 352.0, 153.0, 1178.0, 1006.0, 499.0, 482.0, 58.0, 336.0, 170.0, 1908.0, 869.0, 600.0, 3022.0, 1969.0, 1820.0, 267.0, 56.0, 0.0, 1406.0, 894.0, 1039.0, 386.0, 1106.0, 198.0, 468.0, 1222.0, 46.0, 106.0, 1105.0, 562.0, 277.0, 65.0, 673.0, 665.0, 178.0, 832.0, 1451.0, 2105.0, 1437.0, 1612.0, 66.0, 949.0, 295.0, 1123.0, 278.0, 87.0, 1262.0, 130.0, 198.0, 318.0, 806.0, 1070.0, 400.0, 130.0, 389.0, 615.0, 972.0, 792.0, 2342.0, 186.0, 1062.0, 705.0, 624.0, 343.0, 443.0, 29.0, 197.0, 192.0, 1610.0, 290.0, 36.0, 1599.0, 630.0, 588.0, 306.0, 731.0, 859.0, 197.0, 37.0, 1677.0, 950.0, 2270.0, 2678.0, 1006.0, 212.0, 600.0, 434.0, 484.0, 494.0, 333.0, 647.0, 953.0, 508.0, 989.0, 989.0, 295.0, 79.0, 2075.0, 311.0, 717.0, 2230.0, 1706.0, 444.0, 649.0, 1603.0, 717.0, 661.0, 936.0, 1299.0, 403.0, 728.0, 524.0, 215.0, 761.0, 1877.0, 1223.0, 1604.0, 1002.0, 1535.0, 2072.0, 1406.0, 1765.0, 111.0, 614.0, 566.0, 277.0, 443.0, 189.0, 41.0, 532.0, 1872.0, 310.0, 742.0, 491.0, 1049.0, 524.0, 272.0, 298.0, 661.0, 159.0, 733.0, 2243.0, 2821.0, 1681.0, 1295.0, 58.0, 1815.0, 1461.0, 661.0, 491.0, 717.0, 564.0, 797.0, 302.0, 914.0, 1085.0, 883.0, 2512.0, 857.0, 299.0, 848.0, 956.0, 1227.0, 274.0, 1240.0, 1090.0, 133.0, 1815.0, 1017.0, 914.0, 203.0, 441.0, 914.0, 207.0, 1195.0, 615.0, 958.0, 355.0, 661.0, 814.0, 267.0, 256.0, 881.0, 2325.0, 1965.0, 198.0, 782.0, 440.0, 306.0, 2822.0, 1430.0, 58.0, 905.0, 149.0, 1771.0, 450.0, 130.0, 34.0, 1255.0, 3288.0, 136.0, 1332.0, 1195.0, 2136.0, 502.0, 452.0, 518.0, 208.0, 1519.0, 302.0, 610.0, 881.0, 125.0, 781.0, 617.0, 477.0, 1437.0, 236.0, 2199.0, 332.0, 848.0, 1020.0, 568.0, 562.0, 1268.0, 444.0, 473.0, 829.0, 1558.0, 610.0, 1072.0, 273.0, 2226.0, 379.0, 379.0, 90.0, 457.0, 1610.0, 1273.0, 372.0, 633.0, 453.0, 1918.0, 456.0, 495.0, 190.0, 679.0, 684.0, 106.0, 322.0, 294.0, 1300.0, 1187.0, 1516.0, 539.0, 619.0, 891.0, 400.0, 124.0, 1560.0, 491.0, 204.0, 1.0, 1286.0, 479.0, 619.0, 90.0, 403.0, 562.0, 1735.0, 500.0, 728.0, 653.0, 653.0, 511.0, 189.0, 195.0, 457.0, 3028.0, 1373.0, 258.0, 706.0, 158.0, 905.0, 1428.0, 2055.0, 1047.0, 3341.0, 808.0, 1227.0, 827.0, 1745.0, 947.0, 800.0, 566.0, 940.0, 502.0, 914.0, 1953.0, 1268.0, 684.0, 792.0, 211.0, 425.0, 1745.0, 244.0, 259.0, 914.0, 720.0, 453.0, 1018.0, 448.0, 394.0, 247.0, 286.0, 2065.0, 614.0, 135.0, 930.0, 1769.0, 792.0, 2503.0, 866.0, 1120.0, 503.0, 1623.0, 1598.0, 661.0, 146.0, 190.0, 1524.0, 196.0, 1990.0, 277.0, 1718.0, 38.0, 950.0, 367.0, 947.0, 1849.0, 189.0, 435.0, 1985.0, 243.0, 453.0, 192.0, 1789.0, 130.0, 1695.0, 1977.0, 403.0, 459.0, 215.0, 538.0, 184.0, 1686.0, 729.0, 661.0, 321.0, 500.0, 441.0, 768.0, 645.0, 2470.0, 1217.0, 1029.0, 207.0, 357.0, 1807.0, 2350.0, 545.0, 343.0, 360.0, 2295.0, 443.0, 848.0, 2393.0, 1352.0, 303.0, 950.0, 866.0, 1087.0, 301.0, 790.0, 950.0, 965.0, 705.0, 212.0, 642.0, 663.0, 277.0, 287.0, 121.0, 401.0, 1187.0, 352.0, 984.0, 1981.0, 1212.0, 1599.0, 1195.0, 651.0, 68.0, 1332.0, 1362.0, 352.0, 236.0, 616.0, 1004.0, 1299.0, 443.0, 456.0, 126.0, 521.0, 888.0, 829.0, 1598.0, 376.0, 411.0, 1489.0, 294.0, 213.0, 985.0, 66.0, 1312.0, 953.0, 384.0, 2001.0, 450.0, 299.0, 142.0, 296.0, 115.0, 67.0, 1706.0, 1908.0, 765.0, 1519.0, 152.0, 570.0, 61.0, 293.0, 367.0, 940.0, 823.0, 347.0, 2270.0, 1196.0, 2664.0, 1647.0, 385.0, 296.0, 2164.0, 2427.0, 949.0, 907.0, 523.0, 558.0, 376.0, 154.0, 114.0, 930.0, 942.0, 59.0, 636.0, 1337.0, 840.0, 2844.0, 317.0, 316.0, 569.0, 649.0, 1571.0, 784.0, 309.0, 1020.0, 263.0, 273.0, 294.0, 882.0, 424.0, 1004.0, 46.0, 936.0, 2403.0, 139.0, 665.0, 258.0, 819.0, 136.0, 311.0, 385.0, 289.0, 552.0, 756.0, 236.0, 1243.0, 642.0, 1954.0, 564.0, 1869.0, 1599.0, 1667.0, 2482.0, 1596.0, 629.0, 41.0, 1614.0, 452.0, 159.0, 29.0, 134.0, 997.0, 1274.0, 3094.0, 278.0, 267.0, 1087.0, 401.0, 1816.0, 2294.0, 531.0, 431.0, 124.0, 1.0, 309.0, 214.0, 918.0, 63.0, 696.0, 1503.0, 149.0, 268.0, 17.0, 1869.0, 2280.0, 2325.0, 139.0, 866.0, 2177.0, 1048.0, 207.0, 190.0, 69.0, 1231.0, 279.0, 402.0, 307.0, 215.0, 1803.0, 2125.0, 956.0, 170.0, 684.0, 133.0, 891.0, 2022.0, 332.0, 46.0, 113.0, 1487.0, 650.0, 139.0, 1067.0, 1954.0, 1524.0, 630.0, 1195.0, 1620.0, 1043.0, 3234.0, 368.0, 1610.0, 261.0, 328.0, 160.0, 1709.0, 3.0, 1714.0, 682.0, 400.0, 617.0, 139.0, 130.0, 1187.0, 609.0, 215.0, 723.0, 727.0, 1598.0, 36.0, 1106.0, 211.0, 989.0, 577.0, 1008.0, 1519.0, 1222.0, 2574.0, 293.0, 661.0, 792.0, 2199.0, 300.0, 1440.0, 432.0, 1304.0, 1440.0, 1599.0, 133.0, 661.0, 1221.0, 1616.0, 1286.0, 321.0, 1640.0, 1002.0, 1248.0, 376.0, 500.0, 727.0, 1908.0, 1153.0, 580.0, 955.0, 1302.0, 192.0, 649.0, 781.0, 334.0, 1311.0, 385.0, 3316.0, 195.0, 1435.0, 1311.0, 358.0, 343.0, 403.0, 179.0, 55.0, 791.0, 184.0, 682.0, 1.0, 523.0, 587.0, 1062.0, 465.0, 1143.0, 1449.0, 1091.0, 1816.0, 306.0, 1242.0, 930.0, 180.0, 278.0, 1086.0, 617.0, 386.0, 1467.0, 383.0, 617.0, 1747.0, 109.0, 355.0, 1546.0, 950.0, 67.0, 781.0, 827.0, 1312.0, 431.0, 278.0, 1413.0, 883.0, 124.0, 3020.0, 213.0, 2008.0, 1233.0, 883.0, 1699.0, 1420.0, 644.0, 617.0, 989.0, 2630.0, 1018.0, 619.0, 327.0, 1286.0, 885.0, 1502.0, 146.0, 59.0, 113.0, 604.0, 317.0, 782.0, 729.0, 111.0, 1675.0, 2321.0, 403.0, 170.0, 1049.0, 628.0, 65.0, 458.0, 2.0, 2044.0, 256.0, 400.0, 190.0, 438.0, 635.0, 1085.0, 2192.0, 1467.0, 1494.0, 661.0, 682.0, 2208.0, 484.0, 2482.0, 1181.0, 63.0, 1412.0, 1288.0, 299.0, 615.0, 1299.0, 1376.0, 413.0, 187.0, 1427.0, 2492.0, 180.0, 684.0, 456.0, 1179.0, 1751.0, 1232.0, 1020.0, 570.0, 29.0, 672.0, 1233.0, 1299.0, 400.0, 622.0, 1524.0, 312.0, 1457.0, 245.0, 1025.0, 187.0, 1418.0, 1930.0, 600.0, 649.0, 1068.0, 1264.0, 230.0, 499.0, 3171.0, 619.0, 511.0, 636.0, 1713.0, 715.0, 2139.0, 0.0, 145.0, 827.0, 192.0, 249.0, 362.0, 575.0, 1344.0, 92.0, 456.0, 425.0, 107.0, 1946.0, 177.0, 385.0, 180.0, 636.0, 661.0, 720.0, 177.0, 86.0, 590.0, 1745.0, 661.0, 3194.0, 140.0, 1620.0, 212.0, 2384.0, 3350.0, 557.0, 1852.0, 1385.0, 61.0, 465.0, 46.0, 1034.0, 1300.0, 955.0, 1713.0, 705.0, 788.0, 1302.0, 576.0, 792.0, 1803.0, 259.0, 258.0, 119.0, 1553.0, 857.0, 534.0, 717.0, 3298.0, 534.0, 2174.0, 1599.0, 797.0, 1499.0, 1442.0, 1651.0, 612.0, 999.0, 1640.0, 407.0, 655.0, 620.0, 1767.0, 2264.0, 32.0, 827.0, 731.0, 661.0, 549.0, 1016.0, 566.0, 2810.0, 1745.0, 1143.0, 259.0, 256.0, 400.0, 948.0, 1885.0, 88.0, 997.0, 377.0, 67.0, 665.0, 256.0, 2593.0, 571.0, 1170.0, 67.0, 124.0, 144.0, 914.0, 32.0, 1486.0, 800.0, 1735.0, 386.0, 472.0, 827.0, 511.0, 608.0, 2.0, 729.0, 320.0, 731.0, 394.0, 2073.0, 2404.0, 403.0, 1264.0, 1377.0, 514.0, 1166.0, 1216.0, 2221.0, 717.0, 334.0, 1186.0, 114.0, 236.0, 499.0, 3107.0, 2253.0, 790.0, 911.0, 46.0, 2040.0, 391.0, 554.0, 1299.0, 1299.0, 190.0, 196.0, 1869.0, 190.0, 168.0, 1261.0, 2551.0, 53.0, 756.0, 295.0, 2053.0, 1152.0, 1952.0, 579.0, 2628.0, 1815.0, 212.0, 1647.0, 1053.0, 948.0, 472.0, 409.0, 1066.0, 56.0, 55.0, 59.0, 500.0, 729.0, 1273.0, 684.0, 941.0, 1072.0, 3190.0, 536.0, 2044.0, 684.0, 883.0, 731.0, 1467.0, 797.0, 2132.0, 215.0, 135.0, 408.0, 1063.0, 461.0, 55.0, 1028.0, 1328.0, 45.0, 53.0, 1038.0, 749.0, 1114.0, 1461.0, 684.0, 294.0, 1300.0, 931.0, 1220.0, 400.0, 273.0, 1195.0, 869.0, 827.0, 424.0, 1437.0, 2035.0, 215.0, 661.0, 2926.0, 2199.0, 2121.0, 68.0, 768.0, 947.0, 562.0, 255.0, 378.0, 832.0, 989.0, 159.0, 1116.0, 207.0, 2972.0, 1575.0, 3.0, 1399.0, 984.0, 50.0, 1814.0, 492.0, 1442.0, 947.0, 215.0, 307.0, 823.0, 47.0, 144.0, 3394.0, 2745.0, 1244.0, 1625.0, 2181.0, 381.0, 728.0, 211.0, 68.0, 215.0, 1892.0, 106.0, 2164.0, 502.0, 613.0, 1020.0, 452.0, 1246.0, 1745.0, 1200.0, 29.0, 451.0, 456.0, 604.0, 259.0, 936.0, 286.0, 117.0, 513.0, 196.0, 420.0, 614.0, 407.0, 286.0, 829.0, 558.0, 1805.0, 619.0, 649.0, 1687.0, 731.0, 1612.0, 275.0, 1006.0, 468.0, 1599.0, 318.0, 1989.0, 301.0, 1495.0, 999.0, 306.0, 215.0, 418.0, 772.0, 1007.0, 29.0, 435.0, 1357.0, 545.0, 1735.0, 331.0, 1495.0, 207.0, 787.0, 32.0, 199.0, 577.0, 125.0, 192.0, 1594.0, 521.0, 592.0, 615.0, 1234.0, 185.0, 949.0, 1815.0, 245.0, 111.0, 577.0, 2128.0, 418.0, 1997.0, 728.0, 814.0, 452.0, 612.0, 1437.0, 247.0, 352.0, 86.0, 1074.0, 787.0, 1041.0, 1880.0, 46.0, 1018.0, 230.0, 1292.0, 905.0, 797.0, 278.0, 1299.0, 432.0, 952.0, 3201.0, 908.0, 1442.0, 2817.0, 370.0, 50.0, 1273.0, 1240.0, 684.0, 519.0, 303.0, 2256.0, 109.0, 2745.0, 661.0, 1249.0, 729.0, 1594.0, 552.0, 431.0, 882.0, 343.0, 615.0, 2214.0, 37.0, 2165.0, 881.0, 1465.0, 1879.0, 36.0, 800.0, 2427.0, 319.0, 317.0, 391.0, 649.0, 722.0, 195.0, 65.0, 49.0, 341.0, 710.0, 189.0, 370.0, 178.0, 1686.0, 827.0, 2418.0, 200.0, 829.0, 403.0, 647.0, 776.0, 189.0, 711.0, 396.0, 230.0, 420.0, 607.0, 79.0, 461.0, 780.0, 478.0, 1008.0, 29.0, 866.0, 569.0, 697.0, 66.0, 1348.0, 352.0, 308.0, 620.0, 932.0, 534.0, 404.0, 500.0, 357.0, 885.0, 955.0, 443.0, 745.0, 577.0, 2144.0, 27.0, 988.0, 451.0, 177.0, 1771.0, 114.0, 385.0, 1502.0, 1188.0, 2265.0, 1004.0, 1809.0, 576.0, 124.0, 928.0, 1885.0, 2.0, 158.0, 310.0, 393.0, 204.0, 203.0, 859.0, 1148.0, 385.0, 399.0, 1424.0, 65.0, 1623.0, 1416.0, 556.0, 399.0, 548.0, 54.0, 33.0, 1123.0, 942.0, 114.0, 969.0, 2962.0, 317.0, 2046.0, 1013.0, 2718.0, 556.0, 949.0, 1735.0, 29.0, 782.0, 974.0, 386.0, 697.0, 943.0, 504.0, 420.0, 1299.0, 1457.0, 379.0, 1704.0, 357.0, 1043.0, 1247.0, 672.0, 541.0, 332.0, 651.0, 1085.0, 147.0, 1033.0, 207.0, 503.0, 2929.0, 508.0, 799.0, 1853.0, 374.0, 590.0, 381.0, 196.0, 940.0, 280.0, 3227.0, 144.0, 36.0, 197.0, 500.0, 1952.0, 471.0, 394.0, 195.0, 90.0, 870.0, 2121.0, 819.0, 430.0, 722.0, 577.0, 1065.0, 1437.0, 154.0, 1152.0, 468.0, 496.0, 2882.0, 418.0, 432.0, 139.0, 1195.0, 46.0, 1305.0, 1269.0, 1942.0, 400.0, 522.0, 37.0, 1178.0, 722.0, 914.0, 480.0, 1969.0, 727.0, 184.0, 479.0, 512.0, 121.0, 483.0, 2181.0, 236.0, 144.0, 1889.0, 2035.0, 1941.0, 1309.0, 1049.0, 346.0, 2248.0, 399.0, 121.0, 294.0, 309.0, 2657.0, 649.0, 1645.0, 1065.0, 551.0, 711.0, 2174.0, 1484.0, 534.0, 297.0, 135.0, 947.0, 33.0, 275.0, 495.0, 1088.0, 2016.0, 352.0, 2321.0, 871.0, 661.0, 806.0, 1469.0, 236.0, 941.0, 668.0, 3311.0, 478.0, 399.0, 212.0, 2227.0, 1885.0, 672.0, 827.0, 1942.0, 539.0, 323.0, 452.0, 47.0, 278.0, 2465.0, 504.0, 905.0, 88.0, 2181.0, 312.0, 1528.0, 1059.0, 1986.0, 1456.0, 438.0, 2392.0, 1571.0, 888.0, 310.0, 947.0, 1324.0, 138.0, 30.0, 1188.0, 106.0, 1648.0, 949.0, 180.0, 399.0, 782.0, 940.0, 335.0, 391.0, 724.0, 234.0, 212.0, 1006.0, 1731.0, 221.0, 843.0, 334.0, 2593.0, 777.0, 253.0, 222.0, 248.0, 534.0, 400.0, 800.0, 34.0, 2525.0, 1237.0, 727.0, 619.0, 1774.0, 376.0, 503.0, 1066.0, 784.0, 502.0, 1049.0, 2.0, 668.0, 926.0, 2308.0, 499.0, 411.0, 67.0, 862.0, 2482.0, 114.0, 3165.0, 211.0, 312.0, 754.0, 1342.0, 1226.0, 319.0, 876.0, 438.0, 320.0, 2062.0, 936.0, 1967.0, 458.0, 139.0, 827.0, 109.0, 1019.0, 1066.0, 1068.0, 932.0, 187.0, 357.0, 2193.0, 362.0, 764.0, 32.0, 1195.0, 857.0, 139.0, 761.0, 562.0, 277.0, 156.0, 1867.0, 1011.0, 586.0, 682.0, 191.0, 1299.0, 1663.0, 723.0, 1.0, 204.0, 263.0, 448.0, 317.0, 4.0, 1509.0, 306.0, 48.0, 2709.0, 438.0, 1242.0, 2177.0, 149.0, 1029.0, 504.0, 789.0, 360.0, 433.0, 661.0, 253.0, 3.0, 346.0, 816.0, 248.0, 1534.0, 484.0, 53.0, 111.0, 3285.0, 357.0, 928.0, 1689.0, 1437.0, 2199.0, 307.0, 560.0, 1686.0, 777.0, 236.0, 3200.0, 219.0, 1495.0, 197.0, 495.0, 1457.0, 446.0, 195.0, 828.0, 128.0, 2157.0, 1693.0, 432.0, 2141.0, 971.0, 1222.0, 1299.0, 1088.0, 2227.0, 312.0, 1612.0, 323.0, 661.0, 134.0, 483.0, 393.0, 989.0, 988.0, 213.0, 64.0, 306.0, 507.0, 53.0, 502.0, 1849.0, 412.0, 765.0, 453.0, 498.0, 1224.0, 649.0, 1302.0, 834.0, 300.0, 1618.0, 277.0, 597.0, 378.0, 246.0, 1467.0, 1741.0, 539.0, 2671.0, 3070.0, 81.0, 276.0, 278.0, 133.0, 730.0, 135.0, 2073.0, 106.0, 2811.0, 67.0, 586.0, 258.0, 569.0, 140.0, 465.0, 1818.0, 48.0, 781.0, 2353.0, 402.0, 443.0, 649.0, 256.0, 1557.0, 590.0, 617.0, 1442.0, 272.0, 1877.0, 329.0, 438.0, 1612.0, 411.0, 1348.0, 1751.0, 638.0, 400.0, 913.0, 1880.0, 51.0, 1255.0, 273.0, 2165.0, 1409.0, 310.0, 195.0, 3006.0, 911.0, 99.0, 401.0, 840.0, 531.0, 781.0, 31.0, 2119.0, 432.0, 400.0, 503.0, 108.0, 144.0, 429.0, 958.0, 396.0, 806.0, 295.0, 109.0, 989.0, 523.0, 1987.0, 792.0, 799.0, 117.0, 2174.0, 1002.0, 1304.0, 77.0, 400.0, 1849.0, 711.0, 942.0, 113.0, 959.0, 519.0, 430.0, 1434.0, 661.0, 829.0, 1694.0, 312.0, 297.0, 409.0, 1606.0, 549.0, 32.0, 954.0, 130.0, 54.0, 1188.0, 1540.0, 453.0, 1269.0, 293.0, 1006.0, 311.0, 434.0, 57.0, 1393.0, 595.0, 444.0, 840.0, 482.0, 400.0, 400.0, 728.0, 649.0, 1612.0, 1707.0, 237.0, 891.0, 400.0, 549.0, 320.0, 575.0, 1206.0, 2255.0, 385.0, 47.0, 1179.0, 819.0, 661.0, 267.0, 2267.0, 1059.0, 483.0, 989.0, 255.0, 661.0, 783.0, 714.0, 601.0, 436.0, 1883.0, 159.0, 409.0, 392.0, 1002.0, 2942.0, 617.0, 30.0, 717.0, 947.0, 1399.0, 2195.0, 777.0, 3033.0, 1500.0, 562.0, 3.0, 155.0, 117.0, 764.0, 1062.0, 264.0, 831.0, 604.0, 2602.0, 1241.0, 2035.0, 2174.0, 385.0, 1840.0, 513.0, 590.0, 212.0, 582.0, 190.0, 720.0, 828.0, 56.0, 325.0, 41.0, 411.0, 49.0, 1699.0, 27.0, 932.0, 866.0, 1115.0, 569.0, 729.0, 177.0, 619.0, 759.0, 56.0, 1068.0, 2139.0, 1003.0, 50.0, 1144.0, 661.0, 697.0, 1848.0, 1299.0, 828.0, 331.0, 2923.0, 858.0, 1599.0, 522.0, 148.0, 577.0, 375.0, 317.0, 80.0, 231.0, 1983.0, 1219.0, 547.0, 619.0, 2684.0, 491.0, 1065.0, 2349.0, 974.0, 962.0, 1875.0, 319.0, 987.0, 325.0, 316.0, 185.0, 1120.0, 46.0, 1465.0, 1571.0, 177.0, 513.0, 2103.0, 667.0, 1066.0, 278.0, 642.0, 349.0, 203.0, 906.0, 221.0, 617.0, 1576.0, 391.0, 131.0, 1562.0, 2392.0, 2007.0, 346.0, 1047.0, 1188.0, 262.0, 932.0, 130.0, 988.0, 892.0, 155.0, 932.0, 1018.0, 352.0, 2503.0, 481.0, 661.0, 308.0, 870.0, 438.0, 2008.0, 661.0, 2353.0, 253.0, 653.0, 2075.0, 684.0, 40.0, 274.0, 539.0, 615.0, 189.0, 1188.0, 344.0, 1082.0, 378.0, 1085.0, 370.0, 196.0, 1077.0, 989.0, 351.0, 108.0, 106.0, 2353.0, 121.0, 207.0, 1727.0, 944.0, 647.0, 140.0, 612.0, 255.0, 55.0, 1809.0, 439.0, 784.0, 559.0, 1118.0, 299.0, 214.0, 109.0, 1880.0, 1002.0, 844.0, 223.0, 829.0, 1264.0, 273.0, 1382.0, 87.0, 403.0, 1442.0, 1249.0, 440.0, 2923.0, 344.0, 405.0, 224.0, 2008.0, 195.0, 408.0, 84.0, 1818.0, 306.0, 311.0, 169.0, 1616.0, 2616.0, 657.0, 1725.0, 432.0, 386.0, 287.0, 2560.0, 2150.0, 1195.0, 177.0, 759.0, 728.0, 1640.0, 432.0, 1796.0, 1393.0, 237.0, 196.0, 408.0, 1004.0, 483.0, 162.0, 609.0, 1941.0, 597.0, 3230.0, 1212.0, 1332.0, 1487.0, 580.0, 848.0, 1306.0, 1025.0, 2345.0, 1393.0, 1286.0, 2212.0, 456.0, 311.0, 200.0, 1062.0, 776.0, 622.0, 199.0, 1705.0, 294.0, 1216.0, 482.0, 955.0, 615.0, 236.0, 610.0, 1229.0, 256.0, 1143.0, 617.0, 905.0, 1526.0, 974.0, 193.0, 866.0, 112.0, 1257.0, 696.0, 1509.0, 661.0, 2046.0, 882.0, 264.0, 1598.0, 831.0, 130.0, 806.0, 83.0, 947.0, 121.0, 1651.0, 1188.0, 1687.0, 1033.0, 1221.0, 1334.0, 471.0, 211.0, 756.0, 443.0, 305.0, 306.0, 649.0, 453.0, 491.0, 629.0, 1747.0, 412.0, 419.0, 1072.0, 1017.0, 30.0, 484.0, 661.0, 46.0, 464.0, 405.0, 2311.0, 29.0, 443.0, 1892.0, 135.0, 513.0, 954.0, 1977.0, 293.0, 306.0, 144.0, 853.0, 214.0, 962.0, 428.0, 1486.0, 1210.0, 141.0, 1990.0, 680.0, 370.0, 121.0, 2293.0, 1062.0, 2059.0, 68.0, 3.0, 400.0, 49.0, 1143.0, 473.0, 1873.0, 198.0, 1058.0, 723.0, 1568.0, 1442.0, 482.0, 257.0, 590.0, 697.0, 2109.0, 268.0, 1020.0, 403.0, 139.0, 1081.0, 55.0, 1241.0, 1526.0, 1665.0, 312.0, 190.0, 1133.0, 180.0, 372.0, 800.0, 432.0, 503.0, 1374.0, 134.0, 1695.0, 881.0, 432.0, 65.0, 1077.0, 403.0, 2710.0, 1188.0, 554.0, 711.0, 38.0, 411.0, 1942.0, 2255.0, 1598.0, 149.0, 684.0, 881.0, 400.0, 1085.0, 29.0, 649.0, 647.0, 2271.0, 955.0, 2903.0, 121.0, 424.0, 1019.0, 159.0, 134.0, 1186.0, 199.0, 1206.0, 152.0, 1268.0, 1065.0, 1598.0, 2100.0, 612.0, 138.0, 672.0, 2247.0, 3.0, 932.0, 1931.0, 1373.0, 742.0, 1424.0, 319.0, 428.0, 385.0, 2422.0, 1596.0, 827.0, 1761.0, 215.0, 649.0, 948.0, 507.0, 782.0, 1620.0, 1767.0, 797.0, 854.0, 321.0, 1992.0, 617.0, 2582.0, 319.0, 343.0, 1435.0, 1063.0, 3.0, 1614.0, 1735.0, 2616.0, 1818.0, 1984.0, 2627.0, 491.0, 1062.0, 168.0, 43.0, 1088.0, 610.0, 955.0, 276.0, 553.0, 136.0, 906.0, 1767.0, 1523.0, 190.0, 1020.0, 320.0, 294.0, 797.0, 1434.0, 306.0, 1167.0, 2876.0, 831.0, 412.0, 196.0, 405.0, 684.0, 1195.0, 2166.0, 50.0, 1249.0, 655.0, 1508.0, 184.0, 201.0, 988.0, 1026.0, 619.0, 490.0, 419.0, 1798.0, 948.0, 649.0, 3298.0, 2007.0, 55.0, 221.0, 2902.0, 590.0, 1009.0, 1113.0, 1280.0, 955.0, 1040.0, 612.0, 644.0, 673.0, 772.0, 362.0, 327.0, 287.0, 610.0, 321.0, 1085.0, 99.0, 378.0, 244.0, 300.0, 546.0, 202.0, 823.0, 2321.0, 404.0, 277.0, 1003.0, 1375.0, 395.0, 1002.0, 661.0, 1229.0, 457.0, 1144.0, 1501.0, 819.0, 1233.0, 1614.0, 661.0, 1815.0, 906.0, 1240.0, 947.0, 614.0, 2617.0, 111.0, 512.0, 610.0, 2349.0, 667.0, 425.0, 869.0, 500.0, 256.0, 846.0, 714.0, 113.0, 268.0, 46.0, 1040.0, 2242.0, 67.0, 446.0, 370.0, 160.0, 319.0, 124.0, 195.0, 2400.0, 395.0, 2174.0, 183.0, 969.0, 848.0, 263.0, 3029.0, 914.0, 661.0, 114.0, 65.0, 399.0, 1880.0, 31.0, 1686.0, 710.0, 491.0, 208.0, 1805.0, 1707.0, 135.0, 200.0, 432.0, 628.0, 460.0, 275.0, 460.0, 244.0, 134.0, 134.0, 1515.0, 1686.0, 2009.0, 1025.0, 854.0, 884.0, 683.0, 946.0, 3016.0, 515.0, 402.0, 480.0, 285.0, 136.0, 513.0, 1260.0, 197.0, 577.0, 479.0, 401.0, 1885.0, 636.0, 195.0, 200.0, 2165.0, 914.0, 881.0, 411.0, 1491.0, 3.0, 1377.0, 312.0, 1065.0, 1981.0, 612.0, 1072.0, 1519.0, 111.0, 50.0, 380.0, 641.0, 823.0, 59.0, 151.0, 221.0, 950.0, 303.0, 1872.0, 207.0, 1771.0, 59.0, 681.0, 871.0, 997.0, 1957.0, 2574.0, 790.0, 1295.0, 185.0, 684.0, 2619.0, 629.0, 121.0, 1610.0, 1303.0, 1076.0, 557.0, 651.0, 1462.0, 283.0, 891.0, 370.0, 64.0, 764.0, 731.0, 557.0, 204.0, 403.0, 311.0, 400.0, 200.0, 914.0, 947.0, 1435.0, 936.0, 312.0, 1074.0, 2179.0, 882.0, 486.0, 122.0, 2792.0, 722.0, 551.0, 60.0, 244.0, 267.0, 207.0, 1599.0, 399.0, 1190.0, 128.0, 1731.0, 58.0, 566.0, 286.0, 40.0, 632.0, 432.0, 1504.0, 294.0, 1112.0, 55.0, 1890.0, 1280.0, 449.0, 661.0, 1303.0, 1064.0, 1019.0, 196.0, 303.0, 1348.0, 59.0, 458.0, 432.0, 1735.0, 1598.0, 1849.0, 1484.0, 1723.0, 207.0, 303.0, 1949.0, 386.0, 34.0, 576.0, 629.0, 293.0, 1301.0, 1017.0, 479.0, 130.0, 728.0, 636.0, 459.0, 3.0, 55.0, 1500.0, 1679.0, 957.0, 2162.0, 204.0, 380.0, 604.0, 109.0, 304.0, 2669.0, 2305.0, 1062.0, 914.0, 1161.0, 150.0, 663.0, 23.0, 840.0, 1571.0, 417.0, 857.0, 1066.0, 370.0, 1735.0, 447.0, 1205.0, 124.0, 412.0, 731.0, 180.0, 55.0, 777.0, 728.0, 424.0, 932.0, 526.0, 614.0, 135.0, 47.0, 215.0, 1489.0, 1016.0, 1771.0, 2347.0, 121.0, 310.0, 1241.0, 2096.0, 797.0, 559.0, 811.0, 65.0, 781.0, 2056.0, 343.0, 310.0, 48.0, 932.0, 383.0, 609.0, 41.0, 298.0, 954.0, 914.0, 1174.0, 1066.0, 1877.0, 278.0, 827.0, 55.0, 936.0, 949.0, 535.0, 60.0, 1751.0, 585.0, 649.0, 2078.0, 340.0, 341.0, 765.0, 1805.0, 2893.0, 151.0, 1178.0, 806.0, 2023.0, 1773.0, 1437.0, 1114.0, 333.0, 306.0, 214.0, 199.0, 2121.0, 545.0, 113.0, 47.0, 2174.0, 914.0, 1557.0, 158.0, 139.0, 657.0, 932.0, 319.0, 2482.0, 2008.0, 425.0, 305.0, 697.0, 41.0, 2332.0, 946.0, 204.0, 472.0, 1849.0, 538.0, 194.0, 939.0, 317.0, 213.0, 759.0, 443.0, 781.0, 781.0, 597.0, 30.0, 1755.0, 352.0, 1997.0, 1068.0, 1264.0, 443.0, 106.0, 1437.0, 1068.0, 1623.0, 192.0, 2161.0, 1264.0, 243.0, 574.0, 2688.0, 130.0, 495.0, 322.0, 781.0, 195.0, 2073.0, 1246.0, 1004.0, 386.0, 117.0, 614.0, 376.0, 1028.0, 1957.0, 1997.0, 614.0, 643.0, 211.0, 2455.0, 1063.0, 792.0, 614.0, 513.0, 1043.0, 866.0, 2810.0, 534.0, 1311.0, 1105.0, 894.0, 256.0, 843.0, 908.0, 481.0, 432.0, 736.0, 134.0, 953.0, 2949.0, 1114.0, 614.0, 1596.0, 581.0, 1740.0, 2392.0, 407.0, 2427.0, 792.0, 268.0, 1231.0, 832.0, 1485.0, 58.0, 407.0, 109.0, 281.0, 661.0, 211.0, 128.0, 144.0, 267.0, 1085.0, 2867.0, 620.0, 54.0, 883.0, 1026.0, 1269.0, 800.0, 306.0, 1815.0, 334.0, 1803.0, 205.0, 1981.0, 195.0, 195.0, 306.0, 400.0, 191.0, 649.0, 402.0, 1216.0, 628.0, 30.0, 1800.0, 384.0, 661.0, 1693.0, 401.0, 183.0, 716.0, 1849.0, 115.0, 199.0, 531.0, 332.0, 1599.0, 613.0, 525.0, 789.0, 1825.0, 349.0, 951.0, 661.0, 755.0, 950.0, 1066.0, 950.0, 1687.0, 586.0, 1380.0, 562.0, 513.0, 932.0, 615.0, 661.0, 259.0, 2001.0, 1066.0, 1303.0, 276.0, 939.0, 118.0, 743.0, 206.0, 2075.0, 230.0, 1985.0, 793.0, 566.0, 62.0, 956.0, 1819.0, 294.0, 1041.0, 27.0, 1418.0, 1598.0, 1885.0, 1188.0, 112.0, 500.0, 341.0, 31.0, 313.0, 220.0, 1370.0, 2181.0, 1724.0, 248.0, 1314.0, 1251.0, 1085.0, 1434.0, 294.0, 58.0, 1554.0, 96.0, 68.0, 352.0, 407.0, 1249.0, 253.0, 208.0, 384.0, 643.0, 477.0, 1892.0, 195.0, 828.0, 1223.0, 1233.0, 1183.0, 829.0, 195.0, 1786.0, 1617.0, 56.0, 207.0, 674.0, 199.0, 286.0, 1062.0, 121.0, 442.0, 792.0, 1442.0, 883.0, 829.0, 723.0, 871.0, 41.0, 31.0, 382.0, 322.0, 716.0, 1.0, 1066.0, 59.0, 989.0, 2193.0, 465.0, 55.0, 2007.0, 402.0, 1434.0, 1758.0, 1057.0, 643.0, 2555.0, 453.0, 1437.0, 1105.0, 195.0, 390.0, 1604.0, 769.0, 273.0, 258.0, 76.0, 894.0, 125.0, 1485.0, 106.0, 715.0, 947.0, 2628.0, 1897.0, 661.0, 2220.0, 1361.0, 199.0, 947.0, 447.0, 195.0, 1009.0, 442.0, 989.0, 264.0, 2314.0, 380.0, 190.0, 190.0, 2096.0, 1449.0, 866.0, 322.0, 499.0, 661.0, 264.0, 562.0, 400.0, 1123.0, 1049.0, 187.0, 947.0, 142.0, 941.0, 386.0, 170.0, 310.0, 161.0, 478.0, 570.0, 955.0, 326.0, 452.0, 486.0, 53.0, 1354.0, 201.0, 1696.0, 139.0, 122.0, 385.0, 1879.0, 1002.0, 1215.0, 2139.0, 1003.0, 590.0, 1437.0, 360.0, 388.0, 399.0, 1295.0, 661.0, 39.0, 1452.0, 1081.0, 1264.0, 402.0, 237.0, 1316.0, 1286.0, 133.0, 1305.0, 661.0, 1367.0, 729.0, 1728.0, 950.0, 1614.0, 2465.0, 882.0, 1877.0, 806.0, 258.0, 590.0, 2323.0, 350.0, 777.0, 620.0, 1233.0, 30.0, 535.0, 1005.0, 211.0, 391.0, 3070.0, 1242.0, 189.0, 130.0, 1981.0, 1733.0, 475.0, 310.0, 354.0, 739.0, 706.0, 1047.0, 29.0, 1187.0, 1274.0, 534.0, 3022.0, 716.0, 355.0, 30.0, 1112.0, 797.0, 1337.0, 914.0, 1508.0, 945.0, 178.0, 255.0, 121.0, 316.0, 786.0, 255.0, 1981.0, 37.0, 896.0, 881.0, 950.0, 307.0, 3110.0, 783.0, 1889.0, 716.0, 615.0, 450.0, 132.0, 610.0, 359.0, 33.0, 527.0, 2404.0, 1909.0, 432.0, 317.0, 522.0, 661.0, 402.0, 50.0, 1849.0, 756.0, 1004.0, 237.0, 41.0, 1186.0, 1131.0, 882.0, 613.0, 331.0, 1026.0, 653.0, 131.0, 829.0, 1128.0, 2528.0, 661.0, 782.0, 988.0, 151.0, 309.0, 1294.0, 1540.0, 661.0, 1257.0, 1591.0, 1561.0, 63.0, 1241.0, 696.0, 1144.0, 2053.0, 185.0, 393.0, 1877.0, 0.0, 867.0, 825.0, 310.0, 910.0, 1885.0, 432.0, 109.0, 243.0, 1226.0, 480.0, 937.0, 1004.0, 255.0, 2844.0, 29.0, 661.0, 2022.0, 138.0, 1020.0, 342.0, 661.0, 1087.0, 782.0, 930.0, 55.0, 709.0, 1815.0, 2429.0, 1523.0, 1406.0, 456.0, 590.0, 970.0, 502.0, 1971.0, 956.0, 636.0, 113.0, 195.0, 557.0, 1064.0, 932.0, 792.0, 272.0, 819.0, 1740.0, 756.0, 928.0, 697.0, 132.0, 545.0, 1745.0, 775.0, 1348.0, 2.0, 2680.0, 355.0, 146.0, 64.0, 2407.0, 509.0, 144.0, 1849.0, 482.0, 1143.0, 943.0, 1987.0, 439.0, 1599.0, 130.0, 186.0, 873.0, 1106.0, 2496.0, 1062.0, 114.0, 29.0, 298.0, 1229.0, 400.0, 340.0, 2162.0, 954.0, 300.0, 2242.0, 288.0, 69.0, 3.0, 471.0, 1476.0, 1590.0, 2127.0, 955.0, 54.0, 1006.0, 184.0, 617.0, 424.0, 34.0, 827.0, 135.0, 347.0, 199.0, 1273.0, 989.0, 1227.0, 1286.0, 108.0, 3353.0, 615.0, 1.0, 1286.0, 709.0, 119.0, 727.0, 1922.0, 132.0, 1066.0, 430.0, 273.0, 636.0, 42.0, 133.0, 483.0, 619.0, 1773.0, 1348.0, 2267.0, 110.0, 2075.0, 298.0, 2007.0, 2140.0, 673.0, 87.0, 112.0, 313.0, 869.0, 1499.0, 1614.0, 139.0, 320.0, 2691.0, 832.0, 253.0, 2373.0, 189.0, 409.0, 2109.0, 295.0, 2256.0, 1436.0, 1435.0, 1437.0, 1877.0, 121.0, 655.0, 502.0, 2573.0, 681.0, 2109.0, 2196.0, 236.0, 1502.0, 1187.0, 68.0, 1326.0, 517.0, 576.0, 1745.0, 121.0, 780.0, 862.0, 2403.0, 1328.0, 1767.0, 615.0, 196.0, 563.0, 1003.0, 1798.0, 2.0, 171.0, 158.0, 150.0, 179.0, 417.0, 2835.0, 782.0, 296.0, 1485.0, 400.0, 661.0, 42.0, 14.0, 128.0, 3145.0, 2221.0, 987.0, 64.0, 643.0, 510.0, 294.0, 401.0, 610.0, 808.0, 781.0, 380.0, 2371.0, 394.0, 513.0, 828.0, 2060.0, 636.0, 499.0, 617.0, 1648.0, 144.0, 923.0, 2491.0, 796.0, 911.0, 548.0, 130.0, 196.0, 1337.0, 197.0, 558.0, 2416.0, 234.0, 636.0, 298.0, 61.0, 2072.0, 673.0, 1400.0, 706.0, 1648.0, 962.0, 536.0, 1625.0, 55.0, 1223.0, 1916.0, 299.0, 1798.0, 2035.0, 2270.0, 199.0, 198.0, 592.0, 755.0, 350.0, 627.0, 306.0, 1835.0, 1515.0, 1516.0, 234.0, 120.0, 288.0, 496.0, 1456.0, 263.0, 0.0, 1183.0, 1286.0, 2318.0, 1011.0, 300.0, 661.0, 1188.0, 1434.0, 784.0, 936.0, 954.0, 1079.0, 833.0, 1612.0, 535.0, 1268.0, 379.0, 310.0, 1088.0, 1519.0, 633.0, 1665.0, 2416.0, 840.0, 661.0, 640.0, 550.0, 2686.0, 590.0, 272.0, 90.0, 60.0, 519.0, 1114.0, 391.0, 2219.0, 1533.0, 380.0, 1588.0, 541.0, 736.0, 2400.0, 146.0, 2571.0, 317.0, 783.0, 1745.0, 151.0, 41.0, 713.0, 987.0, 320.0, 391.0, 1571.0, 2845.0, 114.0, 158.0, 1216.0, 1009.0, 1773.0, 613.0, 50.0, 2733.0, 800.0, 2006.0, 491.0, 947.0, 881.0, 647.0, 134.0, 956.0, 1740.0, 1599.0, 1287.0, 1291.0, 896.0, 1437.0, 403.0, 2103.0, 196.0, 661.0, 274.0, 958.0, 400.0, 792.0, 534.0, 1395.0, 2528.0, 481.0, 923.0, 93.0, 178.0, 307.0, 857.0, 2121.0, 937.0, 619.0, 519.0, 1566.0, 424.0, 236.0, 827.0, 447.0, 1606.0, 211.0, 282.0, 453.0, 1735.0, 936.0, 306.0, 1229.0, 539.0, 109.0, 1224.0, 52.0, 2768.0, 947.0, 461.0, 936.0, 425.0, 2443.0, 2046.0, 1062.0, 968.0, 825.0, 2020.0, 362.0, 134.0, 239.0, 1609.0, 375.0, 1034.0, 168.0, 311.0, 117.0, 1436.0, 374.0, 306.0, 661.0, 781.0, 114.0, 569.0, 786.0, 277.0, 1598.0, 2062.0, 1854.0, 496.0, 3.0, 1164.0, 617.0, 2477.0, 461.0, 746.0, 2684.0, 619.0, 2514.0, 307.0, 139.0, 1.0, 560.0, 189.0, 950.0, 133.0, 640.0, 189.0, 868.0, 96.0, 215.0, 1300.0, 1724.0, 615.0, 211.0, 134.0, 1187.0, 2236.0, 1195.0, 438.0, 133.0, 435.0, 311.0, 383.0, 357.0, 478.0, 2174.0, 1995.0, 1348.0, 1189.0, 444.0, 116.0, 215.0, 1815.0, 535.0, 989.0, 885.0, 1329.0, 761.0, 1885.0, 492.0, 649.0, 948.0, 1707.0, 430.0, 1243.0, 503.0, 195.0, 296.0, 1736.0, 344.0, 196.0, 295.0, 590.0, 2007.0, 651.0, 1072.0, 866.0, 1526.0, 55.0, 135.0, 1305.0, 1230.0, 1613.0, 1067.0, 2044.0, 862.0, 199.0, 342.0, 106.0, 829.0, 456.0, 3278.0, 1970.0, 456.0, 615.0, 185.0, 171.0, 194.0, 374.0, 1523.0, 369.0, 2772.0, 882.0, 1526.0, 936.0, 267.0, 355.0, 948.0, 394.0, 1049.0, 781.0, 1592.0, 614.0, 239.0, 2035.0, 499.0, 619.0, 970.0, 729.0, 215.0, 1941.0, 267.0, 349.0, 1943.0, 110.0, 58.0, 944.0, 315.0, 53.0, 691.0, 301.0, 400.0, 1062.0, 717.0, 160.0, 192.0, 1066.0, 334.0, 162.0, 114.0, 1663.0, 456.0, 152.0, 394.0, 59.0, 2820.0, 319.0, 314.0, 353.0, 636.0, 556.0, 46.0, 1875.0, 194.0, 1228.0, 2619.0, 729.0, 352.0, 1406.0, 159.0, 318.0, 179.0, 2363.0, 253.0, 278.0, 1444.0, 781.0, 1509.0, 451.0, 125.0, 281.0, 273.0, 1640.0, 290.0, 1072.0, 306.0, 383.0, 2139.0, 114.0, 37.0, 83.0, 405.0, 1062.0, 109.0, 826.0, 2482.0, 823.0, 208.0, 160.0, 117.0, 3133.0, 661.0, 728.0, 672.0, 800.0, 2575.0, 1243.0, 784.0, 411.0, 178.0, 2244.0, 433.0, 481.0, 1311.0, 2008.0, 1269.0, 1601.0, 661.0, 649.0, 1429.0, 55.0, 263.0, 294.0, 394.0, 561.0, 1035.0, 1640.0, 49.0, 808.0, 292.0, 1348.0, 514.0, 1106.0, 1074.0, 661.0, 547.0, 67.0, 199.0, 968.0, 386.0, 1334.0, 555.0, 192.0, 204.0, 1286.0, 1577.0, 2402.0, 381.0, 1062.0, 1435.0, 148.0, 1560.0, 1678.0, 141.0, 642.0, 1057.0, 1057.0, 1260.0, 1679.0, 141.0, 632.0, 1918.0, 774.0, 400.0, 546.0, 950.0, 2463.0, 1598.0, 308.0, 3089.0, 238.0, 215.0, 2148.0, 404.0, 473.0, 399.0, 2306.0, 1429.0, 930.0, 2756.0, 328.0, 312.0, 29.0, 696.0, 185.0, 2560.0, 109.0, 1603.0, 383.0, 715.0, 1561.0, 1977.0, 1269.0, 1957.0, 1767.0, 561.0, 867.0, 730.0, 1562.0, 263.0, 3277.0, 821.0, 1816.0, 2740.0, 642.0, 1251.0, 1942.0, 1246.0, 851.0, 1596.0, 589.0, 385.0, 1184.0, 331.0, 504.0, 117.0, 706.0, 1385.0, 495.0, 726.0, 349.0, 193.0, 142.0, 1231.0, 438.0, 2015.0, 4.0, 1188.0, 590.0, 307.0, 1981.0, 894.0, 171.0, 2774.0, 615.0, 84.0, 642.0, 672.0, 714.0, 1214.0, 1174.0, 521.0, 192.0, 122.0, 403.0, 1274.0, 1877.0, 213.0, 249.0, 757.0, 1740.0, 2740.0, 1255.0, 200.0, 2119.0, 364.0, 604.0, 509.0, 258.0, 1121.0, 194.0, 370.0, 121.0, 311.0, 534.0, 1449.0, 1041.0, 1519.0, 108.0, 950.0, 106.0, 1553.0, 177.0, 400.0, 403.0, 357.0, 1693.0, 536.0, 340.0, 955.0, 1263.0, 385.0, 669.0, 519.0, 1089.0, 491.0, 1667.0, 115.0, 154.0, 207.0, 1440.0, 1226.0, 2386.0, 709.0, 79.0, 294.0, 529.0, 1105.0, 306.0, 464.0, 320.0, 661.0, 385.0, 461.0, 336.0, 331.0, 197.0, 513.0, 894.0, 1706.0, 307.0, 1596.0, 2219.0, 1966.0, 451.0, 792.0, 590.0, 711.0, 320.0, 380.0, 38.0, 3298.0, 66.0, 1010.0, 1196.0, 1820.0, 999.0, 284.0, 211.0, 1970.0, 1596.0, 319.0, 567.0, 1526.0, 55.0, 558.0, 548.0, 1148.0, 1970.0, 2464.0, 947.0, 1085.0, 400.0, 199.0, 628.0, 545.0, 1807.0, 809.0, 1105.0, 1195.0, 2199.0, 434.0, 2404.0, 311.0, 516.0, 2213.0, 814.0, 1169.0, 630.0, 171.0, 1771.0, 550.0, 2659.0, 1519.0, 16.0, 1105.0, 866.0, 113.0, 797.0, 994.0, 1612.0, 196.0, 1616.0, 1300.0, 527.0, 130.0, 1771.0, 1043.0, 3234.0, 168.0, 303.0, 1090.0, 577.0, 322.0, 502.0, 1798.0, 989.0, 121.0, 1.0, 577.0, 248.0, 2130.0, 376.0, 552.0, 110.0, 518.0, 524.0, 1613.0, 1264.0, 717.0, 2230.0, 706.0, 1814.0, 1233.0, 1640.0, 969.0, 759.0, 756.0, 153.0, 84.0, 199.0, 729.0, 1465.0, 570.0, 1006.0, 2248.0, 121.0, 2061.0, 114.0, 590.0, 306.0, 882.0, 2320.0, 313.0, 522.0, 331.0, 729.0, 2193.0, 789.0, 661.0, 1735.0, 1029.0, 1880.0, 988.0, 27.0, 3316.0, 706.0, 1062.0, 731.0, 949.0, 121.0, 1123.0, 1693.0, 204.0, 213.0, 1401.0, 357.0, 1040.0, 607.0, 1504.0, 135.0, 54.0, 1059.0, 2416.0, 2151.0, 911.0, 1829.0, 355.0, 525.0, 133.0, 1473.0, 148.0, 2007.0, 1611.0, 377.0, 683.0, 185.0, 2033.0, 348.0, 281.0, 276.0, 661.0, 2165.0, 1682.0, 2066.0, 1684.0, 189.0, 152.0, 258.0, 944.0, 1584.0, 1767.0, 653.0, 2306.0, 189.0, 2872.0, 911.0, 1106.0, 696.0, 1526.0, 1062.0, 352.0, 792.0, 303.0, 701.0, 1205.0, 1527.0, 117.0, 460.0, 1612.0, 319.0, 63.0, 1526.0, 135.0, 453.0, 696.0, 1598.0, 132.0, 334.0, 326.0, 797.0, 3029.0, 2841.0, 649.0, 2027.0, 949.0, 635.0, 51.0, 523.0, 791.0, 792.0, 617.0, 1854.0, 1081.0, 2952.0, 1084.0, 399.0, 562.0, 1693.0, 1221.0, 106.0, 2.0, 1023.0, 968.0, 1591.0, 1269.0, 238.0, 471.0, 432.0, 1377.0, 777.0, 1266.0, 1062.0, 385.0, 954.0, 670.0, 1201.0, 1701.0, 966.0, 1995.0, 1648.0, 61.0, 391.0, 247.0, 869.0, 1017.0, 523.0, 197.0, 306.0, 1952.0, 1825.0, 155.0, 644.0, 955.0, 1016.0, 449.0, 190.0, 609.0, 570.0, 2019.0, 240.0, 194.0, 850.0, 2247.0, 790.0, 785.0, 605.0, 239.0, 645.0, 1091.0, 171.0, 423.0, 937.0, 3133.0, 492.0, 1981.0, 391.0, 380.0, 314.0, 211.0, 1872.0, 366.0, 642.0, 2242.0, 1465.0, 613.0, 81.0, 949.0, 132.0, 130.0, 484.0, 614.0, 146.0, 153.0, 151.0, 2248.0, 940.0, 2355.0, 263.0, 974.0, 407.0, 1888.0, 936.0, 52.0, 162.0, 911.0, 197.0, 792.0, 296.0, 108.0, 532.0, 2.0, 777.0, 130.0, 2531.0, 2150.0, 2361.0, 522.0, 435.0, 2253.0, 320.0, 860.0, 782.0, 402.0, 400.0, 2573.0, 1735.0, 2142.0, 970.0, 1112.0, 1212.0, 1188.0, 92.0, 221.0, 1740.0, 1066.0, 1997.0, 193.0, 1005.0, 2599.0, 731.0, 1942.0, 211.0, 761.0, 946.0, 259.0, 710.0, 568.0, 449.0, 1406.0, 711.0, 274.0, 2398.0, 2555.0, 2253.0, 451.0, 398.0, 697.0, 1009.0, 820.0, 724.0, 2184.0, 1849.0, 2420.0, 525.0, 759.0, 113.0, 183.0, 1004.0, 1241.0, 378.0, 110.0, 267.0, 492.0, 1348.0, 906.0, 577.0, 286.0, 419.0, 135.0, 106.0, 130.0, 1930.0, 34.0, 124.0, 90.0, 322.0, 1393.0, 278.0, 420.0, 1498.0, 214.0, 1644.0, 29.0, 300.0, 144.0, 624.0, 377.0, 126.0, 180.0, 67.0, 633.0, 50.0, 783.0, 298.0, 430.0, 39.0, 1009.0, 121.0, 936.0, 189.0, 153.0, 3.0, 294.0, 87.0, 682.0, 291.0, 1250.0, 2686.0, 349.0, 106.0, 424.0, 1916.0, 428.0, 362.0, 1025.0, 2050.0, 788.0, 31.0, 661.0, 2671.0, 941.0, 1987.0, 491.0, 823.0, 311.0, 112.0, 293.0, 1583.0, 190.0, 86.0, 968.0, 1450.0, 1616.0, 2242.0, 259.0, 189.0, 2400.0, 869.0, 386.0, 382.0, 660.0, 2962.0, 40.0, 373.0, 1947.0, 1286.0, 1087.0, 507.0, 937.0, 755.0, 554.0, 36.0, 1899.0, 352.0, 1735.0, 1981.0, 212.0, 1064.0, 134.0, 195.0, 1219.0, 135.0, 723.0, 663.0, 244.0, 243.0, 195.0, 1468.0, 1735.0, 1952.0, 180.0, 3165.0, 478.0, 1065.0, 457.0, 636.0, 527.0, 149.0, 844.0, 841.0, 753.0, 189.0, 1334.0, 728.0, 59.0, 1117.0, 1745.0, 404.0, 1187.0, 68.0, 106.0, 624.0, 151.0, 1723.0, 1510.0, 121.0, 999.0, 989.0, 1941.0, 115.0, 190.0, 1321.0, 783.0, 731.0, 590.0, 496.0, 344.0, 111.0, 637.0, 2934.0, 661.0, 947.0, 461.0, 2055.0, 2127.0, 575.0, 400.0, 819.0, 761.0, 244.0, 1220.0, 199.0, 280.0, 295.0, 432.0, 2848.0, 292.0, 665.0, 948.0, 161.0, 335.0, 783.0, 1771.0, 2383.0, 68.0, 1348.0, 1087.0, 1036.0, 966.0, 150.0, 386.0, 564.0, 1918.0, 335.0, 2179.0, 596.0, 2648.0, 1618.0, 29.0, 425.0, 52.0, 285.0, 1268.0, 259.0, 2526.0, 479.0, 138.0, 1614.0, 376.0, 832.0, 1195.0, 24.0, 1626.0, 577.0, 424.0, 1815.0, 999.0, 148.0, 121.0, 193.0, 629.0, 589.0, 969.0, 936.0, 954.0, 357.0, 2157.0, 1292.0, 1298.0, 317.0, 2181.0, 287.0, 357.0, 1928.0, 400.0, 117.0, 212.0, 1699.0, 112.0, 643.0, 51.0, 784.0, 144.0, 34.0, 346.0, 186.0, 706.0, 478.0, 2484.0, 234.0, 946.0, 2065.0, 1065.0, 1242.0, 2235.0, 212.0, 1735.0, 121.0, 190.0, 196.0, 2127.0, 1066.0, 504.0, 308.0, 2496.0, 3413.0, 222.0, 664.0, 1699.0, 1741.0, 923.0, 2590.0, 349.0, 130.0, 215.0, 267.0, 940.0, 1231.0, 323.0, 274.0, 2380.0, 381.0, 610.0, 239.0, 3307.0, 125.0, 721.0, 960.0, 110.0, 2355.0, 828.0, 1724.0, 1.0, 314.0, 619.0, 402.0, 1495.0, 1768.0, 503.0, 204.0, 1250.0, 2229.0, 288.0, 1026.0, 663.0, 147.0, 50.0, 1587.0, 195.0, 207.0, 352.0, 31.0, 1295.0, 89.0, 211.0, 1739.0, 1217.0, 958.0, 106.0, 214.0, 966.0, 64.0, 2349.0, 1002.0, 2353.0, 305.0, 309.0, 1606.0, 2669.0, 504.0, 200.0, 190.0, 619.0, 1708.0, 656.0, 309.0, 255.0, 787.0, 2344.0, 285.0, 1835.0, 655.0, 375.0, 1332.0, 620.0, 1599.0, 1004.0, 67.0, 2.0, 673.0, 890.0, 1176.0, 1195.0, 997.0, 49.0, 302.0, 1087.0, 2032.0, 1032.0, 2470.0, 236.0, 991.0, 307.0, 305.0, 562.0, 1328.0, 532.0, 936.0, 1500.0, 189.0, 640.0, 935.0, 47.0, 268.0, 532.0, 969.0, 3187.0, 453.0, 1764.0, 310.0, 399.0, 196.0, 1062.0, 207.0, 558.0, 148.0, 190.0, 199.0, 150.0, 121.0, 400.0, 483.0, 2392.0, 432.0, 1889.0, 2159.0, 1519.0, 50.0, 1618.0, 213.0, 403.0, 415.0, 1643.0, 400.0, 784.0, 297.0, 185.0, 2686.0, 139.0, 1599.0, 942.0, 34.0, 1971.0, 90.0, 1314.0, 1849.0, 2008.0, 1133.0, 1526.0, 1604.0, 361.0, 684.0, 726.0, 195.0, 857.0, 800.0, 570.0, 322.0, 1068.0, 522.0, 438.0, 1088.0, 2165.0, 243.0, 137.0, 781.0, 831.0, 114.0, 207.0, 510.0, 2265.0, 208.0, 158.0, 950.0, 723.0, 217.0, 439.0, 352.0, 2363.0, 1301.0, 1723.0, 1040.0, 121.0, 2477.0, 590.0, 32.0, 511.0, 453.0, 1440.0, 473.0, 749.0, 869.0, 500.0, 204.0, 138.0, 134.0, 1440.0, 52.0, 561.0, 192.0, 435.0, 2300.0, 1601.0, 418.0, 293.0, 194.0, 60.0, 1767.0, 661.0, 617.0, 0.0, 1735.0, 1112.0, 1560.0, 64.0, 588.0, 1564.0, 1039.0, 1246.0, 116.0, 1026.0, 1612.0, 40.0, 1885.0, 50.0, 318.0, 212.0, 720.0, 300.0, 499.0, 355.0, 1152.0, 954.0, 277.0, 405.0, 857.0, 936.0, 587.0, 1599.0, 434.0, 1072.0, 1616.0, 173.0, 636.0, 196.0, 2151.0, 1230.0, 2015.0, 451.0, 17.0, 287.0, 267.0, 387.0, 1321.0, 999.0, 1940.0, 23.0, 231.0, 200.0, 862.0, 183.0, 151.0, 180.0, 800.0, 784.0, 1119.0, 224.0, 46.0, 212.0, 453.0, 1599.0, 311.0, 936.0, 772.0, 447.0, 180.0, 373.0, 320.0, 1735.0, 1909.0, 832.0, 30.0, 2308.0, 1229.0, 193.0, 185.0, 729.0, 1152.0, 661.0, 1972.0, 177.0, 312.0, 624.0, 111.0, 989.0, 120.0, 1004.0, 1007.0, 1599.0, 38.0, 539.0, 2601.0, 133.0, 177.0, 2593.0, 195.0, 1178.0, 50.0, 768.0, 162.0, 828.0, 2373.0, 192.0, 1716.0, 130.0, 943.0, 156.0, 1949.0, 45.0, 946.0, 438.0, 258.0, 322.0, 1002.0, 1435.0, 248.0, 276.0, 1751.0, 1015.0, 784.0, 1709.0, 1274.0, 320.0, 189.0, 2525.0, 1999.0, 989.0, 1141.0, 2538.0, 1041.0, 1269.0, 349.0, 452.0, 866.0, 681.0, 340.0, 711.0, 181.0, 661.0, 1571.0, 306.0, 653.0, 902.0, 2698.0, 158.0, 736.0, 1224.0, 610.0, 2754.0, 1561.0, 49.0, 2012.0, 2131.0, 970.0, 149.0, 989.0, 513.0, 1442.0, 1442.0, 570.0, 761.0, 1660.0, 106.0, 561.0, 652.0, 181.0, 1532.0, 300.0, 438.0, 784.0, 2305.0, 577.0, 342.0, 303.0, 199.0, 499.0, 612.0, 1555.0, 997.0, 1747.0, 1231.0, 213.0, 1357.0, 507.0, 1000.0, 2038.0, 391.0, 277.0, 1950.0, 531.0, 1231.0, 1495.0, 142.0, 1767.0, 946.0, 727.0, 629.0, 1434.0, 749.0, 936.0, 2859.0, 274.0, 829.0, 545.0, 941.0, 1362.0, 2002.0, 259.0, 827.0, 277.0, 113.0, 661.0, 1306.0, 385.0, 153.0, 609.0, 538.0, 550.0, 258.0, 1070.0, 1442.0, 310.0, 178.0, 940.0, 432.0, 332.0, 3453.0, 1143.0, 613.0, 1081.0, 1332.0, 1056.0, 1221.0, 111.0, 34.0, 491.0, 400.0, 1228.0, 65.0, 1610.0, 729.0, 282.0, 2413.0, 2353.0, 454.0, 569.0, 789.0, 400.0, 158.0, 1519.0, 2985.0, 653.0, 1815.0, 447.0, 152.0, 2974.0, 989.0, 748.0, 46.0, 2253.0, 1489.0, 619.0, 783.0, 400.0, 1376.0, 295.0, 288.0, 310.0, 2689.0, 1493.0, 2373.0, 552.0, 207.0, 855.0, 1626.0, 1034.0, 1979.0, 1533.0, 989.0, 638.0, 465.0, 785.0, 1440.0, 2929.0, 1344.0, 1725.0, 244.0, 375.0, 139.0, 2099.0, 134.0, 586.0, 840.0, 1965.0, 717.0, 386.0, 190.0, 1090.0, 882.0, 451.0, 478.0, 1590.0, 507.0, 1063.0, 178.0, 1735.0, 1123.0, 1105.0, 320.0, 1230.0, 2014.0, 957.0, 1814.0, 348.0, 147.0, 1966.0, 786.0, 936.0, 425.0, 2363.0, 380.0, 619.0, 661.0, 661.0, 438.0, 273.0, 743.0, 1740.0, 1246.0, 954.0, 1598.0, 49.0, 1619.0, 947.0, 568.0, 2017.0, 68.0, 2418.0, 1772.0, 288.0, 310.0, 1942.0, 840.0, 788.0, 1249.0, 130.0, 649.0, 1970.0, 332.0, 480.0, 433.0, 3298.0, 134.0, 1981.0, 950.0, 516.0, 910.0, 69.0, 1849.0, 1617.0, 571.0, 1215.0, 256.0, 197.0, 538.0, 931.0, 3033.0, 306.0, 517.0, 614.0, 400.0, 375.0, 58.0, 492.0, 1822.0, 1696.0, 800.0, 267.0, 139.0, 1269.0, 428.0, 653.0, 1978.0, 588.0, 312.0, 949.0, 768.0, 1693.0, 1693.0, 1640.0, 241.0, 295.0, 617.0, 523.0, 3.0, 1625.0, 1.0, 180.0, 947.0, 85.0, 3075.0, 2270.0, 1957.0, 1003.0, 48.0, 1299.0, 1612.0, 962.0, 215.0, 722.0, 1267.0, 932.0, 1183.0, 651.0, 1435.0, 212.0, 914.0, 450.0, 109.0, 274.0, 941.0, 1221.0, 171.0, 152.0, 592.0, 357.0, 743.0, 215.0, 968.0, 303.0, 1758.0, 647.0, 451.0, 911.0, 41.0, 947.0, 947.0, 641.0, 1195.0, 958.0, 1875.0, 341.0, 957.0, 273.0, 402.0, 2063.0, 559.0, 175.0, 471.0, 2689.0, 54.0, 576.0, 36.0, 902.0, 1241.0, 275.0, 377.0, 431.0, 959.0, 2972.0, 800.0, 1305.0, 321.0, 345.0, 559.0, 256.0, 1444.0, 1881.0, 65.0, 547.0, 558.0, 751.0, 478.0, 569.0, 1519.0, 355.0, 1571.0, 256.0, 1937.0, 1195.0, 1964.0, 947.0, 906.0, 792.0, 332.0, 111.0, 1789.0, 831.0, 152.0, 888.0, 2240.0, 133.0, 483.0, 1144.0, 1063.0, 2177.0, 400.0, 73.0, 444.0, 23.0, 93.0, 1456.0, 1004.0, 1153.0, 1164.0, 348.0, 404.0, 158.0, 424.0, 2168.0, 370.0, 1392.0, 293.0, 2365.0, 1436.0, 199.0, 759.0, 614.0, 3.0, 1885.0, 1286.0, 144.0, 1059.0, 568.0, 194.0, 343.0, 1144.0, 1571.0, 649.0, 276.0, 1304.0, 243.0, 126.0, 2022.0, 2012.0, 645.0, 554.0, 1028.0, 1266.0, 1526.0, 483.0, 638.0, 1495.0, 946.0, 372.0, 214.0, 358.0, 122.0, 477.0, 831.0, 1017.0, 1085.0, 1318.0, 2039.0, 906.0, 610.0, 483.0, 142.0, 829.0, 2225.0, 661.0, 949.0, 1342.0, 781.0, 230.0, 4.0, 4.0, 207.0, 1112.0, 256.0, 918.0, 447.0, 391.0, 385.0, 654.0, 896.0, 783.0, 1143.0, 343.0, 780.0, 343.0, 328.0, 139.0, 59.0, 1560.0, 399.0, 389.0, 1435.0, 396.0, 1815.0, 1371.0, 2848.0, 661.0, 1187.0, 1736.0, 1300.0, 576.0, 1332.0, 459.0, 300.0, 53.0, 777.0, 1614.0, 1018.0, 1682.0, 399.0, 661.0, 367.0, 296.0, 300.0, 1174.0, 203.0, 846.0, 1229.0, 443.0, 1686.0, 1302.0, 405.0, 724.0, 890.0, 1035.0, 319.0, 452.0, 2139.0, 425.0, 1376.0, 633.0, 969.0, 197.0, 317.0, 792.0, 483.0, 349.0, 2056.0, 1442.0, 1026.0, 1923.0, 1163.0, 867.0, 2008.0, 1435.0, 69.0, 1066.0, 937.0, 1614.0, 310.0, 208.0, 117.0, 195.0, 1174.0, 2551.0, 400.0, 570.0, 471.0, 107.0, 937.0, 1092.0, 620.0, 1950.0, 947.0, 495.0, 484.0, 792.0, 949.0, 1231.0, 407.0, 831.0, 198.0, 617.0, 2600.0, 1700.0, 781.0, 149.0, 306.0, 180.0, 1957.0, 999.0, 1618.0, 667.0, 723.0, 90.0, 34.0, 410.0, 471.0, 2630.0, 390.0, 1217.0, 403.0, 376.0, 122.0, 1087.0, 1713.0, 828.0, 1735.0, 534.0, 1803.0, 787.0, 749.0, 773.0, 1105.0, 1286.0, 288.0, 1059.0, 1571.0, 576.0, 498.0, 2163.0, 684.0, 728.0, 963.0, 331.0, 31.0, 434.0, 575.0, 1105.0, 87.0, 341.0, 2491.0, 1678.0, 499.0, 1241.0, 1027.0, 299.0, 987.0, 1473.0, 445.0, 400.0, 256.0, 551.0, 1457.0, 1026.0, 799.0, 610.0, 431.0, 1500.0, 397.0, 883.0, 2085.0, 277.0, 41.0, 189.0, 2514.0, 3089.0, 1584.0, 240.0, 2017.0, 617.0, 661.0, 1449.0, 141.0, 630.0, 1323.0, 277.0, 263.0, 966.0, 29.0, 538.0, 299.0, 279.0, 480.0, 777.0, 1526.0, 55.0, 196.0, 267.0, 592.0, 186.0, 121.0, 600.0, 1361.0, 130.0, 1814.0, 441.0, 1258.0, 1227.0, 279.0, 1008.0, 1305.0, 1.0, 1625.0, 589.0, 1293.0, 1699.0, 1141.0, 993.0, 3.0, 792.0, 792.0, 778.0, 256.0, 438.0, 132.0, 215.0, 2248.0, 825.0, 491.0, 1068.0, 118.0, 964.0, 1558.0, 298.0, 1286.0, 29.0, 121.0, 1349.0, 1026.0, 965.0, 937.0, 1034.0, 256.0, 1396.0, 198.0, 590.0, 1393.0, 344.0, 468.0, 869.0, 831.0, 661.0, 393.0, 764.0, 1016.0, 629.0, 759.0, 190.0, 1153.0, 256.0, 203.0, 787.0, 465.0, 391.0, 800.0, 829.0, 2233.0, 46.0, 1606.0, 725.0, 1950.0, 253.0, 1052.0, 970.0, 155.0, 1877.0, 159.0, 207.0, 244.0, 1526.0, 2403.0, 297.0, 461.0, 1022.0, 2321.0, 829.0, 1428.0, 171.0, 532.0, 32.0, 112.0, 525.0, 1528.0, 376.0, 236.0, 814.0, 636.0, 834.0, 612.0, 438.0, 250.0, 1805.0, 1735.0, 577.0, 380.0, 189.0, 194.0, 55.0, 663.0, 661.0, 946.0, 1093.0, 65.0, 211.0, 1994.0, 558.0, 38.0, 438.0, 312.0, 1243.0, 296.0, 1777.0, 2233.0, 130.0, 412.0, 1814.0, 1686.0, 1040.0, 2151.0, 28.0, 361.0, 2082.0, 1224.0, 1015.0, 2065.0, 2373.0, 2225.0, 2.0, 649.0, 130.0, 247.0, 761.0, 1508.0, 1905.0, 505.0, 1395.0, 2129.0, 295.0, 1323.0, 1519.0, 2724.0, 499.0, 1932.0, 197.0, 709.0, 613.0, 1029.0, 59.0, 55.0, 673.0, 477.0, 527.0, 1043.0, 519.0, 2175.0, 1112.0, 1347.0, 575.0, 589.0, 616.0, 1011.0, 278.0, 1942.0, 1446.0, 624.0, 517.0, 888.0, 248.0, 273.0, 661.0, 2329.0, 420.0, 2109.0, 950.0, 1592.0, 320.0, 989.0, 1825.0, 1825.0, 48.0, 1231.0, 615.0, 661.0, 902.0, 661.0, 2887.0, 48.0, 973.0, 114.0, 2444.0, 204.0, 1062.0, 649.0, 870.0, 1860.0, 1404.0, 307.0, 2132.0, 1243.0, 610.0, 453.0, 106.0, 936.0, 420.0, 696.0, 453.0, 278.0, 593.0, 619.0, 1515.0, 558.0, 1451.0, 480.0, 168.0, 810.0, 300.0, 317.0, 1892.0, 1026.0, 617.0, 661.0, 959.0, 2361.0, 1941.0, 346.0, 958.0, 944.0, 1740.0, 385.0, 811.0, 3002.0, 1904.0, 211.0, 831.0, 50.0, 516.0, 126.0, 729.0, 147.0, 731.0, 1284.0, 540.0, 2174.0, 1128.0, 800.0, 334.0, 1878.0, 2056.0, 79.0, 349.0, 380.0, 158.0, 262.0, 1557.0, 201.0, 304.0, 4.0, 430.0, 1761.0, 962.0, 1693.0, 2271.0, 424.0, 215.0, 1416.0, 1246.0, 827.0, 32.0, 181.0, 2392.0, 577.0, 111.0, 2015.0, 731.0, 247.0, 2625.0, 562.0, 519.0, 357.0, 1524.0, 1618.0, 128.0, 450.0, 829.0, 620.0, 2571.0, 1256.0, 948.0, 180.0, 502.0, 553.0, 211.0, 1061.0, 47.0, 190.0, 1032.0, 615.0, 1435.0, 2609.0, 1085.0, 153.0, 1872.0, 480.0, 946.0, 229.0, 369.0, 1231.0, 2044.0, 728.0, 2293.0, 88.0, 1735.0, 2139.0, 1814.0, 1112.0, 1592.0, 1081.0, 512.0, 3022.0, 1062.0, 1905.0, 211.0, 432.0, 800.0, 109.0, 784.0, 46.0, 452.0, 966.0, 64.0, 404.0, 477.0, 1486.0, 180.0, 1519.0, 2507.0, 2156.0, 524.0, 955.0, 215.0, 1265.0, 30.0, 834.0, 307.0, 2628.0, 69.0, 1459.0, 1369.0, 577.0, 1026.0, 224.0, 950.0, 450.0, 355.0, 1815.0, 311.0, 840.0, 727.0, 2144.0, 370.0, 504.0, 349.0, 1123.0, 213.0, 149.0, 447.0, 1997.0, 534.0, 936.0, 258.0, 911.0, 243.0, 2165.0, 394.0, 59.0, 1985.0, 195.0, 654.0, 576.0, 456.0, 1945.0, 2164.0, 348.0, 789.0, 121.0, 1623.0, 121.0, 914.0, 1243.0, 793.0, 1437.0, 394.0, 999.0, 935.0, 88.0, 1370.0, 1526.0, 781.0, 1710.0, 204.0, 139.0, 1922.0, 1451.0, 1311.0, 3075.0, 407.0, 277.0, 1885.0, 1195.0, 827.0, 866.0, 27.0, 1011.0, 343.0, 1814.0, 438.0, 579.0, 1263.0, 2787.0, 180.0, 34.0, 1396.0, 3.0, 1027.0, 1816.0, 50.0, 412.0, 1731.0, 870.0, 1292.0, 552.0, 561.0, 64.0, 796.0, 1306.0, 999.0, 370.0, 1970.0, 380.0, 947.0, 293.0, 186.0, 566.0, 1836.0, 4.0, 972.0, 949.0, 151.0, 1257.0, 3.0, 806.0, 29.0, 281.0, 610.0, 83.0, 222.0, 1260.0, 373.0, 198.0, 66.0, 1573.0, 201.0, 189.0, 1020.0, 275.0, 1431.0, 1087.0, 54.0, 661.0, 296.0, 77.0, 823.0, 1229.0, 971.0, 667.0, 1591.0, 614.0, 1849.0, 1815.0, 1099.0, 729.0, 207.0, 784.0, 195.0, 1404.0, 1442.0, 166.0, 311.0, 62.0, 3285.0, 832.0, 128.0, 859.0, 87.0, 342.0, 619.0, 1161.0, 1076.0, 377.0, 113.0, 617.0, 288.0, 125.0, 42.0, 294.0, 1635.0, 215.0, 431.0, 1707.0, 888.0, 1616.0, 1195.0, 821.0, 348.0, 268.0, 970.0, 180.0, 1940.0, 1809.0, 1519.0, 2041.0, 491.0, 517.0, 476.0, 1081.0, 2705.0, 65.0, 1086.0, 491.0, 259.0, 1740.0, 656.0, 1457.0, 1461.0, 1435.0, 950.0, 1671.0, 3194.0, 68.0, 2844.0, 268.0, 794.0, 29.0, 243.0, 706.0, 223.0, 256.0, 129.0, 332.0, 949.0, 381.0, 106.0, 955.0, 2065.0, 343.0, 729.0, 517.0, 711.0, 1618.0, 1066.0, 3271.0, 495.0, 55.0, 2242.0, 1693.0, 1816.0, 1137.0, 272.0, 209.0, 496.0, 3268.0, 481.0, 212.0, 117.0, 947.0, 1892.0, 110.0, 457.0, 1028.0, 2141.0, 1224.0, 514.0, 941.0, 585.0, 1889.0, 357.0, 499.0, 735.0, 147.0, 1386.0, 122.0, 604.0, 152.0, 524.0, 430.0, 457.0, 500.0, 146.0, 323.0, 1304.0, 431.0, 827.0, 459.0, 3006.0, 425.0, 130.0, 1393.0, 1167.0, 459.0, 178.0, 306.0, 403.0, 797.0, 403.0, 402.0, 2759.0, 2143.0, 2291.0, 319.0, 764.0, 494.0, 2366.0, 149.0, 435.0, 409.0, 649.0, 77.0, 402.0, 1176.0, 940.0, 939.0, 729.0, 1609.0, 272.0, 376.0, 1444.0, 562.0, 1431.0, 211.0, 201.0, 1143.0, 317.0, 111.0, 148.0, 549.0, 190.0, 1368.0, 887.0, 870.0, 468.0, 370.0, 198.0, 848.0, 294.0, 348.0, 661.0, 483.0, 321.0, 263.0, 1002.0, 577.0, 586.0, 424.0, 2462.0, 2754.0, 2255.0, 1002.0, 272.0, 215.0, 499.0, 774.0, 1348.0, 402.0, 215.0, 784.0, 1299.0, 435.0, 204.0, 649.0, 419.0, 90.0, 2128.0, 1123.0, 1449.0, 183.0, 52.0, 42.0, 1442.0, 2165.0, 2012.0, 443.0, 331.0, 1558.0, 494.0, 38.0, 1852.0, 306.0, 361.0, 121.0, 443.0, 1500.0, 2321.0, 1224.0, 2244.0, 1286.0, 2174.0, 142.0, 615.0, 90.0, 2226.0, 577.0, 2159.0, 1979.0, 79.0, 2811.0, 684.0, 791.0, 364.0, 346.0, 376.0, 331.0, 519.0, 999.0, 40.0, 776.0, 933.0, 1444.0, 189.0, 491.0, 949.0, 453.0, 800.0, 1526.0, 2014.0, 54.0, 111.0, 828.0, 415.0, 1942.0, 1292.0, 306.0, 562.0, 882.0, 1244.0, 1728.0, 2136.0, 1494.0, 2318.0, 866.0, 199.0, 110.0, 1571.0, 610.0, 662.0, 1908.0, 632.0, 1004.0, 1554.0, 28.0, 1228.0, 1751.0, 1516.0, 121.0, 379.0, 632.0, 761.0, 745.0, 1981.0, 531.0, 444.0, 320.0, 782.0, 68.0, 1743.0, 937.0, 207.0, 3006.0, 439.0, 562.0, 613.0, 1892.0, 1990.0, 1007.0, 1484.0, 940.0, 1905.0, 226.0, 570.0, 286.0, 882.0, 297.0, 523.0, 661.0, 661.0, 29.0, 661.0, 195.0, 1088.0, 1950.0, 184.0, 1695.0, 661.0, 2952.0, 946.0, 130.0, 610.0, 296.0, 808.0, 199.0, 272.0, 149.0, 776.0, 567.0, 1081.0, 2575.0, 1020.0, 30.0, 643.0, 1820.0, 138.0, 562.0, 1606.0, 840.0, 152.0, 950.0, 1087.0, 219.0, 1220.0, 59.0, 825.0, 842.0, 486.0, 590.0, 255.0, 133.0, 177.0, 256.0, 1044.0, 56.0, 256.0, 405.0, 2324.0, 1062.0, 31.0, 3236.0, 711.0, 432.0, 306.0, 3151.0, 908.0, 949.0, 411.0, 135.0, 1233.0, 985.0, 435.0, 195.0, 613.0, 403.0, 1049.0, 847.0, 2166.0, 342.0, 1693.0, 1699.0, 797.0, 1188.0, 1878.0, 614.0, 372.0, 2835.0, 348.0, 452.0, 234.0, 59.0, 323.0, 711.0, 1495.0, 238.0, 1264.0, 749.0, 277.0, 61.0, 300.0, 505.0, 1560.0, 649.0, 1525.0, 189.0, 914.0, 468.0, 1404.0, 525.0, 77.0, 298.0, 376.0, 473.0, 1437.0, 310.0, 661.0, 189.0, 2167.0, 2196.0, 370.0, 1767.0, 3190.0, 484.0, 369.0, 1000.0, 1348.0, 3094.0, 881.0, 300.0, 1393.0, 147.0, 1314.0, 54.0, 1513.0, 213.0, 790.0, 1033.0, 871.0, 539.0, 1361.0, 1269.0, 988.0, 1255.0, 1815.0, 2099.0, 132.0, 1596.0, 1249.0, 2267.0, 665.0, 989.0, 1187.0, 334.0, 2229.0, 377.0, 1820.0, 1693.0, 661.0, 1269.0, 1143.0, 1740.0, 208.0, 2493.0, 609.0, 932.0, 310.0, 418.0, 183.0, 641.0, 1587.0, 1434.0, 29.0, 1571.0, 1635.0, 653.0, 515.0, 244.0, 142.0, 303.0, 231.0, 848.0, 450.0, 1300.0, 661.0, 2131.0, 1100.0, 1228.0, 906.0, 1623.0, 342.0, 1533.0, 349.0, 378.0, 130.0, 1612.0, 710.0, 1115.0, 1240.0, 361.0, 263.0, 39.0, 214.0, 947.0, 357.0, 782.0, 215.0, 194.0, 2072.0, 321.0, 2344.0, 1614.0, 256.0, 532.0, 154.0, 698.0, 317.0, 2054.0, 130.0, 212.0, 2151.0, 1609.0, 88.0, 911.0, 300.0, 471.0, 265.0, 499.0, 146.0, 1404.0, 1765.0, 661.0, 1002.0, 586.0, 2211.0, 2127.0, 3306.0, 1745.0, 1077.0, 956.0, 259.0, 400.0, 510.0, 792.0, 1833.0, 1495.0, 521.0, 275.0, 1003.0, 547.0, 2042.0, 313.0, 1695.0, 661.0, 354.0, 332.0, 361.0, 274.0, 1583.0, 710.0, 402.0, 959.0, 2357.0, 971.0, 2035.0, 661.0, 109.0, 31.0, 1178.0, 125.0, 41.0, 274.0, 53.0, 32.0, 256.0, 1970.0, 519.0, 2497.0, 212.0, 3346.0, 949.0, 1368.0, 1678.0, 166.0, 590.0, 2416.0, 1057.0, 275.0, 947.0, 272.0, 729.0, 553.0, 1066.0, 223.0, 661.0, 66.0, 309.0, 1880.0, 88.0, 1287.0, 296.0, 1213.0, 449.0, 665.0, 431.0, 2403.0, 24.0, 2909.0, 721.0, 499.0, 2008.0, 213.0, 1822.0, 2077.0, 503.0, 1066.0, 366.0, 1592.0, 2078.0, 495.0, 1952.0, 1740.0, 1620.0, 67.0, 459.0, 896.0, 706.0, 661.0, 256.0, 1004.0, 2050.0, 1435.0, 1215.0, 170.0, 124.0, 342.0, 1268.0, 322.0, 2196.0, 483.0, 457.0, 256.0, 649.0, 211.0, 1609.0, 400.0, 106.0, 456.0, 382.0, 882.0, 34.0, 698.0, 2329.0, 192.0, 731.0, 563.0, 559.0, 347.0, 1357.0, 321.0, 1290.0, 2290.0, 805.0, 402.0, 472.0, 1040.0, 139.0, 1614.0, 2825.0, 1611.0, 1878.0, 307.0, 1081.0, 169.0, 1195.0, 294.0, 238.0, 942.0, 1617.0, 661.0, 207.0, 1268.0, 2985.0, 400.0, 721.0, 539.0, 615.0, 92.0, 136.0, 604.0, 2053.0, 343.0, 955.0, 1090.0, 198.0, 171.0, 947.0, 1224.0, 1465.0, 2249.0, 1268.0, 321.0, 866.0, 1332.0, 1767.0, 576.0, 55.0, 3343.0, 1241.0, 180.0, 559.0, 2462.0, 1484.0, 429.0, 623.0, 384.0, 212.0, 499.0, 1246.0, 576.0, 1526.0, 151.0, 240.0, 1839.0, 914.0, 2629.0, 985.0, 2.0, 1392.0, 401.0, 284.0, 198.0, 29.0, 50.0, 400.0, 438.0, 411.0, 29.0, 1610.0, 1907.0, 1519.0, 1508.0, 1348.0, 106.0, 729.0, 682.0, 2165.0, 211.0, 1981.0, 1449.0, 557.0, 661.0, 59.0, 385.0, 2402.0, 208.0, 1442.0, 781.0, 253.0, 620.0, 358.0, 504.0, 30.0, 1301.0, 2553.0, 838.0, 287.0, 121.0, 419.0, 787.0, 667.0, 1121.0, 130.0, 400.0, 2698.0, 2987.0, 820.0, 612.0, 401.0, 281.0, 791.0, 1287.0, 2403.0, 439.0, 1526.0, 759.0, 617.0, 1981.0, 1025.0, 653.0, 1026.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzHtTqFhwMGE",
        "outputId": "f8ff3fa9-837e-4e01-9ad6-1139bab8c120"
      },
      "source": [
        "len(data1['label'].unique())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnucJpa24-7R"
      },
      "source": [
        "# import tensorflow_hub as hub\n",
        "# FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "# bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "vcUsgQOnD3za",
        "outputId": "793083d1-1c56-4cdc-fb55-1b3783ab75ca"
      },
      "source": [
        "train['Detail'] = train['Detail'].str.replace('&&', ' [SEP] ')\n",
        "sentences = train['Detail']\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "sentences[100]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] VICTORIAN GLASS HANGING T-LIGHT [SEP] HANGING HEART JAR T-LIGHT HOLDER [SEP] CREAM HEART CARD HOLDER [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "6e5de485328c45998a366027a9c6f8f2",
            "fec5035d2f4d4e52b8f42f8a3f3e1857",
            "469c01fc6b7248e3a9427e74f0a06e1e",
            "0cdf33a5177f4d9c8af01491a4e698b5",
            "93aecd983bba4f6b9bf8a0f777576252",
            "7d3f23625fed4ab6b625b8e59976a986",
            "12d9c2e56c934dedb5cc664ff4040ae0",
            "d471ead201304ce3ae54c61ea2ac2882",
            "86b62b3f7d32413b918d8510c0f89a0d",
            "17b121dec6a84635bbdaac5d624c03df",
            "99e300fbf6f54155a79d361cca17472f",
            "afe3b311f37542aa96e1d58351f2098f",
            "d3c03f115cc84c2cb27a15d3c12b46b0",
            "9e9bb6cb5fe04b87a7aa0bf7368d86a3",
            "acb54a964d504380aab3404d370325b3",
            "26d79e335b0d4577bb33c7140d68543f",
            "b817fece6cc347b892f4ed1e0df15100",
            "e964aa4d28034c18a734a9a5e6c8ffd2",
            "78ecac05075b44edae010b455d71c5ba",
            "76463f6e942041e5b582713a1a4da226",
            "45672213101c46a38d38d79b58459fa9",
            "4f99ae78fd32430d83f59e08fd01bdac",
            "c97c4e9781ba49c7b36e5695615a2040",
            "f710cac951e34f0e89b64fbe8e008051"
          ]
        },
        "id": "abSsPUxmrlKN",
        "outputId": "a9708526-531f-4644-fc67-52170dc32b19"
      },
      "source": [
        "# # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "# tokenized_texts_list = train['Detail'].to_list()\n",
        "# tokenized_texts = []\n",
        "# for v in tokenized_texts_list:\n",
        "    # tokenized_texts.append([v])\n",
        "print (sentences[0])\n",
        "print (tokenized_texts[0])\n",
        "len(tokenized_texts)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e5de485328c45998a366027a9c6f8f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86b62b3f7d32413b918d8510c0f89a0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b817fece6cc347b892f4ed1e0df15100",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[CLS] DAISY FOLKART HEART DECORATION [SEP] ROSE FOLKART HEART DECORATIONS [SEP] JUNGLE POPSICLES ICE LOLLY HOLDERS [SEP] TOP SECRET PEN SET [SEP] STRAWBERRY CERAMIC TRINKET BOX [SEP] RECYCLING BAG RETROSPOT [SEP] BUTTERFLIES  HONEYCOMB GARLAND [SEP] YELLOW EASTER EGG HUNT START POST [SEP] BINGO SET [SEP] SILVER SKULL HOT WATER BOTTLE [SEP] VINTAGE SNAP CARDS [SEP] SET 12 RETRO WHITE CHALK STICKS [SEP] SWEETHEART CERAMIC TRINKET BOX [SEP] BIRD DECORATION GREEN  SPOT [SEP] HEART DECORATION WITH PEARLS [SEP] SKULLS DESIGN FLANNEL [SEP] PACK OF 72 SKULL CAKE CASES [SEP] PAPER BUNTING COLOURED LACE [SEP] CREAM FELT EASTER EGG BASKET [SEP] DECORATION , WOBBLY RABBIT , METAL [SEP] FELT EGG COSY BLUE RABBIT [SEP] HEART DECORATION RUSTIC HANGING [SEP] WHITE PEARL BEADED HEART, SMALL [SEP] WOBBLING METAL CHICKEN EASTER [SEP] FELT EGG COSY WHITE RABBIT [SEP] RIDGED GLASS T-LIGHT HOLDER [SEP] GARLAND WOODEN HAPPY EASTER [SEP] FELT EGG COSY CHICKEN [SEP] HANGING SPRING FLOWER EGG SMALL [SEP] BIRD DECORATION RED SPOT [SEP] SMALL HEART FLOWERS HOOK [SEP] PAPER BUNTING WHITE LACE [SEP] HANGING SPRING FLOWER EGG LARGE [SEP] REGENCY CAKESTAND 3 TIER [SEP] CHARLIE AND LOLA FIGURES TINS [SEP] CERAMIC CAKE STAND + HANGING CAKES [SEP] FRYING PAN RED POLKADOT [SEP] MILK PAN BLUE RETROSPOT [SEP] PINK SPOTTY CHILDS UMBRELLA [SEP] CERAMIC BIRDHOUSE FINCH BLUE ROOF [SEP] CERAMIC BIRDHOUSE BUTTERFLY SMALL [SEP] CERAMIC CAKE BOWL + HANGING CAKES [SEP] TOADSTOOL MONEY BOX [SEP] GREEN FERN JOURNAL [SEP] GUMBALL COAT RACK [SEP] JUMBO BAG SCANDINAVIAN PAISLEY [SEP] ROUND CONTAINER SET OF 5 RETROSPOT [SEP] FRENCH BOTTLE , LAVENDER [SEP] CERAMIC CAKE BOWL + HANGING CAKES [SEP] GARDEN PATH NOTEBOOK [SEP] RED LOVE HEART SHAPE CUP [SEP] LETTER \"P\" BLING KEY RING [SEP] DECORATION , WOBBLY CHICKEN, METAL [SEP] LETTER \"B\" BLING KEY RING [SEP] RED SPOTTY SHOPPER BAG [SEP] RETRO COFFEE MUGS ASSORTED [SEP] RETRO COFFEE MUGS ASSORTED [SEP] LETTER \"G\" BLING KEY RING [SEP] DAISY GARDEN MARKER [SEP] BEADED CRYSTAL HEART GREEN SMALL [SEP] SET OF 36 DINOSAUR PAPER DOILIES [SEP] SET OF 36 TEATIME PAPER DOILIES [SEP] SMALL HEART MEASURING SPOONS [SEP] SALLE DE BAIN HOOK [SEP] SET OF 36 MUSHROOM PAPER DOILIESMINI JIGSAW DOLLY GIRL [SEP] REGENCY CAKESTAND 3 TIER [SEP] DECORATION , WOBBLY CHICKEN, METAL [SEP] WOBBLING METAL CHICKEN EASTER [SEP] NATURAL SLATE HEART CHALKBOARD [SEP] HANGING SPRING FLOWER EGG SMALL [SEP] ETCHED GLASS COASTER [SEP] RED STRIPE CERAMIC DRAWER KNOB [SEP] MINI JIGSAW SPACEBOY [SEP] HANGING SPRING FLOWER EGG LARGE [SEP] MINI JIGSAW BUNNIES [SEP] EIGHT PIECE SNAKE  SET [SEP] CHILDS GARDEN FORK PINK [SEP] CHILDS GARDEN TROWEL PINK [SEP] RED SPOTTY CANDY BAG [SEP] WHEELBARROW FOR CHILDREN [SEP] TOAST ITS - FAIRY FLOWER [SEP] SET OF 4 ENGLISH ROSE COASTERS [SEP] CHILDRENS GARDEN GLOVES PINK [SEP] CHILDS GARDEN BRUSH PINK [SEP] EASTER TIN BUCKET [SEP] SILK PURSE RUSSIAN DOLL PINK [SEP] ASSORTED COLOUR MINI CASES [SEP]\n",
            "['[CLS]', 'daisy', 'folk', '##art', 'heart', 'decoration', '[SEP]', 'rose', 'folk', '##art', 'heart', 'decorations', '[SEP]', 'jungle', 'pops', '##icles', 'ice', 'lo', '##lly', 'holders', '[SEP]', 'top', 'secret', 'pen', 'set', '[SEP]', 'strawberry', 'ceramic', 'tri', '##nk', '##et', 'box', '[SEP]', 'recycling', 'bag', 'retro', '##sp', '##ot', '[SEP]', 'butterflies', 'honey', '##comb', 'garland', '[SEP]', 'yellow', 'easter', 'egg', 'hunt', 'start', 'post', '[SEP]', 'bingo', 'set', '[SEP]', 'silver', 'skull', 'hot', 'water', 'bottle', '[SEP]', 'vintage', 'snap', 'cards', '[SEP]', 'set', '12', 'retro', 'white', 'chalk', 'sticks', '[SEP]', 'sweetheart', 'ceramic', 'tri', '##nk', '##et', 'box', '[SEP]', 'bird', 'decoration', 'green', 'spot', '[SEP]', 'heart', 'decoration', 'with', 'pearls', '[SEP]', 'skulls', 'design', 'fl', '##anne', '##l', '[SEP]', 'pack', 'of', '72', 'skull', 'cake', 'cases', '[SEP]', 'paper', 'bun', '##ting', 'coloured', 'lace', '[SEP]', 'cream', 'felt', 'easter', 'egg', 'basket', '[SEP]', 'decoration', ',', 'wo', '##bbly', 'rabbit', ',', 'metal', '[SEP]', 'felt', 'egg', 'co', '##sy', 'blue', 'rabbit', '[SEP]', 'heart', 'decoration', 'rustic', 'hanging', '[SEP]', 'white', 'pearl', 'bea', '##ded', 'heart', ',', 'small', '[SEP]', 'wo', '##bbling', 'metal', 'chicken', 'easter', '[SEP]', 'felt', 'egg', 'co', '##sy', 'white', 'rabbit', '[SEP]', 'ridge', '##d', 'glass', 't', '-', 'light', 'holder', '[SEP]', 'garland', 'wooden', 'happy', 'easter', '[SEP]', 'felt', 'egg', 'co', '##sy', 'chicken', '[SEP]', 'hanging', 'spring', 'flower', 'egg', 'small', '[SEP]', 'bird', 'decoration', 'red', 'spot', '[SEP]', 'small', 'heart', 'flowers', 'hook', '[SEP]', 'paper', 'bun', '##ting', 'white', 'lace', '[SEP]', 'hanging', 'spring', 'flower', 'egg', 'large', '[SEP]', 'regency', 'cakes', '##tan', '##d', '3', 'tier', '[SEP]', 'charlie', 'and', 'lola', 'figures', 'tin', '##s', '[SEP]', 'ceramic', 'cake', 'stand', '+', 'hanging', 'cakes', '[SEP]', 'fry', '##ing', 'pan', 'red', 'polka', '##dot', '[SEP]', 'milk', 'pan', 'blue', 'retro', '##sp', '##ot', '[SEP]', 'pink', 'spot', '##ty', 'child', '##s', 'umbrella', '[SEP]', 'ceramic', 'bird', '##house', 'finch', 'blue', 'roof', '[SEP]', 'ceramic', 'bird', '##house', 'butterfly', 'small', '[SEP]', 'ceramic', 'cake', 'bowl', '+', 'hanging', 'cakes', '[SEP]', 'toad', '##sto', '##ol', 'money', 'box', '[SEP]', 'green', 'fern', 'journal', '[SEP]', 'gum', '##ball', 'coat', 'rack', '[SEP]', 'ju', '##mbo', 'bag', 'scandinavian', 'paisley', '[SEP]', 'round', 'container', 'set', 'of', '5', 'retro', '##sp', '##ot', '[SEP]', 'french', 'bottle', ',', 'lavender', '[SEP]', 'ceramic', 'cake', 'bowl', '+', 'hanging', 'cakes', '[SEP]', 'garden', 'path', 'notebook', '[SEP]', 'red', 'love', 'heart', 'shape', 'cup', '[SEP]', 'letter', '\"', 'p', '\"', 'b', '##ling', 'key', 'ring', '[SEP]', 'decoration', ',', 'wo', '##bbly', 'chicken', ',', 'metal', '[SEP]', 'letter', '\"', 'b', '\"', 'b', '##ling', 'key', 'ring', '[SEP]', 'red', 'spot', '##ty', 'shop', '##per', 'bag', '[SEP]', 'retro', 'coffee', 'mug', '##s', 'ass', '##orted', '[SEP]', 'retro', 'coffee', 'mug', '##s', 'ass', '##orted', '[SEP]', 'letter', '\"', 'g', '\"', 'b', '##ling', 'key', 'ring', '[SEP]', 'daisy', 'garden', 'marker', '[SEP]', 'bea', '##ded', 'crystal', 'heart', 'green', 'small', '[SEP]', 'set', 'of', '36', 'dinosaur', 'paper', 'doi', '##lies', '[SEP]', 'set', 'of', '36', 'tea', '##time', 'paper', 'doi', '##lies', '[SEP]', 'small', 'heart', 'measuring', 'spoon', '##s', '[SEP]', 'salle', 'de', 'bain', 'hook', '[SEP]', 'set', 'of', '36', 'mushroom', 'paper', 'doi', '##lies', '##mini', 'ji', '##gs', '##aw', 'dolly', 'girl', '[SEP]', 'regency', 'cakes', '##tan', '##d', '3', 'tier', '[SEP]', 'decoration', ',', 'wo', '##bbly', 'chicken', ',', 'metal', '[SEP]', 'wo', '##bbling', 'metal', 'chicken', 'easter', '[SEP]', 'natural', 'slate', 'heart', 'chalk', '##board', '[SEP]', 'hanging', 'spring', 'flower', 'egg', 'small', '[SEP]', 'etched', 'glass', 'coaster', '[SEP]', 'red', 'stripe', 'ceramic', 'drawer', 'knob', '[SEP]', 'mini', 'ji', '##gs', '##aw', 'space', '##boy', '[SEP]', 'hanging', 'spring', 'flower', 'egg', 'large', '[SEP]', 'mini', 'ji', '##gs', '##aw', 'bun', '##nies', '[SEP]', 'eight', 'piece', 'snake', 'set', '[SEP]', 'child', '##s', 'garden', 'fork', 'pink', '[SEP]', 'child', '##s', 'garden', 'tr', '##owe', '##l', 'pink', '[SEP]', 'red', 'spot', '##ty', 'candy', 'bag', '[SEP]', 'wheel', '##bar', '##row', 'for', 'children', '[SEP]', 'toast', 'its', '-', 'fairy', 'flower', '[SEP]', 'set', 'of', '4', 'english', 'rose', 'coaster', '##s', '[SEP]', 'children', '##s', 'garden', 'gloves', 'pink', '[SEP]', 'child', '##s', 'garden', 'brush', 'pink', '[SEP]', 'easter', 'tin', 'bucket', '[SEP]', 'silk', 'purse', 'russian', 'doll', 'pink', '[SEP]', 'ass', '##orted', 'colour', 'mini', 'cases', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7881"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPaQvnXpSrZ1"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 512"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mx5Hk82HQcA"
      },
      "source": [
        "# tok = []\n",
        "# for i in range(len(tokenized_texts)):\n",
        "#     if len(tokenized_texts[i]) > 512:\n",
        "#         tok.append(len(tokenized_texts[i]))\n",
        "# print(max(tok))\n",
        "# print(len(tok))\n",
        "# print(len(tokenized_texts))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU1MQEOcH_79"
      },
      "source": [
        "# for i in range(len(tokenized_texts)):\n",
        "#     if len(tokenized_texts[i]) > 512:\n",
        "#         while 1:\n",
        "#             id = tokenized_texts[i].index('[SEP]')\n",
        "#             tokenized_texts[i] = tokenized_texts[i][id:]\n",
        "#             tokenized_texts[i][0] = '[CLS]'\n",
        "#             if len(tokenized_texts[i]) <= 512:\n",
        "#                 break\n",
        "# tok = []\n",
        "# for i in range(len(tokenized_texts)):\n",
        "#     if len(tokenized_texts[i]) > 512:\n",
        "#         tok.append(len(tokenized_texts[i]))\n",
        "# # print(max(tok))\n",
        "# print(len(tok))\n",
        "# print(len(tokenized_texts))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBJ7259NPdJJ",
        "outputId": "bdd9fdc6-8594-4a3b-a5d5-dc8c31a1711c"
      },
      "source": [
        "for i in range(len(tokenized_texts)):\n",
        "    if len(tokenized_texts[i]) > MAX_LEN:\n",
        "        while 1:\n",
        "            id = tokenized_texts[i].index('[SEP]')\n",
        "            tokenized_texts[i] = tokenized_texts[i][id:]\n",
        "            tokenized_texts[i][0] = '[CLS]'\n",
        "            if len(tokenized_texts[i]) <= MAX_LEN:\n",
        "                break\n",
        "tok = []\n",
        "for i in range(len(tokenized_texts)):\n",
        "    if len(tokenized_texts[i]) > MAX_LEN:\n",
        "        tok.append(len(tokenized_texts[i]))\n",
        "# print(max(tok))\n",
        "print(len(tok))\n",
        "print(len(tokenized_texts))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "7881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAZRvFtCOV_I",
        "outputId": "41d30a53-7083-4149-e3ea-d36b24e633ea"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "# MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(input_ids[0])\n",
        "len(input_ids)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  101 27137  2275   102  3165  7412  2980  2300  5835   102 13528 10245\n",
            "  5329   102  2275  2260 22307  2317 16833 12668   102 12074 14692 13012\n",
            "  8950  3388  3482   102  4743 11446  2665  3962   102  2540 11446  2007\n",
            " 21944   102 21542  2640 13109 20147  2140   102  5308  1997  5824  7412\n",
            "  9850  3572   102  3259 21122  3436 11401 12922   102  6949  2371 10957\n",
            "  8288 10810   102 11446  1010 24185 24200 10442  1010  3384   102  2371\n",
            "  8288  2522  6508  2630 10442   102  2540 11446 27471  5689   102  2317\n",
            "  7247 26892  5732  2540  1010  2235   102 24185 15343  3384  7975 10957\n",
            "   102  2371  8288  2522  6508  2317 10442   102  5526  2094  3221  1056\n",
            "  1011  2422  9111   102 17017  4799  3407 10957   102  2371  8288  2522\n",
            "  6508  7975   102  5689  3500  6546  8288  2235   102  4743 11446  2417\n",
            "  3962   102  2235  2540  4870  8103   102  3259 21122  3436  2317 12922\n",
            "   102  5689  3500  6546  8288  2312   102 15647 22619  5794  2094  1017\n",
            "  7563   102  4918  1998 15137  4481  9543  2015   102 14692  9850  3233\n",
            "  1009  5689 22619   102 14744  2075  6090  2417 29499 27364   102  6501\n",
            "  6090  2630 22307 13102  4140   102  5061  3962  3723  2775  2015 12977\n",
            "   102 14692  4743  4580 16133  2630  4412   102 14692  4743  4580  9112\n",
            "  2235   102 14692  9850  4605  1009  5689 22619   102 21344 16033  4747\n",
            "  2769  3482   102  2665 20863  3485   102 16031  7384  5435 14513   102\n",
            " 18414 13344  4524 17660 23321   102  2461 11661  2275  1997  1019 22307\n",
            " 13102  4140   102  2413  5835  1010 20920   102 14692  9850  4605  1009\n",
            "  5689 22619   102  3871  4130 14960   102  2417  2293  2540  4338  2452\n",
            "   102  3661  1000  1052  1000  1038  2989  3145  3614   102 11446  1010\n",
            " 24185 24200  7975  1010  3384   102  3661  1000  1038  1000  1038  2989\n",
            "  3145  3614   102  2417  3962  3723  4497  4842  4524   102 22307  4157\n",
            " 14757  2015  4632 15613   102 22307  4157 14757  2015  4632 15613   102\n",
            "  3661  1000  1043  1000  1038  2989  3145  3614   102 10409  3871 12115\n",
            "   102 26892  5732  6121  2540  2665  2235   102  2275  1997  4029 15799\n",
            "  3259  9193 11983   102  2275  1997  4029  5572  7292  3259  9193 11983\n",
            "   102  2235  2540  9854 15642  2015   102 18005  2139 28477  8103   102\n",
            "  2275  1997  4029 18565  3259  9193 11983 25300 10147  5620 10376 19958\n",
            "  2611   102 15647 22619  5794  2094  1017  7563   102 11446  1010 24185\n",
            " 24200  7975  1010  3384   102 24185 15343  3384  7975 10957   102  3019\n",
            " 12796  2540 16833  6277   102  5689  3500  6546  8288  2235   102 20286\n",
            "  3221 16817   102  2417 18247 14692 13065 16859   102  7163 10147  5620\n",
            " 10376  2686 11097   102  5689  3500  6546  8288  2312   102  7163 10147\n",
            "  5620 10376 21122 15580   102  2809  3538  7488  2275   102  2775  2015\n",
            "  3871  9292  5061   102  2775  2015  3871 19817 29385  2140  5061   102\n",
            "  2417  3962  3723  9485  4524   102  5217  8237 10524  2005  2336   102\n",
            " 15174  2049  1011  8867  6546   102  2275  1997  1018  2394  3123 16817\n",
            "  2015   102  2336  2015  3871 11875  5061   102  2775  2015  3871  8248\n",
            "  5061   102 10957  9543 13610   102  6953  8722  2845 10658  5061   102\n",
            "  4632 15613  6120  7163  3572   102     0     0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7881"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd7106EtTF1R",
        "outputId": "a31a1c88-6f7f-412e-ab16-e003a2eadcc6"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qyN3lheTIOj",
        "outputId": "7f08ca9b-3814-40f6-cc7d-55aa8b63ac4d"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                    train_label, \n",
        "                                                                                    # random_state=2018, \n",
        "                                                                                    test_size=0.1)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                       input_ids,\n",
        "                                                    #    random_state=2018, \n",
        "                                                       test_size=0.1)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(train_labels[0])\n",
        "print(train_masks[0])\n",
        "print(validation_inputs[0])\n",
        "print(validation_labels[0])\n",
        "print(validation_masks[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  101,  2665, 18353,  5603,  3286,  6546, 21545,  3482,   102,  2317,\n",
            "         5689,  2540,  1056,  1011,  2422,  9111,   102,  4799,  4853, 14361,\n",
            "         2317,   102,  6652,  3384,  2695, 11522,  3500,   102,  3536,  1016,\n",
            "        13065,  5239,  2317,  3926,   102,  3536,  1055,  1013,  1017,  5239,\n",
            "        14405,  2317,  3926,   102,  2417, 18353,  5603,  3286,  3123, 21545,\n",
            "         3482,   102,  6949, 12074,  7163,  3108,   102, 21722, 15194,  6442,\n",
            "         3104,   102, 21722,  6209,  8722,   102,  4610,  6209,  8722,   102,\n",
            "         2034,  2465,  6209,  8722,   102,  2317, 12074,  2015,  5239,  1018,\n",
            "        22497,   102,  2275,  2509,  2338,  3482,  2665, 18353,  5603,  3286,\n",
            "         6546,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "tensor(1774.)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  2022,  8059,  1997,  1996,  4937,  3384,  3696,   102,  2882,\n",
            "         7967,  9336, 10362,   102,  2417, 19311,  2293,  2540,  6302,  4853,\n",
            "          102,  2540,  4338,  9949, 25422,   102,  2371, 21344, 16033,  4747,\n",
            "         2235,   102, 11554,  5689, 11446,  9112,   102,  2275,  1997,  1017,\n",
            "         2540, 17387, 16343,  2015,   102,  5061,  8072,  3259, 17017,   102,\n",
            "         2540, 17017, 27471, 20633,   102,  2417, 12121,  2135,  2980,  9515,\n",
            "         2317,  2540,  1012,   102, 11446, 21863,  2006,  9089,  1010,  5689,\n",
            "          102, 10957, 11446,  3564, 16291,   102,  3500, 11703,  1010,  5689,\n",
            "        14556,  2665,   102, 21863,  2160, 11446,   102,  6265, 16078,  2686,\n",
            "        11097,  2640,   102,  2775,  2015, 20376,  2686, 11097,  2640,   102,\n",
            "         8288,  2452,  3019,  7975,   102,  5308,  1997,  2260, 15875, 21122,\n",
            "        15580,   102,  2371,  8288,  2522,  6508,  7975,   102,  5689,  9112,\n",
            "         8288,   102, 27566,  4234,  2540,   102,  5061,  6546,  5689,  2540,\n",
            "          102, 18353,  1998, 28157, 14757,   102,  3500, 11703,  1010,  5689,\n",
            "        14556,  6949,   102,  5689,  3500,  6546,  8288,  2312,   102,  2371,\n",
            "         3888,  4111,  7975,   102, 11554,  5689, 11446,  4743,   102,  8962,\n",
            "         3436,  8328,  5572, 14757,   102,  2275,  1997,  5824,  5061,  2540,\n",
            "         3259,  9193, 11983,   102, 22404,  3064, 10442, 10658,   102, 11446,\n",
            "         1010, 24185, 24200, 10442,  1010,  3384,   102,  3221,  7975, 12136,\n",
            "         9841,   102,  1055,  1013,  1017,  2394, 20920,  5490,  2226, 13541,\n",
            "          102,  1055,  1013,  1017,  2413, 21161,  5490,  2226, 13541,   102,\n",
            "        11825, 10850,  2235,  6501, 26536,   102,  3061,  3384, 10442,  2007,\n",
            "         8288,   102,  2275,  1997,  2260,  7163, 21122, 15580,  1999,  1037,\n",
            "        13610,   102, 22307,  3962,  2235,  6501, 26536,   102,  9485,  3962,\n",
            "        16291,   102,  5465,  3601, 19135,  3482,  3503,   102,  2630, 15775,\n",
            "         7485,  4743,  7959, 14728,  2099,   102, 21589,  9485,  3962, 10442,\n",
            "          102, 22307,  3962, 20633,  2835, 22936,   102, 14744,  2075,  6090,\n",
            "         2417, 29499, 27364,   102,  5465,  3601, 20377, 28168,  9543,   102,\n",
            "        22307,  7516, 12136,  9841,   102, 17017,  2007,  8072,  1998, 10118,\n",
            "          102,  9850,  5127,  2293,  9001,  5061,   102,  2892, 26035, 12440,\n",
            "        22936,  3104,   102,  2312,  2461, 15536,  9102, 28005,  2121,   102,\n",
            "         2892, 26035, 12440, 22936,  3104,   102, 12074,  3145,  5239,   102,\n",
            "         2275,  1017, 15536,  9102,  8833, 25946,   102,  2371,  3888,  4111,\n",
            "        10442,   102,  2371,  3888,  4111, 10442,   102,  2371, 10419, 10658,\n",
            "         9618,   102, 10957, 11446,  8288, 16291,   102, 11446,  1010,  5689,\n",
            "         3384, 10442,   102,  3123,  5154,  8445,  2540, 14529,   102,  2341,\n",
            "         6865,  2121, 12954,  1009,  3611,  2015,  2282,   102,  2293,  2540,\n",
            "        13012,  8950,  3388,  8962,   102, 22404,  3064, 10442, 10658,   102,\n",
            "         5061,  2371, 10957,  8288, 10810,   102,  5061,  2540,  9530,  7959,\n",
            "         6916,  1999,  7270,   102,  2540,  5044,  5259,   102,  2833, 11661,\n",
            "         2275,  1017,  2293,  2540,   102,  2371, 10419,  1020,  6546,  2814,\n",
            "          102, 22307,  3962,  5016,  7270,  3503,   102, 27566,  9112, 15281,\n",
            "          102,  7605,  8072,  6861, 18274,  3259, 17017,   102,  2317,  2293,\n",
            "         9001, 12856,   102,  2274,  2540,  5689, 11446,   102,  2413, 29484,\n",
            "         8962,  1059, 11876,   102,  5465,  3601,  5572, 28353,  5149,   102,\n",
            "        27566,  2540, 17017,  2007, 10118,   102,  5061,  2540, 14981,  2980,\n",
            "         2300,  5835,   102,  2484,  5689, 10957,  6763, 18686, 14366,   102,\n",
            "         3336,  8000,  2417, 18353,  5603,  3286,  4377,   102,  2330,  2701,\n",
            "         3384,  3696,   102, 22788,  8000,  2417, 18353,  5603,  3286, 10557,\n",
            "          102, 10957, 11446,  3019, 14556,   102,  2371,  8288,  2522,  6508,\n",
            "         2630, 10442,   102,  2371,  8288,  2522,  6508,  2317, 10442,   102,\n",
            "            0,     0])\n",
            "tensor(1210.)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwHiH9mqTf-q"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 2\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OcCk7qVeppZ",
        "outputId": "73b57f5a-a434-4671-87bf-359acdead8e0"
      },
      "source": [
        "test['Detail'] = test['Detail'].str.replace('&&', ' [SEP] ')\n",
        "sentences = test['Detail']\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "sentences[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] PACK OF 20 SKULL PAPER NAPKINS [SEP] SMALL DOLLY MIX DESIGN ORANGE BOWL [SEP] UNION JACK FLAG PASSPORT COVER [SEP] LOO ROLL  METAL SIGN [SEP] SET/10 RED SPOTTY PARTY CANDLES [SEP] METAL SIGN,CUPCAKE SINGLE HOOK [SEP] 3D SHEET OF CAT STICKERS [SEP] SET/6 WOODLAND PAPER CUPS [SEP] CERAMIC CAKE STAND + HANGING CAKES [SEP] DOOR MAT TOPIARY [SEP] ANTIQUE SILVER TEA GLASS ETCHED [SEP] PINK FAIRY CAKE CUSHION COVER [SEP] CLASSIC FRENCH STYLE BASKET GREEN [SEP] MINATURE COLOURED GARDENING SET [SEP] BLUE FLOCK GLASS CANDLEHOLDER [SEP] BLACK FLOWER CANDLE PLATE [SEP] RED SPOTTY CHARLOTTE BAG [SEP] COLOUR GLASS T-LIGHT HOLDER HANGING [SEP] MULTI COLOUR SILVER T-LIGHT HOLDER [SEP] METAL SIGN CUPCAKE SINGLE HOOK [SEP] DOOR MAT FAIRY CAKE [SEP] METAL SIGN,CUPCAKE SINGLE HOOK [SEP] ANTIQUE LILY FAIRY LIGHTS [SEP] QUEEN OF THE SKIES PASSPORT COVER [SEP] EDWARDIAN PARASOL BLACK [SEP] EDWARDIAN PARASOL RED [SEP] SET/2 RED SPOTTY TEA TOWELS [SEP] JUMBO BAG CHARLIE AND LOLA TOYS [SEP] SILVER GLASS T-LIGHT SET [SEP] SILVER WASHBAG [SEP] TEA TIME TEA SET IN GIFT BOX [SEP] LADIES & GENTLEMEN METAL SIGN [SEP] LADIES & GENTLEMEN METAL SIGN [SEP] CANDY SPOT HAND BAG [SEP] RED/WHITE DOTS RUFFLED UMBRELLA [SEP] BOMBS AWAY METAL SIGN [SEP] FUNKY WASHING UP GLOVES ASSORTED [SEP] TEA TIME TEA TOWELS [SEP] CHERRY CROCHET FOOD COVER [SEP] QUEEN OF SKIES LUGGAGE TAG [SEP] SET OF 72 SKULL PAPER  DOILIES [SEP] BEWARE OF THE CAT METAL SIGN [SEP] HOT BATHS SOAP HOLDER [SEP] JUMBO STORAGE BAG SUKI [SEP] BOMBS AWAY METAL SIGN [SEP] AIRLINE LOUNGE,METAL SIGN [SEP]',\n",
              " '[CLS] SET OF 2 FANCY FONT TEA TOWELS [SEP] KINGS CHOICE TEA CADDY [SEP] LARGE RED RETROSPOT WINDMILL [SEP] PACK OF 6 HANDBAG GIFT BOXES [SEP] KINGS CHOICE GIANT TUBE MATCHES [SEP] ENAMEL MEASURING JUG CREAM [SEP] PING MICROWAVE APRON [SEP] KINGS CHOICE BISCUIT TIN [SEP] ENAMEL COLANDER CREAM [SEP] BOX OF VINTAGE JIGSAW BLOCKS [SEP] ENAMEL BREAD BIN CREAM [SEP] IVORY KITCHEN SCALES [SEP] PICNIC BASKET WICKER LARGE [SEP] PICNIC BASKET WICKER SMALLBAKING SET 9 PIECE RETROSPOT [SEP] COFFEE MUG DOG + BALL DESIGN [SEP] CHILDS BREAKFAST SET CIRCUS PARADE [SEP] JUMBO SHOPPER VINTAGE RED PAISLEY [SEP] SKULL LUNCH BOX WITH CUTLERY [SEP] JUMBO BAG OWLS [SEP] SILK PURSE RUSSIAN DOLL RED [SEP] ROUND SNACK BOXES SET OF4 WOODLAND [SEP] PLASTERS IN TIN STRONGMAN [SEP] JUMBO  BAG BAROQUE BLACK WHITE [SEP] STRAWBERRY LUNCH BOX WITH CUTLERY [SEP] RED RETROSPOT PICNIC BAG [SEP] REGENCY CAKESTAND 3 TIER [SEP] SET/6 RED SPOTTY PAPER CUPS [SEP] SET/6 RED SPOTTY PAPER PLATES [SEP] SET/20 RED SPOTTY PAPER NAPKINS [SEP] PACK OF 6 BIRDY GIFT TAGS [SEP] WHITE BAROQUE WALL CLOCK [SEP] PET MUG, BUDGIE [SEP] ROUND CONTAINER SET OF 5 RETROSPOT [SEP] BLACK BAROQUE CUCKOO CLOCK [SEP] SET/5 RED SPOTTY LID GLASS BOWLS [SEP]',\n",
              " '[CLS] MEDIUM CHINESE STYLE SCISSOR [SEP] SMALL CHINESE STYLE SCISSOR [SEP] LOVEBIRD HANGING DECORATION WHITE [SEP] SWALLOWS GREETING CARD [SEP] BANQUET BIRTHDAY  CARD [SEP] DINOSAUR BIRTHDAY CARD [SEP] COWBOYS AND INDIANS BIRTHDAY CARD [SEP] RAINY LADIES BIRTHDAY CARD [SEP] RING OF ROSES BIRTHDAY CARD [SEP] FAWN AND MUSHROOM GREETING CARD [SEP] ELEPHANT, BIRTHDAY CARD, [SEP] VINTAGE KID DOLLY CARD [SEP] LARGE CHINESE STYLE SCISSOR [SEP] VINTAGE CARAVAN GREETING CARD [SEP] T-LIGHT HOLDER HANGING LACE [SEP] FANCY FONT BIRTHDAY CARD, [SEP] PINK CHERRY LIGHTS [SEP] JUMBO BAG RED WHITE SPOTTY [SEP] PACK/12 BLUE FOLKART CARDS [SEP] DIAMANTE HEART SHAPED WALL MIRROR, [SEP] SILVER CHERRY LIGHTS [SEP] WHITE METAL LANTERN [SEP] GREEN FERN JOURNAL [SEP] BLUE PAISLEY JOURNAL [SEP] GREEN CHERRY LIGHTS [SEP] POMPOM CURTAIN [SEP] CREAM HEART CARD HOLDER [SEP] LIGHT PINK CHERRY LIGHTS [SEP] WHITE CHERRY LIGHTS [SEP] HEART SHAPED MIRROR [SEP] WHITE DOVE HONEYCOMB PAPER GARLAND [SEP]',\n",
              " '[CLS] CERAMIC BIRDHOUSE FINCH BLUE  LARGE [SEP] CERAMIC BIRDHOUSE BLACK ROSE  LARGE [SEP] CERAMIC BIRDHOUSE FINCH BLUE ROOF [SEP] CERAMIC BIRDHOUSE BUTTERFLY SMALL [SEP] METAL MEDINA LANTERN [SEP] WHITE HANGING HEART T-LIGHT HOLDERROMANTIC PINKS RIBBONS [SEP] TRADITIONAL CHRISTMAS RIBBONS [SEP] LUSH GREENS RIBBONS [SEP] RED HANGING HEART T-LIGHT HOLDER [SEP] ENGLISH ROSE NOTEBOOK A6 SIZE [SEP] CACTI T-LIGHT CANDLES [SEP] ASSORTED COLOUR SILK COIN PURSE [SEP] HANGING JAM JAR T-LIGHT HOLDER [SEP] MOROCCAN TEA GLASS [SEP] HYACINTH BULB T-LIGHT CANDLES [SEP] ASSORTED COLOUR SILK COSMETIC PURSE [SEP] PAPER BUNTING VINTAGE PAISLEY [SEP] DOOR MAT NEW ENGLAND [SEP] 6 CHOCOLATE LOVE HEART T-LIGHTS [SEP] S/4 BURGUNDY WINE DINNER CANDLES [SEP] RED SPOTTY ROUND CAKE TINS [SEP] GREEN BLUE FLOWER PIGGY BANK [SEP] BLUE YELLOW FLOWER PIGGY BANK [SEP] RED WHITE SCARF  HOT WATER BOTTLE [SEP] RED SPOT HEART HOT WATER BOTTLE [SEP] RED WOOLLY HOTTIE WHITE HEART. [SEP] REX CASH+CARRY JUMBO SHOPPER [SEP] SET/3 DECOUPAGE STACKING TINS [SEP] CORONA MEXICAN TRAY [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP]',\n",
              " '[CLS] COFFEE MUG CAT + BIRD DESIGN [SEP] LUNCH BAG RED SPOTTY [SEP] LUNCH BAG CARS BLUE [SEP] HOOK, 1 HANGER ,MAGIC GARDEN [SEP] CHILDS GARDEN FORK PINK [SEP] HOT BATHS SOAP HOLDER [SEP] JUMBO BAG WOODLAND ANIMALS [SEP] JUMBO BAG OWLS [SEP] JUMBO BAG TOYS [SEP] BOMBS AWAY METAL SIGN [SEP] SMALL GLASS HEART TRINKET POT [SEP] CHILDS GARDEN TROWEL PINK [SEP] GARDENERS KNEELING PAD [SEP] JUMBO BAG STRAWBERRY [SEP] COFFEE MUG APPLES DESIGN [SEP] SLATE TILE NATURAL HANGING [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] HEART OF WICKER SMALL [SEP] COLOUR GLASS T-LIGHT HOLDER HANGING [SEP] JUMBO BAG SPACEBOY DESIGN [SEP] JUMBO BAG RED RETROSPOT [SEP] JUMBO BAG PINK WITH WHITE SPOTS [SEP] JUMBO STORAGE BAG SUKI [SEP] LUNCH BAG SUKI  DESIGN [SEP] LUNCH BAG PINK RETROSPOT [SEP] LUNCH BAG SPACEBOY DESIGN [SEP] AREA PATROLLED METAL SIGN [SEP] GROW YOUR OWN PLANT IN A CAN [SEP] PLACE SETTING WHITE HEART [SEP] PLASTERS IN TIN CIRCUS PARADE [SEP] POTTERING IN THE SHED METAL SIGN [SEP] SET/5 RED SPOTTY LID GLASS BOWLS [SEP] COOKING SET RETROSPOT [SEP] DOOR MAT HEARTS [SEP] ENCHANTED BIRD COATHANGER 5 HOOK [SEP] SWEETHEART CARRY-ALL BASKET [SEP] CHILDS BREAKFAST SET CIRCUS PARADE [SEP] RED/CREAM STRIPE FRINGE HAMMOCK [SEP] BLUE/CREAM STRIPE FRINGE HAMMOCK [SEP] DOOR MAT ENGLISH ROSE [SEP] DOOR MAT NEW ENGLAND [SEP] BAKING SET SPACEBOY DESIGN [SEP] RETRO SPOT TEA SET CERAMIC 11 PC [SEP] FRYING PAN PINK POLKADOT [SEP] FRYING PAN UNION FLAG [SEP] FRYING PAN RED POLKADOT [SEP] BLUE 3 PIECE MINI DOTS CUTLERY SET [SEP] RED 3 PIECE MINI DOTS CUTLERY SET [SEP] ENGLISH ROSE MUG IN GIFT BOX [SEP] PINK 3 PIECE MINI DOTS CUTLERY SET [SEP] PLASTERS IN TIN WOODLAND ANIMALS [SEP] DOOR MAT RED SPOT [SEP] CHILDS GARDEN FORK BLUE [SEP] CHILDS GARDEN TROWEL BLUE [SEP] PLASTERS IN TIN SKULLS [SEP] HOOK, 5 HANGER ,MAGIC TOADSTOOL RED [SEP] HOOK, 3 HANGER ,MAGIC GARDEN [SEP] SET/5 RED SPOTTY LID GLASS BOWLS [SEP] HOOK, 5 HANGER , MAGIC TOADSTOOL [SEP] DOORSTOP RETROSPOT HEART [SEP] ENCHANTED BIRD PLANT CAGE [SEP] BROWN CHECK CAT DOORSTOP [SEP] ENAMEL MEASURING JUG CREAM [SEP] VINTAGE UNION JACK BUNTING [SEP] RED KITCHEN SCALES [SEP]',\n",
              " '[CLS] TEA BAG PLATE RED SPOTTY [SEP] GARLAND WOODEN HAPPY EASTER [SEP] CERAMIC CAKE DESIGN SPOTTED MUG [SEP] HEN HOUSE DECORATION [SEP] CHOCOLATE THIS WAY METAL SIGN [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] DOOR MAT UNION FLAG [SEP] DOOR MAT MULTICOLOUR STRIPE [SEP] DOOR MAT RED SPOT [SEP] RED DINER WALL CLOCK [SEP] EAU DE NIL DINER WALL CLOCK [SEP] ORGANISER WOOD ANTIQUE WHITE [SEP] AREA PATROLLED METAL SIGNTEXRIO TOMATOES CANDLE+CUP [SEP] KINGS CHOICE MUG [SEP] PINK CREAM FELT CRAFT TRINKET BOX [SEP] POSY CANDY BAG [SEP] GLASS JAR ENGLISH CONFECTIONERY [SEP] VINTAGE UNION JACK BUNTING [SEP] PARTY BUNTING [SEP] GLASS JAR DIGESTIVE BISCUITS [SEP] REGENCY CAKESTAND 3 TIER [SEP] JUMBO BAG WOODLAND ANIMALS [SEP] JUMBO BAG SCANDINAVIAN PAISLEY [SEP] FELTCRAFT DOLL MARIA [SEP] LADIES & GENTLEMEN METAL SIGN [SEP] NEW ENGLAND MILK JUG W GIFT BOX [SEP] KINGS CHOICE BISCUIT TIN [SEP] SET OF 2 TINS VINTAGE BATHROOM [SEP] RED FLORAL FELTCRAFT SHOULDER BAG [SEP] PHARMACIE FIRST AID TIN [SEP] FIRST AID TIN [SEP] SET OF 2 ROUND TINS CAMEMBERT [SEP] METAL SIGN EMPIRE TEA [SEP] GLASS JAR KINGS CHOICE [SEP]',\n",
              " \"[CLS] SPRING DEC , HANGING CHICK  GREEN [SEP] GREEN FERN POCKET BOOK [SEP] DECORATION  BUTTERFLY  MAGIC GARDEN [SEP] PACK OF 12 STICKY BUNNIES [SEP] CAKE STAND 3 TIER MAGIC GARDEN [SEP] STANDING METAL RABBIT WITH EGG [SEP] DECORATION HEN ON NEST, HANGING [SEP] GARLAND WOODEN HAPPY EASTER [SEP] EASTER CRAFT IVY WREATH WITH CHICK [SEP] LAVENDER SCENTED FABRIC HEART [SEP] EASTER DECORATION SITTING BUNNY [SEP] 60 TEATIME FAIRY CAKE CASES [SEP] PACK OF 60 PINK PAISLEY CAKE CASES [SEP] PACK OF 72 RETRO SPOT CAKE CASES [SEP] SPACEBOY BIRTHDAY CARD [SEP] TEA PARTY BIRTHDAY CARD [SEP] BOTANICAL ROSE GREETING CARD [SEP] DECORATION WHITE CHICK MAGIC GARDEN [SEP] GLITTER BUTTERFLY CLIPS [SEP] PACK OF 60 SPACEBOY CAKE CASES [SEP] TEA TIME OVEN GLOVE [SEP] EASTER DECORATION SITTING BUNNY [SEP] FELTCRAFT 6 FLOWER FRIENDS [SEP] GLITTER HANGING BUTTERFLY STRING [SEP] DECORATION HEN ON NEST, HANGING [SEP] BRIGHT BLUES RIBBONS [SEP] FELT TOADSTOOL  SMALL [SEP] HEART GARLAND RUSTIC PADDED [SEP] FELT FARM ANIMAL WHITE BUNNY [SEP] DECORATION  BUTTERFLY  MAGIC GARDEN [SEP] FELTCRAFT BUTTERFLY HEARTS [SEP] DECORATION WHITE CHICK MAGIC GARDEN [SEP] HEART DECORATION WITH PEARLS [SEP] HEART DECORATION RUSTIC HANGING [SEP] PLACE SETTING WHITE STAR [SEP] HEART STRING MEMO HOLDER HANGING [SEP] PAPER BUNTING COLOURED LACE [SEP] PAPER BUNTING VINTAGE PAISLEY [SEP] SAVE THE PLANET COTTON TOTE BAG [SEP] BUNNY DECORATION MAGIC GARDEN12 PENCIL SMALL TUBE WOODLAND [SEP] FELTCRAFT HAIRBAND RED AND BLUE [SEP] SMALL POPCORN HOLDER [SEP] BEST DAD CANDLE LETTERS [SEP] PACK OF 6 SANDCASTLE FLAGS ASSORTED [SEP] TRADITIONAL MODELLING CLAY [SEP] PLASTERS IN TIN CIRCUS PARADE [SEP] CHILDRENS GARDEN GLOVES PINK [SEP] PICTURE DOMINOES [SEP] PLASTERS IN TIN SPACEBOY [SEP] BALLOON PUMP WITH 10 BALLOONS [SEP] 60 TEATIME FAIRY CAKE CASES [SEP] MAGNETS PACK OF 4 VINTAGE COLLAGE [SEP] PACK OF 60 DINOSAUR CAKE CASES [SEP] MAGNETS PACK OF 4 CHILDHOOD MEMORY [SEP] GLASS JAR ENGLISH CONFECTIONERY [SEP] NEW ENGLAND CERAMIC CAKE SERVER [SEP] PINK FAIRY CAKE CHILD'S APRON [SEP] PACK OF 6 BIRDY GIFT TAGS [SEP] PACK OF 60 MUSHROOM CAKE CASES [SEP] CHILDS APRON SPACEBOY DESIGN [SEP] BALLOON ART MAKE YOUR OWN FLOWERS [SEP] SMALL RED RETROSPOT WINDMILL [SEP] PINK LOVE HEART SHAPE CUP [SEP] 12 COLOURED PARTY BALLOONS [SEP] PACK OF 60 PINK PAISLEY CAKE CASES [SEP] SET OF 9 HEART SHAPED BALLOONS [SEP] HANGING METAL STAR LANTERN [SEP] 72 SWEETHEART FAIRY CAKE CASES [SEP] TOY TIDY PINK RETROSPOT [SEP] REGENCY CAKESTAND 3 TIER [SEP] PAPER BUNTING VINTAGE PAISLEY [SEP] MODERN VINTAGE COTTON SHOPPING BAG [SEP] REGENCY CAKESTAND 3 TIER [SEP] DOOR MAT ENGLISH ROSE [SEP] BAKING SET SPACEBOY DESIGN [SEP] FRYING PAN PINK POLKADOT [SEP] MILK PAN PINK RETROSPOT [SEP] PAPER BUNTING VINTAGE PAISLEY [SEP] PINK PAPER PARASOL [SEP] HEART GARLAND RUSTIC PADDED [SEP] S/16 VINTAGE ROSE CUTLERY [SEP] COFFEE MUG PEARS  DESIGN [SEP] COFFEE MUG APPLES DESIGN [SEP] DANISH ROSE PHOTO FRAME [SEP] RETRO BLUE SPOTTY WASHING UP GLOVES [SEP] PENCIL CASE LIFE IS BEAUTIFUL [SEP] FANNY'S REST STOPMETAL SIGN [SEP] WOODEN UNION JACK BUNTING [SEP] WOODLAND  STICKERS [SEP] ENAMEL PINK TEA CONTAINER [SEP] VINTAGE RED TEATIME MUG [SEP] SAVE THE PLANET MUG [SEP] BOUDOIR SQUARE TISSUE BOX [SEP] 200 RED + WHITE BENDY STRAWS [SEP] SANDWICH BATH SPONGE [SEP] PLASTERS IN TIN CIRCUS PARADE [SEP] PLASTERS IN TIN VINTAGE PAISLEY [SEP] BLUE FAIRY CAKE CHILD'S APRON [SEP] SET OF 20 KIDS COOKIE CUTTERS [SEP] BALLOON ART MAKE YOUR OWN FLOWERS [SEP]\",\n",
              " '[CLS] PARTY METAL SIGN [SEP] TROPICAL LUGGAGE TAG [SEP] SET/20 STRAWBERRY PAPER NAPKINS [SEP] PINK FLY SWAT [SEP] PACK OF 72 RETRO SPOT CAKE CASES [SEP] HOT WATER BOTTLE TEA AND SYMPATHY [SEP] SET/4 MODERN VINTAGE COTTON NAPKINS [SEP] HEART SHAPE WIRELESS DOORBELL [SEP] S/4 BLACK DINNER CANDLE SILVER FLOC [SEP] BLUE & WHITE BREAKFAST TRAY [SEP] JUMBO STORAGE BAG SKULLS [SEP] HANGING HEART ZINC T-LIGHT HOLDER [SEP] JUMBO BAG STRAWBERRY [SEP] CACTI T-LIGHT CANDLES [SEP] CHARLOTTE BAG , PINK/WHITE SPOTS [SEP] RED RETROSPOT JUMBO BAG [SEP] DINOSAUR PARTY BAG + STICKER SET [SEP] BOX OF 24 COCKTAIL PARASOLS [SEP] JUMBO SHOPPER VINTAGE RED PAISLEY [SEP] PAPER BUNTING WHITE LACE [SEP] JUMBO BAG PINK WITH WHITE SPOTS [SEP] JUMBO BAG WOODLAND ANIMALS [SEP] DOOR MAT WELCOME PUPPIES [SEP] SET/4 WHITE RETRO STORAGE CUBES [SEP] DOOR MAT RED SPOT [SEP] POMPOM CURTAIN [SEP] PEACE SMALL WOOD LETTERS [SEP] BLUE FLYING SINGING CANARY [SEP] CORONA MEXICAN TRAY [SEP] PIN CUSHION RUSSIAN DOLL PINK [SEP] MILK PAN RED RETROSPOT [SEP] STRIPES DESIGN MONKEY DOLL [SEP] WALL TIDY RETROSPOT [SEP] VIPPASSPORT COVER [SEP] RETRO SPOT CANDLE  MEDIUM [SEP] ENGLISH ROSE DESIGN PEG BAG [SEP] TROPICAL PASSPORT COVER [SEP] RED SPOTTY PEG BAG [SEP] GUMBALL COAT RACK [SEP]',\n",
              " '[CLS] CREAM FELT EASTER EGG BASKET [SEP]',\n",
              " '[CLS] SKULLS  STICKERS [SEP] SET/4 SKULL BADGES [SEP] SET/4 BADGES DOGS [SEP] ASSORTED TUTTI FRUTTI PEN [SEP] SET OF 6 FUNKY BEAKERS [SEP] OFFICE MUG WARMER PINK [SEP] MEASURING TAPE RUSSIAN DOLL BLUE [SEP] SAVE THE PLANET COTTON TOTE BAG [SEP] PAPER BUNTING COLOURED LACE [SEP] WOODEN ROUNDERS GARDEN SET [SEP] ASSORTED TUTTI FRUTTI PEN [SEP] GIN AND TONIC MUG [SEP] SET/4 BADGES CUTE CREATURES [SEP] ASSORTED TUTTI FRUTTI PEN [SEP] ASSORTED TUTTI FRUTTI MIRROR [SEP] I CAN ONLY PLEASE ONE PERSON MUG [SEP] SKULLS WRITING SET [SEP] HOOK, 3 HANGER ,MAGIC GARDEN [SEP] ASSORTED TUTTI FRUTTI NOTEBOOK [SEP] CREAM SLICE FLANNEL PINK SPOT [SEP] BLUE POT PLANT CANDLE [SEP] SET/4 BADGES BALLOON GIRL [SEP] LIPSTICK PEN RED [SEP] DINOSAUR KEYRINGS ASSORTED [SEP] PINK DOUGHNUT TRINKET POT [SEP] MINI PAINT SET VINTAGE [SEP] WORLD WAR 2 GLIDERS ASSTD DESIGNS [SEP] TOOTHPASTE TUBE PEN [SEP] BAG 125g SWIRLY MARBLES [SEP] ASSTD DESIGN BUBBLE GUM RING [SEP] KITTY PENCIL ERASERS [SEP] FANCY FONTS BIRTHDAY WRAP [SEP] SKULL SHOULDER BAG [SEP] ASSORTED COLOURS SILK FAN [SEP] SET OF 6 3D KIT CARDS FOR KIDS [SEP] 12 PENCILS TALL TUBE SKULLS [SEP] MIRRORED WALL ART SKULLS [SEP] LIPSTICK PEN FUSCHIA [SEP] CACTI T-LIGHT CANDLES [SEP] LIPSTICK PEN FUSCHIA [SEP] LIPSTICK PEN BABY PINK [SEP] FANCY FONT BIRTHDAY CARD, [SEP] SKULLS GREETING CARD [SEP] SWALLOWS GREETING CARD [SEP] COWBOYS AND INDIANS BIRTHDAY CARD [SEP] CARD DOLLY GIRL [SEP] ASSORTED TUTTI FRUTTI NOTEBOOK [SEP] CAKE STAND LACE WHITE [SEP] WOODLAND  STICKERS [SEP] VINTAGE UNION JACK DOORSTOP [SEP] PLASTERS IN TIN CIRCUS PARADE [SEP] RED GINGHAM ROSE JEWELLERY BOX [SEP] PLASTERS IN TIN WOODLAND ANIMALS [SEP] WOODLAND ANIMAL  WRITING SET [SEP] SCOTTIES DESIGN WASHBAG [SEP] SET/6 PINK  BUTTERFLY T-LIGHTS [SEP] WOODEN PICTURE FRAME WHITE FINISH [SEP] MIRRORED WALL ART POPPIES [SEP] GLITTER BUTTERFLY CLIPS [SEP] WOODEN FRAME ANTIQUE WHITE [SEP] WHITE LOVEBIRD LANTERN [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] PINK POT PLANT CANDLE [SEP] S/4 CACTI CANDLES [SEP] TV DINNER TRAY VINTAGE PAISLEY [SEP] RETRO SPOT TEA SET CERAMIC 11 PC [SEP] HEART EAR MUFF HEADPHONES [SEP] UNION STRIPE WITH FRINGE  HAMMOCK [SEP] COOKING SET RETROSPOT [SEP] GLASS CAKE COVER AND PLATE [SEP] MIRRORED DISCO BALL [SEP] PLASTERS IN TIN SPACEBOY [SEP] HEART IVORY TRELLIS LARGE [SEP] PLASTERS IN TIN SKULLS [SEP] JUNGLE POPSICLES ICE LOLLY HOLDERS [SEP] LARGE CAKE TOWEL PINK SPOTS [SEP] CAKE STAND WHITE TWO TIER LACE [SEP] TEA TIME CAKE STAND IN GIFT BOX [SEP] BOX OF 24 COCKTAIL PARASOLS [SEP] LIPSTICK PEN BABY PINK [SEP] PACK 20 ENGLISH ROSE PAPER NAPKINS [SEP] REX CASH+CARRY JUMBO SHOPPER [SEP] REX CASH+CARRY JUMBO SHOPPER [SEP] HEART FILIGREE DOVE LARGE [SEP] 36 PENCILS TUBE RED SPOTTY [SEP] SUKI  SHOULDER BAG [SEP] 36 PENCILS TUBE WOODLAND [SEP] BINGO SET [SEP] SWALLOW SQUARE TISSUE BOX [SEP] MADRAS NOTEBOOK MEDIUM [SEP] REX CASH+CARRY JUMBO SHOPPER [SEP] PACK 20 ENGLISH ROSE PAPER NAPKINS [SEP] SET/10 RED SPOTTY PARTY CANDLES [SEP] LIPSTICK PEN FUSCHIA [SEP] PICNIC BASKET WICKER SMALL [SEP] DISCO BALL ROTATOR BATTERY OPERATED [SEP] MADRAS NOTEBOOK LARGE [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBN3ht28XoNK",
        "outputId": "d398518f-dd70-48ca-efc7-2e8a37c16d79"
      },
      "source": [
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print (sentences[0])\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] PACK OF 20 SKULL PAPER NAPKINS [SEP] SMALL DOLLY MIX DESIGN ORANGE BOWL [SEP] UNION JACK FLAG PASSPORT COVER [SEP] LOO ROLL  METAL SIGN [SEP] SET/10 RED SPOTTY PARTY CANDLES [SEP] METAL SIGN,CUPCAKE SINGLE HOOK [SEP] 3D SHEET OF CAT STICKERS [SEP] SET/6 WOODLAND PAPER CUPS [SEP] CERAMIC CAKE STAND + HANGING CAKES [SEP] DOOR MAT TOPIARY [SEP] ANTIQUE SILVER TEA GLASS ETCHED [SEP] PINK FAIRY CAKE CUSHION COVER [SEP] CLASSIC FRENCH STYLE BASKET GREEN [SEP] MINATURE COLOURED GARDENING SET [SEP] BLUE FLOCK GLASS CANDLEHOLDER [SEP] BLACK FLOWER CANDLE PLATE [SEP] RED SPOTTY CHARLOTTE BAG [SEP] COLOUR GLASS T-LIGHT HOLDER HANGING [SEP] MULTI COLOUR SILVER T-LIGHT HOLDER [SEP] METAL SIGN CUPCAKE SINGLE HOOK [SEP] DOOR MAT FAIRY CAKE [SEP] METAL SIGN,CUPCAKE SINGLE HOOK [SEP] ANTIQUE LILY FAIRY LIGHTS [SEP] QUEEN OF THE SKIES PASSPORT COVER [SEP] EDWARDIAN PARASOL BLACK [SEP] EDWARDIAN PARASOL RED [SEP] SET/2 RED SPOTTY TEA TOWELS [SEP] JUMBO BAG CHARLIE AND LOLA TOYS [SEP] SILVER GLASS T-LIGHT SET [SEP] SILVER WASHBAG [SEP] TEA TIME TEA SET IN GIFT BOX [SEP] LADIES & GENTLEMEN METAL SIGN [SEP] LADIES & GENTLEMEN METAL SIGN [SEP] CANDY SPOT HAND BAG [SEP] RED/WHITE DOTS RUFFLED UMBRELLA [SEP] BOMBS AWAY METAL SIGN [SEP] FUNKY WASHING UP GLOVES ASSORTED [SEP] TEA TIME TEA TOWELS [SEP] CHERRY CROCHET FOOD COVER [SEP] QUEEN OF SKIES LUGGAGE TAG [SEP] SET OF 72 SKULL PAPER  DOILIES [SEP] BEWARE OF THE CAT METAL SIGN [SEP] HOT BATHS SOAP HOLDER [SEP] JUMBO STORAGE BAG SUKI [SEP] BOMBS AWAY METAL SIGN [SEP] AIRLINE LOUNGE,METAL SIGN [SEP]\n",
            "['[CLS]', 'pack', 'of', '20', 'skull', 'paper', 'napkin', '##s', '[SEP]', 'small', 'dolly', 'mix', 'design', 'orange', 'bowl', '[SEP]', 'union', 'jack', 'flag', 'passport', 'cover', '[SEP]', 'lo', '##o', 'roll', 'metal', 'sign', '[SEP]', 'set', '/', '10', 'red', 'spot', '##ty', 'party', 'candles', '[SEP]', 'metal', 'sign', ',', 'cup', '##cake', 'single', 'hook', '[SEP]', '3d', 'sheet', 'of', 'cat', 'stick', '##ers', '[SEP]', 'set', '/', '6', 'woodland', 'paper', 'cups', '[SEP]', 'ceramic', 'cake', 'stand', '+', 'hanging', 'cakes', '[SEP]', 'door', 'mat', 'top', '##iary', '[SEP]', 'antique', 'silver', 'tea', 'glass', 'etched', '[SEP]', 'pink', 'fairy', 'cake', 'cushion', 'cover', '[SEP]', 'classic', 'french', 'style', 'basket', 'green', '[SEP]', 'mina', '##ture', 'coloured', 'gardening', 'set', '[SEP]', 'blue', 'flock', 'glass', 'candle', '##holder', '[SEP]', 'black', 'flower', 'candle', 'plate', '[SEP]', 'red', 'spot', '##ty', 'charlotte', 'bag', '[SEP]', 'colour', 'glass', 't', '-', 'light', 'holder', 'hanging', '[SEP]', 'multi', 'colour', 'silver', 't', '-', 'light', 'holder', '[SEP]', 'metal', 'sign', 'cup', '##cake', 'single', 'hook', '[SEP]', 'door', 'mat', 'fairy', 'cake', '[SEP]', 'metal', 'sign', ',', 'cup', '##cake', 'single', 'hook', '[SEP]', 'antique', 'lily', 'fairy', 'lights', '[SEP]', 'queen', 'of', 'the', 'skies', 'passport', 'cover', '[SEP]', 'edward', '##ian', 'para', '##sol', 'black', '[SEP]', 'edward', '##ian', 'para', '##sol', 'red', '[SEP]', 'set', '/', '2', 'red', 'spot', '##ty', 'tea', 'towels', '[SEP]', 'ju', '##mbo', 'bag', 'charlie', 'and', 'lola', 'toys', '[SEP]', 'silver', 'glass', 't', '-', 'light', 'set', '[SEP]', 'silver', 'wash', '##bag', '[SEP]', 'tea', 'time', 'tea', 'set', 'in', 'gift', 'box', '[SEP]', 'ladies', '&', 'gentlemen', 'metal', 'sign', '[SEP]', 'ladies', '&', 'gentlemen', 'metal', 'sign', '[SEP]', 'candy', 'spot', 'hand', 'bag', '[SEP]', 'red', '/', 'white', 'dots', 'ru', '##ffled', 'umbrella', '[SEP]', 'bombs', 'away', 'metal', 'sign', '[SEP]', 'funky', 'washing', 'up', 'gloves', 'ass', '##orted', '[SEP]', 'tea', 'time', 'tea', 'towels', '[SEP]', 'cherry', 'cr', '##oche', '##t', 'food', 'cover', '[SEP]', 'queen', 'of', 'skies', 'luggage', 'tag', '[SEP]', 'set', 'of', '72', 'skull', 'paper', 'doi', '##lies', '[SEP]', 'be', '##ware', 'of', 'the', 'cat', 'metal', 'sign', '[SEP]', 'hot', 'baths', 'soap', 'holder', '[SEP]', 'ju', '##mbo', 'storage', 'bag', 'su', '##ki', '[SEP]', 'bombs', 'away', 'metal', 'sign', '[SEP]', 'airline', 'lounge', ',', 'metal', 'sign', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIrsNX6S0Vql"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "# MAX_LEN = 128"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8js9Zgz0XH6",
        "outputId": "b171701e-5b91-40fe-96bc-ea173ef99746"
      },
      "source": [
        "for i in range(len(tokenized_texts)):\n",
        "    if len(tokenized_texts[i]) > MAX_LEN:\n",
        "        while 1:\n",
        "            id = tokenized_texts[i].index('[SEP]')\n",
        "            tokenized_texts[i] = tokenized_texts[i][id:]\n",
        "            tokenized_texts[i][0] = '[CLS]'\n",
        "            if len(tokenized_texts[i]) <= MAX_LEN:\n",
        "                break\n",
        "tok = []\n",
        "for i in range(len(tokenized_texts)):\n",
        "    if len(tokenized_texts[i]) > MAX_LEN:\n",
        "        tok.append(len(tokenized_texts[i]))\n",
        "# print(max(tok))\n",
        "print(len(tok))\n",
        "print(len(tokenized_texts))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "3378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihP1hg9F0PI5",
        "outputId": "205d2e77-a9ac-4b21-d55c-3e471f1c9bf4"
      },
      "source": [
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  101,  5308,  1997,  2322,  7412,  3259, 20619,  2015,   102,\n",
              "        2235, 19958,  4666,  2640,  4589,  4605,   102,  2586,  2990,\n",
              "        5210, 12293,  3104,   102,  8840,  2080,  4897,  3384,  3696,\n",
              "         102,  2275,  1013,  2184,  2417,  3962,  3723,  2283, 14006,\n",
              "         102,  3384,  3696,  1010,  2452, 17955,  2309,  8103,   102,\n",
              "        7605,  7123,  1997,  4937,  6293,  2545,   102,  2275,  1013,\n",
              "        1020, 11051,  3259, 10268,   102, 14692,  9850,  3233,  1009,\n",
              "        5689, 22619,   102,  2341, 13523,  2327, 17302,   102, 14361,\n",
              "        3165,  5572,  3221, 20286,   102,  5061,  8867,  9850, 22936,\n",
              "        3104,   102,  4438,  2413,  2806, 10810,  2665,   102, 19808,\n",
              "       11244, 11401, 21529,  2275,   102,  2630, 19311,  3221, 13541,\n",
              "       14528,   102,  2304,  6546, 13541,  5127,   102,  2417,  3962,\n",
              "        3723,  5904,  4524,   102,  6120,  3221,  1056,  1011,  2422,\n",
              "        9111,  5689,   102,  4800,  6120,  3165,  1056,  1011,  2422,\n",
              "        9111,   102,  3384,  3696,  2452, 17955,  2309,  8103,   102,\n",
              "        2341, 13523,  8867,  9850,   102,  3384,  3696,  1010,  2452,\n",
              "       17955,  2309,  8103,   102, 14361,  7094,  8867,  4597,   102,\n",
              "        3035,  1997,  1996, 15717, 12293,  3104,   102,  3487,  2937,\n",
              "       11498, 19454,  2304,   102,  3487,  2937, 11498, 19454,  2417,\n",
              "         102,  2275,  1013,  1016,  2417,  3962,  3723,  5572, 24213,\n",
              "         102, 18414, 13344,  4524,  4918,  1998, 15137, 10899,   102,\n",
              "        3165,  3221,  1056,  1011,  2422,  2275,   102,  3165,  9378,\n",
              "       16078,   102,  5572,  2051,  5572,  2275,  1999,  5592,  3482,\n",
              "         102,  6456,  1004, 11218,  3384,  3696,   102,  6456,  1004,\n",
              "       11218,  3384,  3696,   102,  9485,  3962,  2192,  4524,   102,\n",
              "        2417,  1013,  2317, 14981, 21766, 28579, 12977,   102,  9767,\n",
              "        2185,  3384,  3696,   102, 24151, 12699,  2039, 11875,  4632,\n",
              "       15613,   102,  5572,  2051,  5572, 24213,   102,  9115, 13675,\n",
              "       23555,  2102,  2833,  3104,   102,  3035,  1997, 15717, 17434,\n",
              "        6415,   102,  2275,  1997,  5824,  7412,  3259,  9193, 11983,\n",
              "         102,  2022,  8059,  1997,  1996,  4937,  3384,  3696,   102,\n",
              "        2980, 19692,  7815,  9111,   102, 18414, 13344,  5527,  4524,\n",
              "       10514,  3211,   102,  9767,  2185,  3384,  3696,   102,  8582,\n",
              "       11549,  1010,  3384,  3696,   102,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c3hMlpr0SLZ",
        "outputId": "725f59df-67f0-4b4b-a985-e7b6467e2b23"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ulFavqXnQP",
        "outputId": "e899aeb0-2caa-4c82-869a-ab5845bf60ea"
      },
      "source": [
        "# 데이터를 파이토치의 텐서로 변환\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(test_label)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "print(test_inputs[0])\n",
        "print(test_labels[0])\n",
        "print(test_masks[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  101,  5308,  1997,  2322,  7412,  3259, 20619,  2015,   102,  2235,\n",
            "        19958,  4666,  2640,  4589,  4605,   102,  2586,  2990,  5210, 12293,\n",
            "         3104,   102,  8840,  2080,  4897,  3384,  3696,   102,  2275,  1013,\n",
            "         2184,  2417,  3962,  3723,  2283, 14006,   102,  3384,  3696,  1010,\n",
            "         2452, 17955,  2309,  8103,   102,  7605,  7123,  1997,  4937,  6293,\n",
            "         2545,   102,  2275,  1013,  1020, 11051,  3259, 10268,   102, 14692,\n",
            "         9850,  3233,  1009,  5689, 22619,   102,  2341, 13523,  2327, 17302,\n",
            "          102, 14361,  3165,  5572,  3221, 20286,   102,  5061,  8867,  9850,\n",
            "        22936,  3104,   102,  4438,  2413,  2806, 10810,  2665,   102, 19808,\n",
            "        11244, 11401, 21529,  2275,   102,  2630, 19311,  3221, 13541, 14528,\n",
            "          102,  2304,  6546, 13541,  5127,   102,  2417,  3962,  3723,  5904,\n",
            "         4524,   102,  6120,  3221,  1056,  1011,  2422,  9111,  5689,   102,\n",
            "         4800,  6120,  3165,  1056,  1011,  2422,  9111,   102,  3384,  3696,\n",
            "         2452, 17955,  2309,  8103,   102,  2341, 13523,  8867,  9850,   102,\n",
            "         3384,  3696,  1010,  2452, 17955,  2309,  8103,   102, 14361,  7094,\n",
            "         8867,  4597,   102,  3035,  1997,  1996, 15717, 12293,  3104,   102,\n",
            "         3487,  2937, 11498, 19454,  2304,   102,  3487,  2937, 11498, 19454,\n",
            "         2417,   102,  2275,  1013,  1016,  2417,  3962,  3723,  5572, 24213,\n",
            "          102, 18414, 13344,  4524,  4918,  1998, 15137, 10899,   102,  3165,\n",
            "         3221,  1056,  1011,  2422,  2275,   102,  3165,  9378, 16078,   102,\n",
            "         5572,  2051,  5572,  2275,  1999,  5592,  3482,   102,  6456,  1004,\n",
            "        11218,  3384,  3696,   102,  6456,  1004, 11218,  3384,  3696,   102,\n",
            "         9485,  3962,  2192,  4524,   102,  2417,  1013,  2317, 14981, 21766,\n",
            "        28579, 12977,   102,  9767,  2185,  3384,  3696,   102, 24151, 12699,\n",
            "         2039, 11875,  4632, 15613,   102,  5572,  2051,  5572, 24213,   102,\n",
            "         9115, 13675, 23555,  2102,  2833,  3104,   102,  3035,  1997, 15717,\n",
            "        17434,  6415,   102,  2275,  1997,  5824,  7412,  3259,  9193, 11983,\n",
            "          102,  2022,  8059,  1997,  1996,  4937,  3384,  3696,   102,  2980,\n",
            "        19692,  7815,  9111,   102, 18414, 13344,  5527,  4524, 10514,  3211,\n",
            "          102,  9767,  2185,  3384,  3696,   102,  8582, 11549,  1010,  3384,\n",
            "         3696,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "tensor(1532.)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElwAjTlAXhgI"
      },
      "source": [
        "# 배치 사이즈\n",
        "# batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Te48eRGTipU",
        "outputId": "8c25ec38-1561-4a70-c937-d00a2574ecd4"
      },
      "source": [
        "# GPU 디바이스 이름 구함\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# GPU 디바이스 이름 검사\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGl9Ybi0Togd",
        "outputId": "e4a9dbed-ca2a-4ba7-946f-3ac640e3d604"
      },
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HynI-WeVBO_"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c939ef82cf642378cfe68976ce31bc4",
            "8f9fce561bce4c389483b796ca977198",
            "1683a54c6a2948b1a37dc924698faf78",
            "5f6425a1bf40462f8abc0ff7780f6784",
            "1995aa937dd64a0fbe636c8eb7ee7947",
            "8629f73d46164e6fb86ba30fc8615fa4",
            "928431d359a0477685be431f04b72ccc",
            "fd99a5305fbf4c87be1a7b1bd654bd6d",
            "7db2a14d158644fcab8f84c0d0d31cff",
            "a04c887ada2c43bfbb8146162ec6a48e",
            "335ed7f0aca64a05a95b75918ebffef6",
            "0928efcd6ce7450eb9f781ed5c473762",
            "e0c041864da947ebab5b98ab5bb88a9e",
            "4b5e64d38a274644be99c7b74ba30ceb",
            "eb4d014715a54d65a5c803ff9d54321f",
            "ff13fe6b521e4cb5ae2bc1c44ffe8189"
          ]
        },
        "id": "BVWMRDfVTtGp",
        "outputId": "8f9c65f8-b59f-4a29-de15-5780e4dad688"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(detail))\n",
        "model.cuda()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c939ef82cf642378cfe68976ce31bc4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7db2a14d158644fcab8f84c0d0d31cff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3487, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saCkXNoOTxYa"
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 에폭수\n",
        "epochs = 17\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyFFmI3T1Ih"
      },
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lVtNjJP7tia"
      },
      "source": [
        "def intop10(logit, label, mode=0, top=10):\n",
        "    '''mode = 0 (default) if you want to see boolean return'''\n",
        "    l = []\n",
        "    for _ in range(top):\n",
        "        pred = np.argmax(logit)\n",
        "        l.append(pred)\n",
        "        logit[0][pred] = np.min(logit)\n",
        "    if mode == 0:\n",
        "        print(l)\n",
        "        if label in l:\n",
        "            return True\n",
        "        else: return False\n",
        "    elif mode == 1:\n",
        "        if label in l:\n",
        "            return 1\n",
        "        else: return 0\n",
        "    elif mode == 2:\n",
        "        print(l)\n",
        "        return l"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E9UwT3KEIQR"
      },
      "source": [
        "def cal_acc(preds, label):\n",
        "    acc = 0\n",
        "    for v in preds:\n",
        "        acc += intop10(v, label, mode=1)\n",
        "    return acc / len(preds)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlhjtXv1T7e2"
      },
      "source": [
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f34HfLIyT8wl",
        "outputId": "ce5b56f1-5e1f-494f-f133-ef1673f6e4bc"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\n",
        "# seed_val = 42\n",
        "# random.seed(seed_val)\n",
        "# np.random.seed(seed_val)\n",
        "# torch.manual_seed(seed_val)\n",
        "# torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "\n",
        "# 그래디언트 초기화\n",
        "model.zero_grad()\n",
        "\n",
        "# 에폭만큼 반복\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        b_labels = b_labels.type(torch.LongTensor).to(device)\n",
        "        # print(b_labels.type())\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    acc = 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "        # acc = cal_acc(logits, label_ids)\n",
        "    print(\"Accuracy: {0:.3f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    # print(\"  Accuracy: {0:.3f}\".format(acc))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:01:55.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:03:54.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:05:55.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:07:57.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:09:59.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:00.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:02.\n",
            "\n",
            "  Average training loss: 7.56\n",
            "  Training epcoh took: 0:14:13\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.013\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 2 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:01.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:09.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 7.05\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.013\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 3 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:01.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:09.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 6.95\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.011\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 4 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:09.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 6.86\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.013\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 5 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:09.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 6.76\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.009\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 6 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 6.64\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.009\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 7 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:04.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:09.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 6.48\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.006\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 8 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:09.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 6.31\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.008\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 9 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 6.15\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.009\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 10 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:09.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 5.98\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.009\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 11 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 5.82\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.006\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 12 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 5.67\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.011\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 13 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:09.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 5.54\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.011\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 14 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 5.41\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.010\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 15 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:07.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:12.\n",
            "\n",
            "  Average training loss: 5.32\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.011\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 16 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 5.24\n",
            "  Training epcoh took: 0:14:22\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.013\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 17 / 17 ========\n",
            "Training...\n",
            "  Batch   500  of  3,546.    Elapsed: 0:02:02.\n",
            "  Batch 1,000  of  3,546.    Elapsed: 0:04:03.\n",
            "  Batch 1,500  of  3,546.    Elapsed: 0:06:05.\n",
            "  Batch 2,000  of  3,546.    Elapsed: 0:08:06.\n",
            "  Batch 2,500  of  3,546.    Elapsed: 0:10:08.\n",
            "  Batch 3,000  of  3,546.    Elapsed: 0:12:10.\n",
            "  Batch 3,500  of  3,546.    Elapsed: 0:14:11.\n",
            "\n",
            "  Average training loss: 5.18\n",
            "  Training epcoh took: 0:14:23\n",
            "\n",
            "Running Validation...\n",
            "Accuracy: 0.010\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu47xdjbUG0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fdb747a-cfa3-4ad8-cd7b-eb5708cde336"
      },
      "source": [
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "# 평가모드로 변경\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "acc = 0\n",
        "\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "    # 경과 정보 표시\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.3f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    # acc = cal_acc(logits, label_ids)\n",
        "# print(\"  Accuracy: {0:.3f}\".format(acc)) # eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of  1,689.    Elapsed: 0:00:08.\n",
            "  Batch   200  of  1,689.    Elapsed: 0:00:15.\n",
            "  Batch   300  of  1,689.    Elapsed: 0:00:23.\n",
            "  Batch   400  of  1,689.    Elapsed: 0:00:31.\n",
            "  Batch   500  of  1,689.    Elapsed: 0:00:39.\n",
            "  Batch   600  of  1,689.    Elapsed: 0:00:47.\n",
            "  Batch   700  of  1,689.    Elapsed: 0:00:54.\n",
            "  Batch   800  of  1,689.    Elapsed: 0:01:02.\n",
            "  Batch   900  of  1,689.    Elapsed: 0:01:10.\n",
            "  Batch 1,000  of  1,689.    Elapsed: 0:01:18.\n",
            "  Batch 1,100  of  1,689.    Elapsed: 0:01:25.\n",
            "  Batch 1,200  of  1,689.    Elapsed: 0:01:33.\n",
            "  Batch 1,300  of  1,689.    Elapsed: 0:01:41.\n",
            "  Batch 1,400  of  1,689.    Elapsed: 0:01:49.\n",
            "  Batch 1,500  of  1,689.    Elapsed: 0:01:56.\n",
            "  Batch 1,600  of  1,689.    Elapsed: 0:02:04.\n",
            "\n",
            "Accuracy: 0.012\n",
            "Test took: 0:02:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb_3NCkWHfWU"
      },
      "source": [
        "name = 'BERTmodel_bert_' + str(epochs) + '.pt'\n",
        "\n",
        "torch.save(model, path + name)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8MCMVixN_N2"
      },
      "source": [
        "# torch.save(model.state_dict(), './BERTmodel_stat.pt')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blw4UIKBVsUt"
      },
      "source": [
        "# 입력 데이터 변환\n",
        "def convert_input_data(sentences):\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 512\n",
        "    # if len(tokenized_texts) > MAX_LEN:\n",
        "    #     tokenized_texts = tokenized_texts[-Max_Len::]    \n",
        "    for i in range(len(tokenized_texts)):\n",
        "        if len(tokenized_texts[i]) > MAX_LEN:\n",
        "            while 1:\n",
        "                id = tokenized_texts[i].index('[SEP]')\n",
        "                tokenized_texts[i] = tokenized_texts[i][id:]\n",
        "                tokenized_texts[i][0] = '[CLS]'\n",
        "                if len(tokenized_texts[i]) <= MAX_LEN:\n",
        "                    break\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7wVX7eoVvdZ"
      },
      "source": [
        "# 문장 테스트\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrMkEe-iStYx"
      },
      "source": [
        "# def kv(dic, val):\n",
        "#     return [k for k, v in dic.items() if v == val]\n",
        "\n",
        "prod_dic = {v:k for k,v in dic.items()}\n",
        "# print(prod_dic)\n",
        "\n",
        "def findprod(v):\n",
        "    p = prod_dic.get(v)\n",
        "    return p"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0chLGYbpuOJa",
        "outputId": "fa87a916-2f86-4bae-c3ee-8fdaf18193ae"
      },
      "source": [
        "test.reset_index(inplace=True)\n",
        "test.pop('index')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        8108\n",
              "1        8109\n",
              "2        8110\n",
              "3        8111\n",
              "4        8112\n",
              "        ...  \n",
              "3373    11570\n",
              "3374    11571\n",
              "3375    11572\n",
              "3376    11573\n",
              "3377    11574\n",
              "Name: index, Length: 3378, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wcBYf1COsPbb",
        "outputId": "11f62953-b0f8-496d-b61a-f8e1b3218e77"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer</th>\n",
              "      <th>Detail</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35311.0</td>\n",
              "      <td>PACK OF 20 SKULL PAPER NAPKINS [SEP] SMALL DOL...</td>\n",
              "      <td>SET OF RED SALAD SERVERS</td>\n",
              "      <td>1532.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35527.0</td>\n",
              "      <td>SET OF 2 FANCY FONT TEA TOWELS [SEP] KINGS CHO...</td>\n",
              "      <td>BLACK RECORD COVER FRAME</td>\n",
              "      <td>357.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32978.0</td>\n",
              "      <td>MEDIUM CHINESE STYLE SCISSOR [SEP] SMALL CHINE...</td>\n",
              "      <td>ABSTRACT CIRCLE JOURNAL</td>\n",
              "      <td>1163.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33173.0</td>\n",
              "      <td>CERAMIC BIRDHOUSE FINCH BLUE  LARGE [SEP] CERA...</td>\n",
              "      <td>HOT WATER BOTTLE TEA AND SYMPATHY</td>\n",
              "      <td>258.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34415.0</td>\n",
              "      <td>COFFEE MUG CAT + BIRD DESIGN [SEP] LUNCH BAG R...</td>\n",
              "      <td>GREEN 3 PIECE MINI DOTS CUTLERY SET</td>\n",
              "      <td>458.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Customer  ...       0\n",
              "0   35311.0  ...  1532.0\n",
              "1   35527.0  ...   357.0\n",
              "2   32978.0  ...  1163.0\n",
              "3   33173.0  ...   258.0\n",
              "4   34415.0  ...   458.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef5h410LyavB"
      },
      "source": [
        "model = torch.load(path + name)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtGnrkpZ7u4R",
        "outputId": "6f5729a1-b3dd-4fbd-c011-d689989f30f1"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1, top=5)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uBAKy67r-28",
        "outputId": "f96af353-acc4-4110-e665-ab486b22e578"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8MiWl5477I4",
        "outputId": "9825c314-6045-424c-8570-dfc5fdfaaea9"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1, top=15)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M8AdEKV7-Vu",
        "outputId": "e15b59cb-e49b-4b5a-db15-12972dc26142"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1, top=20)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdM3CVrJ8BfD",
        "outputId": "5d7287d0-4ac6-4d99-c90a-8efc43065d80"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1, top=25)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501SUPhD8Ee0",
        "outputId": "a0739cc1-68b3-4430-c6d7-140a174fc249"
      },
      "source": [
        "acc = 0\n",
        "for i in range(len(test)):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    acc += intop10(logits, test[0][i], mode=1, top=30)\n",
        "print(\"Accuracy: {0:.3f}\".format(acc / len(test)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDIsjv6j8-xF"
      },
      "source": [
        "# table = test.copy()\n",
        "# for i in range(len(table)):\n",
        "#     logits = test_sentences([table['Detail'][i]])\n",
        "#     table['preds'][i] = intop10(logits, test[0][i], mode=2)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJ6Xx2pznAn",
        "outputId": "5722fb6d-e83e-4f70-e06c-4e318ead9fd2"
      },
      "source": [
        "print(\"Hits: {}\".format(acc))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hits: 491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhvGsmGSVvVL",
        "outputId": "be7fdcd1-770e-4897-a112-e4eaea49d09a"
      },
      "source": [
        "logits = test_sentences([test['Detail'][6]])\n",
        "\n",
        "print(logits)\n",
        "print((np.argmax(logits)))\n",
        "print(len(logits[0]))\n",
        "print(test[0][6])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.1604877  -0.77230936  1.0132667  ... -3.2165518  -3.5134737\n",
            "  -3.5878131 ]]\n",
            "196\n",
            "3487\n",
            "645.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJfPyY0Xz_qx",
        "outputId": "445b24b2-7b27-47c7-d8c6-9d4e1f88c402"
      },
      "source": [
        "logits = test_sentences([test['Detail'][100]])\n",
        "\n",
        "print(logits)\n",
        "print(np.argmax(logits))\n",
        "# print(len(logits))\n",
        "print(test['label'][100])\n",
        "intop10(logits, test[0][100], mode=1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.1493559  1.3466905  2.7635603 ... -9.861267  -9.81743   -9.8647585]]\n",
            "1640\n",
            "CHRYSANTHEMUM NOTEBOOK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r3HLNF2VxKM",
        "outputId": "94051492-b3af-4a75-a61c-61b6b0ecaf56"
      },
      "source": [
        "logits = test_sentences([data1['Detail'][100]])\n",
        "\n",
        "# print(logits)\n",
        "print(np.argmax(logits))\n",
        "# print(len(logits))\n",
        "print(data1[0][100])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195\n",
            "306.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "HREiXuP-LMIx",
        "outputId": "65c40232-08ad-4ceb-9c1d-3fc04c0f176f"
      },
      "source": [
        "a = []\n",
        "\n",
        "for i in range(0, 10):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    a.append(findprod(np.argmax(logits)))\n",
        "asdf = test[:10].copy()\n",
        "asdf['pred'] = a\n",
        "asdf.pop(0)\n",
        "asdf.pop('Customer')\n",
        "asdf"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Detail</th>\n",
              "      <th>label</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PACK OF 20 SKULL PAPER NAPKINS [SEP] SMALL DOL...</td>\n",
              "      <td>SET OF RED SALAD SERVERS</td>\n",
              "      <td>SET OF 12 LILY BOTANICAL T-LIGHTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SET OF 2 FANCY FONT TEA TOWELS [SEP] KINGS CHO...</td>\n",
              "      <td>BLACK RECORD COVER FRAME</td>\n",
              "      <td>CHERRY BLOSSOM  DECORATIVE FLASK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MEDIUM CHINESE STYLE SCISSOR [SEP] SMALL CHINE...</td>\n",
              "      <td>ABSTRACT CIRCLE JOURNAL</td>\n",
              "      <td>GROW YOUR OWN FLOWERS SET OF 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CERAMIC BIRDHOUSE FINCH BLUE  LARGE [SEP] CERA...</td>\n",
              "      <td>HOT WATER BOTTLE TEA AND SYMPATHY</td>\n",
              "      <td>LIGHT PINK CHERRY LIGHTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COFFEE MUG CAT + BIRD DESIGN [SEP] LUNCH BAG R...</td>\n",
              "      <td>GREEN 3 PIECE MINI DOTS CUTLERY SET</td>\n",
              "      <td>HOME BUILDING BLOCK WORD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TEA BAG PLATE RED SPOTTY [SEP] GARLAND WOODEN ...</td>\n",
              "      <td>GREY FLORAL FELTCRAFT SHOULDER BAG</td>\n",
              "      <td>I'M ON HOLIDAY METAL SIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SPRING DEC , HANGING CHICK  GREEN [SEP] GREEN ...</td>\n",
              "      <td>SET OF 36 MUSHROOM PAPER DOILIES</td>\n",
              "      <td>ROUND SNACK BOXES SET OF4 WOODLAND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PARTY METAL SIGN [SEP] TROPICAL LUGGAGE TAG [S...</td>\n",
              "      <td>SPRING FLOWER CHOPSTICKS SET/5</td>\n",
              "      <td>GARLAND WOODEN HAPPY EASTER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CREAM FELT EASTER EGG BASKET</td>\n",
              "      <td>PACK OF 6 BIRDY GIFT TAGS</td>\n",
              "      <td>PARTY BUNTING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SKULLS  STICKERS [SEP] SET/4 SKULL BADGES [SEP...</td>\n",
              "      <td>BOX OF 24 COCKTAIL PARASOLS</td>\n",
              "      <td>WHITE WOOD GARDEN PLANT LADDER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Detail  ...                                pred\n",
              "0  PACK OF 20 SKULL PAPER NAPKINS [SEP] SMALL DOL...  ...   SET OF 12 LILY BOTANICAL T-LIGHTS\n",
              "1  SET OF 2 FANCY FONT TEA TOWELS [SEP] KINGS CHO...  ...    CHERRY BLOSSOM  DECORATIVE FLASK\n",
              "2  MEDIUM CHINESE STYLE SCISSOR [SEP] SMALL CHINE...  ...      GROW YOUR OWN FLOWERS SET OF 3\n",
              "3  CERAMIC BIRDHOUSE FINCH BLUE  LARGE [SEP] CERA...  ...            LIGHT PINK CHERRY LIGHTS\n",
              "4  COFFEE MUG CAT + BIRD DESIGN [SEP] LUNCH BAG R...  ...            HOME BUILDING BLOCK WORD\n",
              "5  TEA BAG PLATE RED SPOTTY [SEP] GARLAND WOODEN ...  ...           I'M ON HOLIDAY METAL SIGN\n",
              "6  SPRING DEC , HANGING CHICK  GREEN [SEP] GREEN ...  ...  ROUND SNACK BOXES SET OF4 WOODLAND\n",
              "7  PARTY METAL SIGN [SEP] TROPICAL LUGGAGE TAG [S...  ...         GARLAND WOODEN HAPPY EASTER\n",
              "8                       CREAM FELT EASTER EGG BASKET  ...                       PARTY BUNTING\n",
              "9  SKULLS  STICKERS [SEP] SET/4 SKULL BADGES [SEP...  ...      WHITE WOOD GARDEN PLANT LADDER\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvg-MFD7CW_q",
        "outputId": "d00377a6-b0a6-4602-a9a2-41e659b5fdbf"
      },
      "source": [
        "for i in range(0, len(test), 500):\n",
        "    logits = test_sentences([test['Detail'][i]])\n",
        "    # print(np.argmax(logits))\n",
        "    print(intop10(logits, test[0][i], mode=0))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1088, 1686, 2131, 547, 64, 1606, 495, 407, 3022, 2143]\n",
            "False\n",
            "[317, 130, 1735, 989, 199, 144, 321, 1299, 1526, 1598]\n",
            "False\n",
            "[792, 294, 438, 619, 195, 109, 620, 452, 189, 55]\n",
            "True\n",
            "[139, 1062, 783, 988, 1686, 106, 46, 1571, 58, 577]\n",
            "False\n",
            "[196, 310, 391, 424, 322, 31, 298, 399, 299, 970]\n",
            "False\n",
            "[400, 661, 947, 121, 649, 827, 130, 914, 190, 615]\n",
            "False\n",
            "[207, 457, 190, 1081, 989, 180, 130, 453, 947, 400]\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b3REdxw4A9T"
      },
      "source": [
        "# print(findprod(661))\n",
        "# print(findprod(400))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-M38gXvPGQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11679a9-a02e-4105-e350-34a77888e11e"
      },
      "source": [
        "a = test['Detail'][199]\n",
        "logits = test_sentences([a])\n",
        "print('sent:',a)\n",
        "b = intop10(logits, test[0][199], mode=2)\n",
        "print('label:', findprod(test[0][199]), test[0][199])\n",
        "p = []\n",
        "for v in b:\n",
        "    p.append(findprod(v))\n",
        "print(p)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent: 6 RIBBONS RUSTIC CHARM [SEP] UNION JACK FLAG LUGGAGE TAG [SEP] RED HEART LUGGAGE TAG [SEP] CLOTHES PEGS RETROSPOT PACK 24 [SEP] 20 DOLLY PEGS RETROSPOT [SEP] RED SPOTTY LUGGAGE TAG [SEP] ZINC WILLIE WINKIE  CANDLE STICK [SEP] 12 PENCILS TALL TUBE RED SPOTTY [SEP] TEA BAG PLATE RED SPOTTY [SEP] STRAWBERRY SCENTED SET/9 T-LIGHTS [SEP] CINAMMON SET OF 9 T-LIGHTS [SEP] ORANGE SCENTED SET/9 T-LIGHTS [SEP] S/3 FRENCH VANILLA SQU CANDLE [SEP] 12 PENCILS SMALL TUBE RED SPOTTY [SEP] PAPER BUNTING RETRO SPOTS [SEP] WOODEN SCHOOL COLOURING SET [SEP] HANGING HEART ZINC T-LIGHT HOLDER [SEP] NATURAL SLATE RECTANGLE CHALKBOARD [SEP] 72 SWEETHEART FAIRY CAKE CASES [SEP] LAVENDER SCENTED FABRIC HEART [SEP] PACK OF 72 RETRO SPOT CAKE CASES [SEP] HANGING JAM JAR T-LIGHT HOLDER [SEP] SET/20 RED SPOTTY PAPER NAPKINS [SEP] GROW YOUR OWN BASIL IN ENAMEL MUG [SEP] ZINC METAL HEART DECORATION [SEP] RED ROSE AND LACE C/COVER [SEP] SET 3 WICKER LOG BASKETS [SEP] FAIRY CAKE WICKER PICNIC BASKET [SEP] SET/5 RED SPOTTY LID GLASS BOWLS [SEP] GROW YOUR OWN HERBS SET OF 3 [SEP] ENAMEL FIRE BUCKET CREAM [SEP] TEA TIME PARTY BUNTING [SEP] SMALL RED RETROSPOT MUG IN BOX [SEP] SMALL RED RETROSPOT MUG IN BOX [SEP] PAPER CHAIN KIT EMPIRE [SEP] PAPER CHAIN KIT RETRO SPOT [SEP] FOLKART HEART NAPKIN RINGS [SEP] WOODEN CROQUET GARDEN SET [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] DOORSTOP RETROSPOT HEART [SEP] BABY BOOM RIBBONS [SEP] CHOCOLATE BOX RIBBONS [SEP] LUSH GREENS RIBBONS [SEP] ZINC METAL HEART DECORATION [SEP] RIBBON REEL SPOTS DESIGN [SEP] 6 RIBBONS RUSTIC CHARM [SEP] SPRING FLOWER CHOPSTICKS SET/5 [SEP] NEW ENGLAND CERAMIC CAKE SERVER [SEP] TRADITIONAL CHRISTMAS RIBBONSZINC METAL HEART DECORATION [SEP] WOODEN SCHOOL COLOURING SET [SEP] S/6 WOODEN SKITTLES IN COTTON BAG [SEP] WHITE HANGING HEART T-LIGHT HOLDER [SEP] SMALL RED RETROSPOT MUG IN BOX [SEP] DOORSTOP RETROSPOT HEART [SEP] GINGHAM HEART  DOORSTOP RED [SEP] WOODEN ROUNDERS GARDEN SET\n",
            "[67, 2, 499, 989, 932, 424, 130, 570, 370, 661]\n",
            "label: PAPER CHAIN KIT RETRO SPOT 610.0\n",
            "['WHITE WOOD GARDEN PLANT LADDER', 'EDWARDIAN PARASOL NATURAL', 'PINK HAPPY BIRTHDAY BUNTING', 'REX CASH+CARRY JUMBO SHOPPER', 'RETRO RED SPOTTY WASHING UP GLOVES', 'BAKING SET 9 PIECE RETROSPOT', 'REGENCY CAKESTAND 3 TIER', 'PACK OF 60 PINK PAISLEY CAKE CASES', 'SET/20 STRAWBERRY PAPER NAPKINS', 'WHITE HANGING HEART T-LIGHT HOLDER']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dNTsPyVFaDg",
        "outputId": "da3d891c-2b86-4f09-9d4c-e08d1da11ce3"
      },
      "source": [
        "data1['label'].value_counts()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WHITE HANGING HEART T-LIGHT HOLDER    157\n",
              "DOOR MAT UNION FLAG                    64\n",
              "REGENCY CAKESTAND 3 TIER               53\n",
              "HOME BUILDING BLOCK WORD               51\n",
              "PARTY BUNTING                          50\n",
              "                                     ... \n",
              "PAINTED LIGHTBULB STAR+ MOON            1\n",
              "MINI PAINTED GARDEN DECORATION          1\n",
              "ORIGAMI ROSE INCENSE IN TUBE            1\n",
              "ORANGE SCENTED SET/9 T-LIGHTS           1\n",
              "SPOTS MUG                               1\n",
              "Name: label, Length: 1818, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmX2T669G3qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6995b6ad-31c0-4f1d-df91-45738eec9f20"
      },
      "source": [
        "data1[0].value_counts()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "661.0     157\n",
              "400.0      64\n",
              "130.0      53\n",
              "121.0      51\n",
              "649.0      50\n",
              "         ... \n",
              "2352.0      1\n",
              "1284.0      1\n",
              "2393.0      1\n",
              "3346.0      1\n",
              "1953.0      1\n",
              "Name: 0, Length: 1818, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKg59rd0j51o"
      },
      "source": [
        "모델 참고 링크\n",
        "\n",
        "https://zzaebok.github.io/deep_learning/nlp/Bert-for-classification/\n",
        "\n",
        "https://huggingface.co/transformers/model_doc/bert.html\n",
        "\n",
        "https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP#scrollTo=VJ76KiP_dLn-&uniqifier=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGlcT-8ggKtT"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CWfj4TkgKrd"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY1LA7PigKob"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNR85sEgKl6"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0t6yfWG4nfE"
      },
      "source": [
        "# class NsmcDataset(Dataset):\n",
        "#     ''' Naver Sentiment Movie Corpus Dataset '''\n",
        "#     def __init__(self, df):\n",
        "#         self.df = df\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.df.iloc[idx, 0]\n",
        "#         label = self.df.iloc[idx, 1]\n",
        "#         return text, label"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_-qFShyCKZN"
      },
      "source": [
        "# la = pd.DataFrame(labels)\n",
        "# display(la.head())\n",
        "# train_data = pd.concat([train, la], axis=1)\n",
        "# train_data.head()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL1ZsJWqr3vu"
      },
      "source": [
        "# nsmc_train_dataset = NsmcDataset(train_data)\n",
        "# train_loader = DataLoader(nsmc_train_dataset, batch_size=2, shuffle=True, num_workers=2)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sJra_w8A05"
      },
      "source": [
        "# device = torch.device(\"cuda\")\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(detail))\n",
        "# model.to(device)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juqzM7JA2qbK"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sud8GF652sG8"
      },
      "source": [
        "# !git clone https://github.com/google-research/bert.git"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TXaeexZ2uxc"
      },
      "source": [
        "# !python /content/bert/create_pretraining_data.py \\\n",
        "#   --input_file=train \\\n",
        "#   --output_file=/tmp/tf_examples.tfrecord \\\n",
        "#   --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
        "#   --do_lower_case=False \\\n",
        "#   --max_seq_length=128 \\\n",
        "#   --max_predictions_per_seq=20 \\\n",
        "#   --masked_lm_prob=0.15 \\\n",
        "#   --random_seed=12345 \\\n",
        "#   --dupe_factor=5"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp2vsWI72uho"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2XjCrPb8hJm"
      },
      "source": [
        "# optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "# itr = 1\n",
        "# p_itr = 500\n",
        "# epochs = 1\n",
        "# total_loss = 0\n",
        "# total_len = 0\n",
        "# total_correct = 0\n",
        "\n",
        "\n",
        "# model.train()\n",
        "# for epoch in range(epochs):\n",
        "    \n",
        "#     for text, label in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "        \n",
        "#         # encoding and zero padding\n",
        "#         # encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "#         encoded_list = tokenized_texts\n",
        "#         # padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "#         # 토큰을 숫자 인덱스로 변환\n",
        "#         input_ids = [tokenizer.convert_tokens_to_ids(x) for x in encoded_list]\n",
        "\n",
        "#         # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "#         padded_list = pad_sequences(input_ids, maxlen=256, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "#         # print(padded_list[0])\n",
        "#         # print(len(padded_list[0]))\n",
        "#         # print(len(padded_list))\n",
        "#         # print(type(padded_list))\n",
        "#         sample = torch.tensor(padded_list).to(torch.int32)\n",
        "#         sample, label = sample.to(device), label.to(device)\n",
        "#         labels = torch.tensor(label).to(torch.int32)\n",
        "#         outputs = model(sample, labels=labels)\n",
        "#         loss, logits = outputs\n",
        "\n",
        "#         pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "#         correct = pred.eq(labels)\n",
        "#         total_correct += correct.sum().item()\n",
        "#         total_len += len(labels)\n",
        "#         total_loss += loss.item()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         if itr % p_itr == 0:\n",
        "#             print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
        "#             total_loss = 0\n",
        "#             total_len = 0\n",
        "#             total_correct = 0\n",
        "\n",
        "#         itr+=1"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6S4P88uH4D_"
      },
      "source": [
        "# prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "# next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "# encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "\n",
        "# outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "# logits = outputs.logits\n",
        "# assert logits[0, 0] < logits[0, 1] # next sentence was random"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YexC053uZri"
      },
      "source": [
        ""
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vf6QGTRu9BR"
      },
      "source": [
        ""
      ],
      "execution_count": 83,
      "outputs": []
    }
  ]
}